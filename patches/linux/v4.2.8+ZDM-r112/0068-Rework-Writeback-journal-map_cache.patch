From 4eb0560a4a361d226aabc9865849330a707a4c60 Mon Sep 17 00:00:00 2001
From: Shaun Tancheff <shaun@tancheff.com>
Date: Tue, 3 May 2016 22:20:08 -0500
Subject: [PATCH 68/71] Rework: Writeback journal / map_cache

Map cache add a header entry rather than opencoding entry #0.
Rework wb journal map and lookup cache.

Signed-off-by: Shaun Tancheff <shaun.tancheff@seagate.com>
---
 drivers/md/dm-zdm.c |   2 +-
 drivers/md/dm-zdm.h |  36 ++-
 drivers/md/libzdm.c | 877 ++++++++++++++++++++++++++++++----------------------
 3 files changed, 533 insertions(+), 382 deletions(-)

diff --git a/drivers/md/dm-zdm.c b/drivers/md/dm-zdm.c
index 44d03ec..15bc59a 100644
--- a/drivers/md/dm-zdm.c
+++ b/drivers/md/dm-zdm.c
@@ -467,7 +467,7 @@ static int zoned_ctr(struct dm_target *ti, unsigned int argc, char **argv)
 	u64 first_data_zone = 0;
 	u64 mz_md_provision = MZ_METADATA_ZONES;
 
-	BUILD_BUG_ON(Z_C4K != (sizeof(struct map_sect_to_lba) * Z_UNSORTED));
+	BUILD_BUG_ON(Z_C4K != (sizeof(struct map_cache_data)));
 	BUILD_BUG_ON(Z_C4K != (sizeof(struct io_4k_block)));
 	BUILD_BUG_ON(Z_C4K != (sizeof(struct mz_superkey)));
 
diff --git a/drivers/md/dm-zdm.h b/drivers/md/dm-zdm.h
index 30ea11a..303ab24 100644
--- a/drivers/md/dm-zdm.h
+++ b/drivers/md/dm-zdm.h
@@ -59,7 +59,8 @@
 #define Z_AQ_META_STREAM	(Z_AQ_META | Z_AQ_STREAM_ID | 0xFE)
 
 #define Z_C4K			(4096ul)
-#define Z_UNSORTED		(Z_C4K / sizeof(struct map_sect_to_lba))
+#define Z_UNSORTED		(Z_C4K / sizeof(struct map_cache_entry))
+#define Z_MAP_MAX		(Z_UNSORTED - 1)
 #define Z_BLOCKS_PER_DM_SECTOR	(Z_C4K/512)
 #define MZ_METADATA_ZONES	(8ul)
 #define Z_SHFT4K		(3)
@@ -72,6 +73,11 @@
 #define Z_VERSION		1
 #define MAX_ZONED_VERSION	1
 
+#define ZONE_SECT_BITS		19
+#define Z_BLKBITS		16
+#define Z_BLKSZ			(1ul << Z_BLKBITS)
+#define Z_SMR_SZ_BYTES		(Z_C4K << Z_BLKBITS)
+
 #define UUID_LEN		16
 
 #define Z_TYPE_SMR		2
@@ -183,6 +189,7 @@ enum pg_flag_enum {
 	WB_JRNL_2,
 	WB_DIRECT,
 	WB_RE_CACHE,
+	IN_WB_JOURNAL,
 
 	R_IN_FLIGHT,
 	W_IN_FLIGHT,
@@ -279,17 +286,32 @@ struct map_addr {
 };
 
 /**
- * struct map_sect_to_lba - Sector to LBA mapping.
+ * struct map_cache_entry - Sector to LBA mapping.
  * @tlba: tlba
  * @physical: blba or number of blocks
  *
  * Longer description of this structure.
  */
-struct map_sect_to_lba {
+struct map_cache_entry {
 	__le64 tlba;		/* record type [16 bits] + logical sector # */
 	__le64 bval;	/* csum 16 [16 bits] + 'physical' block lba */
 } __packed;
 
+struct map_cache_data {
+	struct map_cache_entry header;
+	struct map_cache_entry maps[Z_MAP_MAX];
+} __packed;
+
+struct gc_map_cache_data {
+	struct map_cache_entry header;
+	struct map_cache_entry maps[Z_BLKSZ];
+} __packed;
+
+struct jrnl_map_cache_data {
+	struct map_cache_entry header;
+	struct map_cache_entry maps[WB_JRNL_MAX];
+} __packed;
+
 /**
  * enum map_type_enum - Map Cache pool types
  * @IS_MAP: Is an ingress map pool
@@ -299,8 +321,8 @@ struct map_sect_to_lba {
 enum map_type_enum {
 	IS_MAP,
 	IS_DISCARD,
-	IS_JRNL_PG,
 	MAP_COUNT,
+	IS_JRNL_PG,
 	IS_POST_MAP,
 };
 
@@ -320,10 +342,10 @@ enum map_type_enum {
  */
 struct map_cache {
 	struct list_head mclist;
-	struct map_sect_to_lba *jdata;
+	void *mcd;
 	atomic_t refcount;
-	struct mutex cached_lock;
 	atomic_t busy_locked;
+	struct mutex cached_lock;
 	int jcount;
 	int jsorted;
 	int jsize;
@@ -605,6 +627,7 @@ struct stale_tracking {
  * @z_mega:
  * @meta_wq:
  * @gc_postmap:
+ * @jrnl_map:
  * @io_client:
  * @io_wq:
  * @zone_action_wq:
@@ -720,6 +743,7 @@ struct zdm {
 	u64 flush_age;
 	struct workqueue_struct *meta_wq;
 	struct map_cache gc_postmap;
+	struct map_cache jrnl_map;
 	struct dm_io_client *io_client;
 	struct workqueue_struct *io_wq;
 	struct workqueue_struct *zone_action_wq;
diff --git a/drivers/md/libzdm.c b/drivers/md/libzdm.c
index a301264..ffd27d6 100644
--- a/drivers/md/libzdm.c
+++ b/drivers/md/libzdm.c
@@ -12,7 +12,7 @@
  * warranty of any kind, whether express or implied.
  */
 
-#define BUILD_NO		111
+#define BUILD_NO		112
 
 #define EXTRA_DEBUG		0
 
@@ -49,11 +49,7 @@
 #define Z_KEY_SIG		0xFEDCBA987654321Ful
 
 #define Z_CRC_4K		4096
-#define ZONE_SECT_BITS		19
-#define Z_BLKBITS		16
-#define Z_BLKSZ			(1ul << Z_BLKBITS)
 #define MAX_ZONES_PER_MZ	1024
-#define Z_SMR_SZ_BYTES		(Z_C4K << Z_BLKBITS)
 
 #define GC_READ			(1ul << 15)
 #define BAD_ADDR		(~0ul)
@@ -95,6 +91,7 @@ static int do_init_zoned(struct dm_target *ti, struct zdm *znd);
 static void update_all_stale_ratio(struct zdm *znd);
 static int z_discard_partial(struct zdm *znd, u32 blks, gfp_t gfp);
 static u64 z_discard_range(struct zdm *znd, u64 addr, gfp_t gfp);
+static u64 z_lookup_journal_cache(struct zdm *znd, u64 addr);
 static u64 z_lookup_cache(struct zdm *znd, u64 addr, int type);
 static u64 z_lookup_table(struct zdm *znd, u64 addr, gfp_t gfp);
 static u64 current_mapping(struct zdm *znd, u64 addr, gfp_t gfp);
@@ -103,8 +100,8 @@ static int z_mapped_discard(struct zdm *znd, u64 tlba, u64 count, gfp_t gfp);
 static int z_mapped_addmany(struct zdm *znd, u64 dm_s, u64 lba, u64,
 			    gfp_t gfp);
 static int z_to_map_list(struct zdm *znd, u64 dm_s, u64 lba, gfp_t gfp);
-static int z_to_journal_list(struct zdm *znd, u64 dm_s, u64 lba, gfp_t gfp);
 static int z_to_discard_list(struct zdm *znd, u64 dm_s, u64 blks, gfp_t gfp);
+static int md_journal_add_map(struct zdm *znd, u64 addr, u64 lba);
 static int discard_merge(struct zdm *znd, u64 tlba, u64 blks);
 static int z_mapped_sync(struct zdm *znd);
 static int z_mapped_init(struct zdm *znd);
@@ -287,7 +284,7 @@ static inline u32 crcpg(void *data)
  *
  * Return: 32 bit CRC in little endian format.
  */
-static inline __le32 crc32c_le32(u32 init, void * data, u32 sz)
+static inline __le32 crc32c_le32(u32 init, void *data, u32 sz)
 {
 	return cpu_to_le32(crc32c(init, data, sz));
 }
@@ -748,9 +745,9 @@ static inline int is_ready_for_gc(struct zdm *znd, u32 z_id)
 #define PG_11    (GET_ZPG | 11) /* superblock: temporary */
 #define PG_27    (GET_ZPG | 27) /* map_pg data block */
 
-#define KM_00    (GET_KM  |  0)  /* ZDM: Instance */
-#define KM_07    (GET_KM  |  7)  /* mcache struct */
-#define KM_16    (GET_KM  | 16)  /* gc descriptor */
+#define KM_00    (GET_KM  |  0) /* ZDM: Instance */
+#define KM_07    (GET_KM  |  7) /* mcache struct */
+#define KM_16    (GET_KM  | 16) /* gc descriptor */
 #define KM_18    (GET_KM  | 18) /* wset : sync */
 #define KM_19    (GET_KM  | 19) /* wset */
 #define KM_20    (GET_KM  | 20) /* map_pg struct */
@@ -1092,7 +1089,7 @@ static int release_memcache(struct zdm *znd)
 		SpinLock(&znd->mclck[no]);
 		list_for_each_entry_safe(mcache, _mc, head, mclist) {
 			list_del(&mcache->mclist);
-			ZDM_FREE(znd, mcache->jdata, Z_C4K, PG_08);
+			ZDM_FREE(znd, mcache->mcd, Z_C4K, PG_08);
 			ZDM_FREE(znd, mcache, sizeof(*mcache), KM_07);
 		}
 		spin_unlock(&znd->mclck[no]);
@@ -1367,10 +1364,15 @@ static void zoned_destroy(struct zdm *znd)
 		ZDM_FREE(znd, znd->md_crcs, Z_C4K * 2, MP_22);
 	if (znd->gc_io_buf)
 		ZDM_FREE(znd, znd->gc_io_buf, Z_C4K * GC_MAX_STRIPE, VM_04);
-	if (znd->gc_postmap.jdata) {
-		size_t sz = Z_BLKSZ * sizeof(*znd->gc_postmap.jdata);
+	if (znd->gc_postmap.mcd) {
+		size_t sz = sizeof(struct gc_map_cache_data);
 
-		ZDM_FREE(znd, znd->gc_postmap.jdata, sz, VM_03);
+		ZDM_FREE(znd, znd->gc_postmap.mcd, sz, VM_03);
+	}
+	if (znd->jrnl_map.mcd) {
+		size_t sz = sizeof(struct jrnl_map_cache_data);
+
+		ZDM_FREE(znd, znd->jrnl_map.mcd, sz, VM_03);
 	}
 
 	for (purge = 0; purge < ARRAY_SIZE(znd->io_vcache); purge++) {
@@ -1771,6 +1773,7 @@ static int do_init_zoned(struct dm_target *ti, struct zdm *znd)
 	mutex_init(&znd->pool_mtx);
 	mutex_init(&znd->gc_wait);
 	mutex_init(&znd->gc_postmap.cached_lock);
+	mutex_init(&znd->jrnl_map.cached_lock);
 	mutex_init(&znd->gc_vcio_lock);
 	mutex_init(&znd->vcio_lock);
 	mutex_init(&znd->mz_io_mutex);
@@ -1881,8 +1884,8 @@ static int do_init_zoned(struct dm_target *ti, struct zdm *znd)
 	Z_INFO(znd, "Reverse CRC table %" PRIx64, znd->c_mid );
 	Z_INFO(znd, "Normal data start %" PRIx64, znd->data_lba);
 
-	znd->gc_postmap.jdata = ZDM_CALLOC(znd, Z_BLKSZ + 1,
-		sizeof(*znd->gc_postmap.jdata), VM_03, NORMAL);
+	znd->gc_postmap.mcd = ZDM_ALLOC(znd, sizeof(struct gc_map_cache_data),
+					VM_03, NORMAL);
 	znd->md_crcs = ZDM_ALLOC(znd, Z_C4K * 2, MP_22, NORMAL);
 	znd->gc_io_buf = ZDM_CALLOC(znd, GC_MAX_STRIPE, Z_C4K, VM_04, NORMAL);
 	znd->wp = _alloc_wp(znd);
@@ -1891,7 +1894,7 @@ static int do_init_zoned(struct dm_target *ti, struct zdm *znd)
 	znd->io_vcache[1] = ZDM_CALLOC(znd, IO_VCACHE_PAGES,
 				sizeof(struct io_4k_block), VM_12, NORMAL);
 
-	if (!znd->gc_postmap.jdata || !znd->md_crcs || !znd->gc_io_buf ||
+	if (!znd->gc_postmap.mcd || !znd->md_crcs || !znd->gc_io_buf ||
 	    !znd->wp || !znd->io_vcache[0] || !znd->io_vcache[1]) {
 		rcode = -ENOMEM;
 		goto out;
@@ -1900,6 +1903,15 @@ static int do_init_zoned(struct dm_target *ti, struct zdm *znd)
 	znd->gc_postmap.map_content = IS_POST_MAP;
 	_init_mdcrcs(znd);
 
+	znd->jrnl_map.mcd = ZDM_ALLOC(znd, sizeof(struct jrnl_map_cache_data),
+					VM_03, NORMAL);
+	if (!znd->jrnl_map.mcd) {
+		rcode = -ENOMEM;
+		goto out;
+	}
+	znd->jrnl_map.jsize = WB_JRNL_MAX;
+	znd->jrnl_map.map_content = IS_JRNL_PG;
+
 	znd->io_client = dm_io_client_create();
 	if (!znd->io_client) {
 		rcode = -ENOMEM;
@@ -2172,20 +2184,27 @@ static int zoned_init_disk(struct dm_target *ti, struct zdm *znd,
 }
 
 /**
- * compare_tlba() - Compare on tlba48 ignoring high 16 bits.
+ * mcache_cmp() - Compare on tlba48 ignoring high 16 bits.
  * @x1: Page of map cache
  * @x2: Page of map cache
  *
- * Return: -1 less than, 1 greater than, 0 if equal.
+ * Return: 1 less than, -1 greater than, 0 if equal.
  */
-static int compare_tlba(const void *x1, const void *x2)
+static int mcache_cmp(const void *x1, const void *x2)
 {
-	const struct map_sect_to_lba *r1 = x1;
-	const struct map_sect_to_lba *r2 = x2;
+	const struct map_cache_entry *r1 = x1;
+	const struct map_cache_entry *r2 = x2;
 	const u64 v1 = le64_to_lba48(r1->tlba, NULL);
 	const u64 v2 = le64_to_lba48(r2->tlba, NULL);
 
-	return (v1 < v2) ? -1 : ((v1 > v2) ? 1 : 0);
+	return (v1 < v2) ? 1 : ((v1 > v2) ? -1 : 0);
+}
+
+static inline void *get_mcd(struct map_cache *mcache)
+{
+	BUG_ON(!mcache->mcd);
+
+	return mcache->mcd;
 }
 
 /**
@@ -2198,13 +2217,14 @@ static int compare_tlba(const void *x1, const void *x2)
 static int _lsearch_tlba(struct map_cache *mcache, u64 dm_s)
 {
 	int at = -1;
-	int jentry;
+	int idx;
+	struct map_cache_data *mcd = get_mcd(mcache);
 
-	for (jentry = mcache->jcount; jentry > 0; jentry--) {
-		u64 logi = le64_to_lba48(mcache->jdata[jentry].tlba, NULL);
+	for (idx = 0; idx < mcache->jcount; idx++) {
+		u64 logi = le64_to_lba48(mcd->maps[idx].tlba, NULL);
 
 		if (logi == dm_s) {
-			at = jentry - 1;
+			at = idx;
 			goto out;
 		}
 	}
@@ -2213,32 +2233,34 @@ out:
 	return at;
 }
 
-/**
- * _lsearch_extent() - Do a linear search over a page of discard entries.
- * @mcache: Page of map cache entries to search.
- * @dm_s: tlba being sought.
- *
- * Return: 0 to jcount - 1 if found. -1 if not found
- *
- * NOTE: In this case the match is if the tlba is include in the extent
- */
-static int _lsearch_extent(struct map_cache *mcache, u64 dm_s)
-{
-	int at = -1;
-	int jentry;
+/* Perform a binary search for KEY in BASE which has NMEMB elements
+   of SIZE bytes each.  The comparisons are done by (*COMPAR)().  */
 
-	for (jentry = mcache->jcount; jentry > 0; jentry--) {
-		u64 addr = le64_to_lba48(mcache->jdata[jentry].tlba, NULL);
-		u64 blocks = le64_to_lba48(mcache->jdata[jentry].bval, NULL);
+static void *gnu_bsearch(const void *key, const void *base,
+			 size_t nmemb, size_t size,
+			 int (*compar) (const void *, const void *))
+{
+	size_t l, u, idx;
+	const void *p;
+	int comparison;
 
-		if ((dm_s >= addr) && dm_s < (addr+blocks)) {
-			at = jentry - 1;
-			goto out;
-		}
+	l = 0;
+	u = nmemb;
+	while (l < u)
+	{
+		idx = (l + u) / 2;
+		p = (void *) (((const char *) base) + (idx * size));
+
+		comparison = (*compar) (key, p);
+		if (comparison < 0)
+			u = idx;
+		else if (comparison > 0)
+			l = idx + 1;
+		else
+			return (void *) p;
 	}
 
-out:
-	return at;
+	return NULL;
 }
 
 /**
@@ -2251,18 +2273,43 @@ out:
 static int _bsrch_tlba(struct map_cache *mcache, u64 tlba)
 {
 	int at = -1;
-	void *base = &mcache->jdata[1];
+	struct map_cache_data *mcd = get_mcd(mcache);
+	void *base = &mcd->maps[0];
 	void *map;
-	struct map_sect_to_lba find;
+	struct map_cache_entry find;
 
 	find.tlba = lba48_to_le64(0, tlba);
 
 	if (mcache->jcount < 0 || mcache->jcount > mcache->jsize)
-		return at;
+		goto out;
 
-	map = bsearch(&find, base, mcache->jcount, sizeof(find), compare_tlba);
+	map = gnu_bsearch(&find, base, mcache->jcount, sizeof(find), mcache_cmp);
 	if (map)
 		at = (map - base) / sizeof(find);
+out:
+
+#if 1
+	{
+	int cmp = _lsearch_tlba(mcache, tlba);
+
+	if (cmp != at) {
+		int kbsrc = -1;
+
+		map = bsearch(&find, base, mcache->jcount,
+			      sizeof(find), mcache_cmp);
+		if (map)
+			kbsrc = (map - base) / sizeof(find);
+
+		pr_err("_bsrch_tlba is failing got %d need %d [%d] "
+		       "[items: %d, sorted %s.\n",
+			at, cmp, kbsrc, mcache->jcount,
+			(mcache->jcount == mcache->jsorted)
+				? "yes" : "no");
+
+		at = cmp;
+	}
+	}
+#endif
 
 	return at;
 }
@@ -2276,19 +2323,47 @@ static int _bsrch_tlba(struct map_cache *mcache, u64 tlba)
  */
 static int compare_ext(const void *x1, const void *x2)
 {
-	const struct map_sect_to_lba *r1 = x1;
-	const struct map_sect_to_lba *r2 = x2;
+	const struct map_cache_entry *r1 = x1;
+	const struct map_cache_entry *r2 = x2;
 	const u64 key = le64_to_lba48(r1->tlba, NULL);
 	const u64 val = le64_to_lba48(r2->tlba, NULL);
 	const u64 ext = le64_to_lba48(r2->bval, NULL);
 
 	if (key < val)
-		return -1;
+		return 1;
 	if (key >= val && key < (val+ext))
 		return 0;
-	return 1;
+	return -1;
 }
 
+/**
+ * _lsearch_extent() - Do a linear search over a page of discard entries.
+ * @mcache: Page of map cache entries to search.
+ * @dm_s: tlba being sought.
+ *
+ * Return: 0 to jcount - 1 if found. -1 if not found
+ *
+ * NOTE: In this case the match is if the tlba is include in the extent
+ */
+static int _lsearch_extent(struct map_cache *mcache, u64 dm_s)
+{
+	int at = -1;
+	int idx;
+	struct map_cache_data *mcd = get_mcd(mcache);
+
+	for (idx = 0; idx < mcache->jcount; idx++) {
+		u64 addr = le64_to_lba48(mcd->maps[idx].tlba, NULL);
+		u64 blocks = le64_to_lba48(mcd->maps[idx].bval, NULL);
+
+		if ((dm_s >= addr) && dm_s < (addr+blocks)) {
+			at = idx;
+			goto out;
+		}
+	}
+
+out:
+	return at;
+}
 
 /**
  * _bsrch_extent() - Do a binary search over a page of map_cache extents.
@@ -2300,12 +2375,13 @@ static int compare_ext(const void *x1, const void *x2)
 static int _bsrch_extent(struct map_cache *mcache, u64 tlba)
 {
 	int at = -1;
-	void *base = &mcache->jdata[1];
+	struct map_cache_data *mcd = get_mcd(mcache);
+	void *base = &mcd->maps[0];
 	void *map;
-	struct map_sect_to_lba find;
+	struct map_cache_entry find;
 
 	find.tlba = lba48_to_le64(0, tlba);
-	map = bsearch(&find, base, mcache->jcount, sizeof(find), compare_ext);
+	map = gnu_bsearch(&find, base, mcache->jcount, sizeof(find), compare_ext);
 	if (map)
 		at = (map - base) / sizeof(find);
 
@@ -2325,7 +2401,7 @@ static int _bsrch_extent(struct map_cache *mcache, u64 tlba)
 }
 
 /**
- * _bsrch_tlba() - Mutex Lock and binary search.
+ * _bsrch_tlba_lck() - Mutex Lock and binary search.
  * @mcache: Page of map cache entries to search.
  * @tlba: tlba being sought.
  *
@@ -2376,9 +2452,6 @@ static inline int mcache_bsearch(struct map_cache *mcache, u64 tlba)
 	case IS_DISCARD:
 		at = _bsrch_extent(mcache, tlba);
 		break;
-	case IS_JRNL_PG:
-		at = _bsrch_tlba(mcache, tlba);
-		break;
 	default:
 		pr_err("Invalid map type: %u\n", mcache->map_content);
 		dump_stack();
@@ -2405,9 +2478,6 @@ static inline int mcache_lsearch(struct map_cache *mcache, u64 tlba)
 	case IS_DISCARD:
 		at = _lsearch_extent(mcache, tlba);
 		break;
-	case IS_JRNL_PG:
-		at = _lsearch_tlba(mcache, tlba);
-		break;
 	default:
 		pr_err("Invalid map type: %u\n", mcache->map_content);
 		dump_stack();
@@ -2485,7 +2555,7 @@ static u64 _current_mapping(struct zdm *znd, int nodisc, u64 addr, gfp_t gfp)
 	u64 found = 0ul;
 
 	if (addr < znd->data_lba) {
-		found = z_lookup_cache(znd, addr, IS_JRNL_PG);
+		found = z_lookup_journal_cache(znd, addr);
 		if (!found)
 			found = addr;
 	}
@@ -2578,38 +2648,6 @@ static int z_mapped_add_one(struct zdm *znd, u64 dm_s, u64 lba, gfp_t gfp)
 }
 
 /**
- * md_journal_add_map() - Add an entry to the map cache block mapping.
- * @znd: ZDM Instance
- * @tlba: Address being discarded.
- * @blks: number of blocks being discard.
- * @gfp: Current memory allocation scheme.
- *
- * Add a new extent entry.
- */
-static int md_journal_add_map(struct zdm *znd, u64 dm_s, u64 lba, gfp_t gfp)
-{
-	int err = 0;
-
-	if (dm_s < znd->data_lba) {
-		struct md_journal *jrnl = &znd->jrnl;
-		u32 entry = lba - WB_JRNL_BASE;
-
-		if (entry < jrnl->size) {
-			SpinLock(&jrnl->wb_alloc);
-			jrnl->wb[entry] = dm_s;
-			set_bit(IS_DIRTY, &jrnl->flags);
-			spin_unlock(&jrnl->wb_alloc);
-		}
-
-		do {
-			err = z_to_journal_list(znd, dm_s, lba, gfp);
-		} while (-EBUSY == err);
-	}
-
-	return err;
-}
-
-/**
  * z_discard_small() - Translate discard extents to map cache block mapping.
  * @znd: ZDM Instance
  * @tlba: Address being discarded.
@@ -2721,17 +2759,16 @@ static struct map_cache *mcache_alloc(struct zdm *znd, int type, gfp_t gfp)
 		mcache->map_content = type;
 		mcache->jcount = 0;
 		mcache->jsorted = 0;
-		mcache->jdata = ZDM_CALLOC(znd, Z_UNSORTED,
-			sizeof(*mcache->jdata), PG_08, gfp);
-
-		if (mcache->jdata) {
+		mcache->mcd = ZDM_ALLOC(znd, sizeof(struct map_cache_data),
+					PG_08, gfp);
+		if (mcache->mcd) {
+			struct map_cache_data *data = mcache->mcd;
 			u64 logical = Z_LOWER48;
 			u64 physical = Z_LOWER48;
 
-			mcache->jdata[0].tlba = cpu_to_le64(logical);
-			mcache->jdata[0].bval = cpu_to_le64(physical);
-			mcache->jsize = Z_UNSORTED - 1;
-
+			data->header.tlba = cpu_to_le64(logical);
+			data->header.bval = cpu_to_le64(physical);
+			mcache->jsize = ARRAY_SIZE(data->maps);
 		} else {
 			Z_ERR(znd, "%s: mcache is out of space.",
 			      __func__);
@@ -2858,8 +2895,10 @@ static void memcache_sort(struct zdm *znd, struct map_cache *mcache)
 {
 	mcache_ref(mcache);
 	if (mcache->jcount > 1 && mcache->jsorted < mcache->jcount) {
-		sort(&mcache->jdata[1], mcache->jcount,
-		     sizeof(*mcache->jdata), compare_tlba, NULL);
+		struct map_cache_data *mcd = get_mcd(mcache);
+		struct map_cache_entry *base = &mcd->maps[0];
+
+		sort(base, mcache->jcount, sizeof(*base), mcache_cmp, NULL);
 		mcache->jsorted = mcache->jcount;
 	}
 	mcache_deref(mcache);
@@ -2889,6 +2928,35 @@ static int memcache_lock_and_sort(struct zdm *znd, struct map_cache *mcache)
 }
 
 /**
+ * z_lookup_journal_cache() - Scan mcache entries for addr
+ * @znd: ZDM Instance
+ * @addr: Address [tLBA] to find.
+ * @type: mcache type (MAP or DISCARD cache).
+ */
+static u64 z_lookup_journal_cache(struct zdm *znd, u64 addr)
+{
+	u64 found = 0ul;
+	struct map_cache *mcache = &znd->jrnl_map;
+	int err = memcache_lock_and_sort(znd, mcache);
+	int at;
+
+	/* sort, if needed. only err is -EBUSY so do a linear find. */
+	if (!err)
+		at = _bsrch_tlba(mcache, addr);
+	else
+		at = _lsearch_tlba(mcache, addr);
+
+	if (at != -1) {
+		struct jrnl_map_cache_data *mcd = mcache->mcd;
+		struct map_cache_entry *entry = &mcd->maps[at];
+
+		found = le64_to_lba48(entry->bval, NULL);
+	}
+
+	return found;
+}
+
+/**
  * z_lookup_cache() - Scan mcache entries for addr
  * @znd: ZDM Instance
  * @addr: Address [tLBA] to find.
@@ -2912,7 +2980,8 @@ static u64 z_lookup_cache(struct zdm *znd, u64 addr, int type)
 			at = mcache_lsearch(mcache, addr);
 
 		if (at != -1) {
-			struct map_sect_to_lba *data = &mcache->jdata[at + 1];
+			struct map_cache_data *mcd = get_mcd(mcache);
+			struct map_cache_entry *data = &mcd->maps[at];
 
 			found = le64_to_lba48(data->bval, NULL);
 		}
@@ -2937,34 +3006,17 @@ out:
 static void mcache_insert(struct zdm *znd, struct map_cache *mcache,
 			 u64 tlba, u64 blba)
 {
-	u16 fido = ++mcache->jcount;
-	u16 idx;
+	struct map_cache_data *mcd = get_mcd(mcache);
+	u16 top = mcache->jcount;
 
-	WARN_ON(mcache->jcount > mcache->jsize);
+	WARN_ON(mcache->jcount >= mcache->jsize);
 
-	for (idx = fido; idx > 1; --idx) {
-		u16 prev = idx - 1;
-		u64 a0 = le64_to_lba48(mcache->jdata[prev].tlba, NULL);
+	mcd->maps[top].tlba = lba48_to_le64(0, tlba);
+	mcd->maps[top].bval = lba48_to_le64(0, blba);
 
-		if (a0 < tlba) {
-			mcache->jdata[idx].tlba = lba48_to_le64(0, tlba);
-			mcache->jdata[idx].bval = lba48_to_le64(0, blba);
-			if ((mcache->jsorted + 1) == mcache->jcount)
-				mcache->jsorted = mcache->jcount;
-			break;
-		}
-		/* move UP .. tlba < a0 */
-		mcache->jdata[idx].tlba = mcache->jdata[prev].tlba;
-		mcache->jdata[idx].bval = mcache->jdata[prev].bval;
-	}
-	if (idx == 1) {
-		mcache->jdata[idx].tlba = lba48_to_le64(0, tlba);
-		mcache->jdata[idx].bval = lba48_to_le64(0, blba);
-		mcache->jsorted = mcache->jcount;
-	}
+	++mcache->jcount;
 }
 
-
 /**
  * mc_delete_entry - Overwrite a map_cache entry (for mostly sorted data)
  * @mcache: Map Cache block
@@ -2974,26 +3026,37 @@ static __always_inline
 void mc_delete_entry_locked(struct map_cache *mcache, int entry)
 {
 	int iter;
+	int last = mcache->jcount - 1;
+	struct map_cache_data *mcd = get_mcd(mcache);
 
-	mcache->jdata[entry].tlba = lba48_to_le64(0, 0ul);
-	mcache->jdata[entry].bval = lba48_to_le64(0, 0ul);
+	if (mcache->jcount <= 0) {
+		pr_err("mcache delete %d ... discard below empty\n", entry);
+		dump_stack();
+		return;
+	}
+	/* if unsorted .. just fill the hole and leave. */
+	if (mcache->jsorted != mcache->jcount) {
+		mcd->maps[entry].tlba = mcd->maps[last].tlba;
+		mcd->maps[entry].bval = mcd->maps[last].bval;
+		mcache->jsorted = 0;
+		mcache->jcount--;
+		return;
+	}
 
-	for (iter = entry; iter < mcache->jcount; iter++) {
+	/* if sorted .. fill in the hole */
+	mcd->maps[entry].tlba = lba48_to_le64(0, 0ul);
+	mcd->maps[entry].bval = lba48_to_le64(0, 0ul);
+
+	for (iter = entry; iter < last; iter++) {
 		int next = iter + 1;
 
-		mcache->jdata[iter].tlba = mcache->jdata[next].tlba;
-		mcache->jdata[iter].bval = mcache->jdata[next].bval;
-	}
-	if (mcache->jcount > 0) {
-		if (mcache->jsorted == mcache->jcount)
-			mcache->jsorted--;
-		mcache->jcount--;
-	} else {
-		pr_err("mcache delete ... discard below empty\n");
+		mcd->maps[iter].tlba = mcd->maps[next].tlba;
+		mcd->maps[iter].bval = mcd->maps[next].bval;
 	}
+	mcache->jsorted--;
+	mcache->jcount--;
 }
 
-
 /**
  * mc_delete_entry - Overwrite a map_cache entry (for mostly sorted data)
  * @mcache: Map Cache block
@@ -3022,7 +3085,8 @@ static int _discard_split_entry(struct zdm *znd, struct map_cache *mcache,
 {
 	const int merge = 0;
 	int err = 0;
-	struct map_sect_to_lba *entry = &mcache->jdata[at + 1];
+	struct map_cache_data *mcd = get_mcd(mcache);
+	struct map_cache_entry *entry = &mcd->maps[at];
 	u64 tlba   = le64_to_lba48(entry->tlba, NULL);
 	u64 blocks = le64_to_lba48(entry->bval, NULL);
 	u64 dblks  = blocks;
@@ -3047,7 +3111,7 @@ static int _discard_split_entry(struct zdm *znd, struct map_cache *mcache,
 		entry->bval = lba48_to_le64(0, blocks);
 
 		if (blocks == 0ul) {
-			mc_delete_entry(mcache, at + 1);
+			mc_delete_entry(mcache, at);
 			goto out;
 		}
 		dblks = blocks;
@@ -3089,7 +3153,7 @@ static int _discard_split_entry(struct zdm *znd, struct map_cache *mcache,
 		entry->tlba = lba48_to_le64(0, tlba);
 		entry->bval = lba48_to_le64(0, blocks);
 		if (blocks == 0ul)
-			mc_delete_entry(mcache, at + 1);
+			mc_delete_entry(mcache, at);
 	} else {
 		u64 start = addr + dblks;
 
@@ -3196,9 +3260,9 @@ static int z_discard_partial(struct zdm *znd, u32 minblks, gfp_t gfp)
 
 		if (at != -1) {
 			const int nodisc = 1;
-			s32 e = at + 1;
-			u64 tlba = le64_to_lba48(mcache->jdata[e].tlba, NULL);
-			u64 blks = le64_to_lba48(mcache->jdata[e].bval, NULL);
+			struct map_cache_data *mcd = get_mcd(mcache);
+			u64 tlba = le64_to_lba48(mcd->maps[at].tlba, NULL);
+			u64 blks = le64_to_lba48(mcd->maps[at].bval, NULL);
 			u64 chunk = DISCARD_MAX_INGRESS - 1;
 			u32 dcount = 0;
 			u32 dstop = minblks;
@@ -3223,7 +3287,7 @@ static int z_discard_partial(struct zdm *znd, u32 minblks, gfp_t gfp)
 				chunk = blks;
 
 			if (chunk == 0) {
-				mc_delete_entry(mcache, e);
+				mc_delete_entry(mcache, at);
 				completions++;
 			} else if (chunk < DISCARD_MAX_INGRESS) {
 				err = mapped_discard(znd, tlba, chunk, 0, gfp);
@@ -3234,10 +3298,10 @@ static int z_discard_partial(struct zdm *znd, u32 minblks, gfp_t gfp)
 				blks -= chunk;
 				tlba += chunk;
 
-				mcache->jdata[e].tlba = lba48_to_le64(0, tlba);
-				mcache->jdata[e].bval = lba48_to_le64(0, blks);
+				mcd->maps[at].tlba = lba48_to_le64(0, tlba);
+				mcd->maps[at].bval = lba48_to_le64(0, blks);
 				if (blks == 0ul)
-					mc_delete_entry(mcache, e);
+					mc_delete_entry(mcache, at);
 			}
 			completions++;
 		}
@@ -3253,7 +3317,7 @@ static int z_discard_partial(struct zdm *znd, u32 minblks, gfp_t gfp)
 			mcache_put(mcache);
 
 			if (deleted) {
-				ZDM_FREE(znd, mcache->jdata, Z_C4K, PG_08);
+				ZDM_FREE(znd, mcache->mcd, Z_C4K, PG_08);
 				ZDM_FREE(znd, mcache, sizeof(*mcache), KM_07);
 				mcache = NULL;
 				znd->dc_entries--;
@@ -3278,13 +3342,14 @@ out:
  */
 static int lba_in_zone(struct zdm *znd, struct map_cache *mcache, u32 zone)
 {
-	int jentry;
+	int idx;
+	struct map_cache_data *mcd = get_mcd(mcache);
 
 	if (zone >= znd->data_zones)
 		goto out;
 
-	for (jentry = mcache->jcount; jentry > 0; jentry--) {
-		u64 lba = le64_to_lba48(mcache->jdata[jentry].bval, NULL);
+	for (idx = 0; idx < mcache->jcount; idx++) {
+		u64 lba = le64_to_lba48(mcd->maps[idx].bval, NULL);
 
 		if (lba && _calc_zone(znd, lba) == zone)
 			return 1;
@@ -3390,7 +3455,7 @@ static int __cached_to_tables(struct zdm *znd, int type, u32 zone)
 		mcache_deref(mcache);
 
 		if (deleted) {
-			ZDM_FREE(znd, mcache->jdata, Z_C4K, PG_08);
+			ZDM_FREE(znd, mcache->mcd, Z_C4K, PG_08);
 			ZDM_FREE(znd, mcache, sizeof(*mcache), KM_07);
 			mcache = NULL;
 			znd->mc_entries--;
@@ -3402,7 +3467,6 @@ out:
 	return err;
 }
 
-
 /**
  * _cached_to_tables() - Migrate memcache entries to lookup tables
  * @znd: ZDM instance
@@ -3423,7 +3487,6 @@ static int _cached_to_tables(struct zdm *znd, u32 zone)
 	return err;
 }
 
-
 /**
  * z_flush_bdev() - Request backing device flushed to disk.
  * @znd: ZDM instance
@@ -3554,7 +3617,7 @@ static int manage_lazy_activity(struct zdm *znd)
 			set_bit(DELAY_ADD, &expg->flags);
 
 		if (test_bit(WB_RE_CACHE, &expg->flags)) {
-			if (expg->last_write && expg->lba != expg->last_write) {
+			if (!expg->data.addr) {
 				if (entries < MAX_WSET) {
 					ref_pg(expg);
 					wset[entries] = expg;
@@ -3562,7 +3625,8 @@ static int manage_lazy_activity(struct zdm *znd)
 					clear_bit(WB_RE_CACHE, &expg->flags);
 				}
 			} else {
-				/* block is not in wb-journal */
+				set_bit(IS_DIRTY, &expg->flags);
+				clear_bit(IS_FLUSH, &expg->flags);
 				clear_bit(WB_RE_CACHE, &expg->flags);
 			}
 		}
@@ -3623,8 +3687,9 @@ static inline void pg_toggle_wb_journal(struct zdm *znd, struct map_pg *expg)
 {
 	if (test_and_clear_bit(WB_JRNL_2, &expg->flags)) {
 		/* migrating from journal -> home, must return home */
-		set_bit(WB_DIRECT,  &expg->flags);
-		set_bit(WB_RE_CACHE,  &expg->flags);
+		set_bit(WB_DIRECT, &expg->flags);
+		if (test_and_clear_bit(IN_WB_JOURNAL, &expg->flags))
+			set_bit(WB_RE_CACHE, &expg->flags);
 	} else if (test_and_clear_bit(WB_JRNL_1, &expg->flags)) {
 		/* migrating from 1 to 2 is okay stay in journal */
 		set_bit(WB_JRNL_2,  &expg->flags);
@@ -3903,19 +3968,19 @@ static void meta_work_task(struct work_struct *work)
 static int _update_entry(struct zdm *znd, struct map_cache *mcache, int at,
 			 u64 dm_s, u64 lba, gfp_t gfp)
 {
-	struct map_sect_to_lba *data;
+	struct map_cache_data *mcd = get_mcd(mcache);
+	struct map_cache_entry *entry = &mcd->maps[at];
 	u64 lba_was;
 	int err = 0;
 
-	data = &mcache->jdata[at + 1];
-	lba_was = le64_to_lba48(data->bval, NULL);
+	lba_was = le64_to_lba48(entry->bval, NULL);
 	lba &= Z_LOWER48;
-	data->bval = lba48_to_le64(0, lba);
+	entry->bval = lba48_to_le64(0, lba);
 
 	if (lba != lba_was) {
 		Z_DBG(znd, "Remap %" PRIx64 " -> %" PRIx64
 			   " (was %" PRIx64 "->%" PRIx64 ")",
-		      dm_s, lba, le64_to_lba48(data->tlba, NULL), lba_was);
+		      dm_s, lba, le64_to_lba48(entry->tlba, NULL), lba_was);
 		err = unused_phy(znd, lba_was, 0, gfp);
 		if (err == 1)
 			err = 0;
@@ -3935,60 +4000,18 @@ static int _update_entry(struct zdm *znd, struct map_cache *mcache, int at,
 static int _update_disc(struct zdm *znd, struct map_cache *mcache, int at,
 			u64 dm_s, u64 blks)
 {
-	struct map_sect_to_lba *data;
+	struct map_cache_data *mcd = get_mcd(mcache);
+	struct map_cache_entry *entry = &mcd->maps[at];
 	u64 oldsz;
 	int err = 0;
 
-	data = &mcache->jdata[at + 1];
-	oldsz = le64_to_lba48(data->bval, NULL);
-	data->bval = lba48_to_le64(0, blks);
+	oldsz = le64_to_lba48(entry->bval, NULL);
+	entry->bval = lba48_to_le64(0, blks);
 
 	return err;
 }
 
 /**
- * _update_journal() - Update an existing map cache entry.
- * @znd: ZDM instance
- * @mcache: Map cache block
- * @at: Data entry in map cache block
- * @dm_s: tBLA mapping from
- * @lba: New lba
- * @gfp: Allocation flags to use.
- */
-static int _update_journal(struct zdm *znd, struct map_cache *mcache, int at,
-			   u64 dm_s, u64 lba)
-{
-	struct map_sect_to_lba *data;
-	u64 lba_was;
-
-	data = &mcache->jdata[at + 1];
-	lba_was = le64_to_lba48(data->bval, NULL);
-	lba &= Z_LOWER48;
-	data->bval = lba48_to_le64(0, lba);
-
-	if (!lba)
-		mc_delete_entry_locked(mcache, at + 1);
-
-	if (lba_was) {
-		struct md_journal *jrnl = &znd->jrnl;
-		u32 entry = lba_was - WB_JRNL_BASE;
-
-		if (entry >= jrnl->size) {
-			Z_ERR(znd, "BAD WB Journal! Got %" PRIx64 " max %x",
-			           lba_was, jrnl->size);
-			return -EIO;
-		}
-		SpinLock(&jrnl->wb_alloc);
-		jrnl->in_use--;
-		jrnl->wb[entry] = MZTEV_UNUSED;
-		set_bit(IS_DIRTY, &jrnl->flags);
-		spin_unlock(&jrnl->wb_alloc);
-	}
-
-	return 0;
-}
-
-/**
  * mc_update() - Update an existing cache entry
  * @znd: ZDM instance
  * @mcache: Map cache block
@@ -4007,8 +4030,6 @@ static int mc_update(struct zdm *znd, struct map_cache *mcache, int at,
 		rcode = _update_entry(znd, mcache, at, dm_s, lba, gfp);
 	else if (type == IS_DISCARD)
 		rcode = _update_disc(znd, mcache, at, dm_s, lba);
-	else if (type == IS_JRNL_PG)
-		rcode = _update_journal(znd, mcache, at, dm_s, lba);
 	else
 		dump_stack();
 
@@ -4023,8 +4044,7 @@ static int mc_update(struct zdm *znd, struct map_cache *mcache, int at,
  * @type: List (type) to be adding to (MAP or DISCARD)
  * @gfp: Allocation (kmalloc) flags
  */
-static
-int _mapped_list(struct zdm *znd, u64 dm_s, u64 lba, int type, gfp_t gfp)
+static int _mapped_list(struct zdm *znd, u64 dm_s, u64 lba, int type, gfp_t gfp)
 {
 	struct map_cache *mcache = NULL;
 	struct map_cache *mc_add = NULL;
@@ -4091,18 +4111,17 @@ int _mapped_list(struct zdm *znd, u64 dm_s, u64 lba, int type, gfp_t gfp)
 
 	if (mc_add) {
 		MutexLock(&mc_add->cached_lock);
-
-		if (!mc_add->jdata) {
-			mc_add->jdata = ZDM_CALLOC(znd, Z_UNSORTED,
-				sizeof(*mc_add->jdata), PG_08, gfp);
+		if (!mc_add->mcd) {
+			mc_add->mcd = ZDM_ALLOC(znd,
+						sizeof(struct map_cache_data),
+						PG_08, gfp);
 		}
-		if (!mc_add->jdata) {
+		if (!mc_add->mcd) {
 			Z_ERR(znd, "%s: in memory journal is out of space.",
 			      __func__);
 			err = -ENOMEM;
 			goto out;
 		}
-
 		if (mc_add->jcount < mc_add->jsize) {
 			mcache_insert(znd, mc_add, dm_s, lba);
 			if (add_to_list)
@@ -4142,25 +4161,6 @@ static int z_to_map_list(struct zdm *znd, u64 dm_s, u64 lba, gfp_t gfp)
 }
 
 /**
- * z_to_journal_list() - Add a map cache entry to the map cache
- * @znd: ZDM instance
- * @dm_s: tBLA mapping from
- * @lba: bLBA mapping to
- * @gfp: Allocation (kmalloc) flags
- */
-static int z_to_journal_list(struct zdm *znd, u64 dm_s, u64 lba, gfp_t gfp)
-{
-	if (lba != 0ul) {
-		u32 entry = lba - WB_JRNL_BASE;
-
-		if (entry >= znd->jrnl.size)
-			Z_ERR(znd, "Warning: Invalid bLBA: %"PRIx64, lba);
-	}
-
-	return _mapped_list(znd, dm_s, lba, IS_JRNL_PG, gfp);
-}
-
-/**
  * discard_merge() - Merge a discard request with a existing entry.
  * @znd: ZDM Instance
  * @tlba: Starting address
@@ -4211,11 +4211,13 @@ static int discard_merge(struct zdm *znd, u64 tlba, u64 blks)
 		}
 		at = _bsrch_tlba(mcache, ends);
 		if (at != -1) {
-			struct map_sect_to_lba *data;
+			struct map_cache_data *mcd;
+			struct map_cache_entry *data;
 			u64 oldsz;
 
 			mcache_ref(mcache);
-			data = &mcache->jdata[at + 1];
+			mcd = get_mcd(mcache);
+			data = &mcd->maps[at];
 			data->tlba = lba48_to_le64(0, tlba);
 			oldsz = le64_to_lba48(data->bval, NULL);
 			data->bval = lba48_to_le64(0, oldsz + blks);
@@ -4237,13 +4239,15 @@ static int discard_merge(struct zdm *znd, u64 tlba, u64 blks)
 				at = _bsrch_extent_lck(mcache, ends);
 		}
 		if (at != -1) {
-			struct map_sect_to_lba *data;
+			struct map_cache_data *mcd;
+			struct map_cache_entry *data;
 			u64 oldsz;
 			u64 origin;
 			u64 extend;
 
 			mcache_ref(mcache);
-			data = &mcache->jdata[at + 1];
+			mcd = get_mcd(mcache);
+			data = &mcd->maps[at];
 			origin = le64_to_lba48(data->tlba, NULL);
 			oldsz = le64_to_lba48(data->bval, NULL);
 			extend = (tlba - origin) + blks;
@@ -4460,15 +4464,17 @@ static int z_mapped_sync(struct zdm *znd)
 
 		mc = mcache_first_get(znd, no);
 		while (mc) {
-			u64 phy = le64_to_lba48(mc->jdata[0].bval, NULL);
+			struct map_cache_data *mcd = mc->mcd;
+
+			u64 phy = le64_to_lba48(mcd->header.bval, NULL);
 			u16 jcount = mc->jcount & 0xFFFF;
 
 			if (jcount) {
-				mc->jdata[0].bval = lba48_to_le64(jcount, phy);
-				znd->bmkeys->crcs[idx] = crc_md_le16(mc->jdata,
-								     Z_CRC_4K);
+				mcd->header.bval = lba48_to_le64(jcount, phy);
+				znd->bmkeys->crcs[idx] =
+						crc_md_le16(mcd, Z_CRC_4K);
 				idx++;
-				bio_add_km(bio, mc->jdata, 1);
+				bio_add_km(bio, mcd, 1);
 				if (wset_count < SYNC_MAX) {
 					wset[wset_count] = mc;
 					wset_count++;
@@ -4547,7 +4553,8 @@ static int z_mapped_sync(struct zdm *znd)
 			mcache_deref(mc);
 	}
 	mark_clean_flush(znd, 1);
-	Z_DBG(znd, "Sync/Flush: %d writes / %d blocks written", n_writes, n_blocks);
+	Z_DBG(znd, "Sync/Flush: %d writes / %d blocks written [gen %"PRIx64"]",
+		n_writes, n_blocks, generation);
 out:
 	if (wset)
 		ZDM_FREE(znd, wset, sizeof(*wset) * SYNC_MAX, KM_18);
@@ -4816,29 +4823,32 @@ static int do_load_cache(struct zdm *znd, int type, u64 lba, int idx, int wq)
 	int rc = 0;
 	int blks = 1;
 	struct map_cache *mcache = mcache_alloc(znd, type, NORMAL);
+	struct map_cache_data *mcd;
 
 	if (!mcache) {
 		rc = -ENOMEM;
 		goto out;
 	}
 
-	rc = read_block(znd->ti, DM_IO_KMEM, mcache->jdata, lba, blks, wq);
+	mcd = get_mcd(mcache);
+	rc = read_block(znd->ti, DM_IO_KMEM, mcd, lba, blks, wq);
 	if (rc) {
 		Z_ERR(znd, "%s: mcache-> %" PRIu64
 			   " [%d blks] %p -> %d",
-		      __func__, lba, blks, mcache->jdata, rc);
+		      __func__, lba, blks, mcd, rc);
 
-		ZDM_FREE(znd, mcache->jdata, Z_C4K, PG_08);
+		ZDM_FREE(znd, mcd, Z_C4K, PG_08);
 		ZDM_FREE(znd, mcache, sizeof(*mcache), KM_07);
 		goto out;
 	}
-	crc = crc_md_le16(mcache->jdata, Z_CRC_4K);
+	crc = crc_md_le16(mcd, Z_CRC_4K);
 	if (crc != znd->bmkeys->crcs[idx]) {
 		rc = -EIO;
 		Z_ERR(znd, "%s: bad crc %" PRIu64,  __func__, lba);
 		goto out;
 	}
-	(void)le64_to_lba48(mcache->jdata[0].bval, &count);
+
+	(void)le64_to_lba48(mcd->header.bval, &count);
 	mcache->jcount = count;
 	mclist_add(znd, mcache, type);
 
@@ -5033,16 +5043,8 @@ static int z_mapped_init(struct zdm *znd)
 		blba = idx + WB_JRNL_BASE;
 		tlba = le32_to_cpu(jrnl->wb[idx]);
 
-		Z_ERR(znd, " .. restore map: %"PRIx64 " -> %"PRIx64,
-		      tlba, blba);
-
-		/* Create the journal mcache entries */
-		do {
-			rc = z_to_journal_list(znd, tlba, blba, gfp);
-		} while (-EBUSY == rc);
-
-		if (rc)
-			goto out;
+		Z_ERR(znd, " .. redux map: %" PRIx64 " -> %"PRIx64, tlba, blba);
+		mcache_insert(znd, &znd->jrnl_map, tlba, blba);
 	}
 
 	for (idx = 0; idx < jrnl->size; idx++) {
@@ -5059,6 +5061,7 @@ static int z_mapped_init(struct zdm *znd)
 		if (!expg)
 			goto out;
 
+		set_bit(IN_WB_JOURNAL, &expg->flags);
 		expg->last_write = idx + WB_JRNL_BASE;
 		pg_toggle_wb_journal(znd, expg);
 		put_map_entry(expg);
@@ -5483,12 +5486,11 @@ static int gc_post_add(struct zdm *znd, u64 addr, u64 lba)
 		return handled;
 
 	if (post->jcount < post->jsize) {
-		int idx = ++post->jcount;
-
-		WARN_ON(post->jcount > post->jsize);
+		struct gc_map_cache_data *data = post->mcd;
 
-		post->jdata[idx].tlba = lba48_to_le64(0, addr);
-		post->jdata[idx].bval = lba48_to_le64(0, lba);
+		data->maps[post->jcount].tlba = lba48_to_le64(0, addr);
+		data->maps[post->jcount].bval = lba48_to_le64(0, lba);
+		post->jcount++;
 		handled = 1;
 	} else {
 		Z_ERR(znd, "*CRIT* post overflow L:%" PRIx64 "-> S:%" PRIx64,
@@ -5624,52 +5626,52 @@ static int z_zone_gc_read(struct gc_state *gc_entry)
 	struct zdm *znd = gc_entry->znd;
 	struct io_4k_block *io_buf = znd->gc_io_buf;
 	struct map_cache *post = &znd->gc_postmap;
+	struct gc_map_cache_data *mcd = post->mcd;
 	unsigned long flags;
 	u64 start_lba;
 	int nblks;
 	int rcode = 0;
 	int fill = 0;
 	int jstart;
-	int jentry;
+	int idx;
 
 	spin_lock_irqsave(&znd->gc_lock, flags);
 	jstart = gc_entry->r_ptr;
 	spin_unlock_irqrestore(&znd->gc_lock, flags);
 
-	if (!jstart)
-		jstart++;
-
 	MutexLock(&post->cached_lock);
 
 	/* A discard may have puched holes in the postmap. re-sync lba */
-	jentry = jstart;
-	while (jentry <= post->jcount && (Z_LOWER48 ==
-			le64_to_lba48(post->jdata[jentry].bval, NULL))) {
-		jentry++;
+	idx = jstart;
+	while (idx < post->jcount &&
+	       Z_LOWER48 == le64_to_lba48(mcd->maps[idx].bval, NULL)) {
+		idx++;
 	}
 	/* nothing left to move */
-	if (jentry > post->jcount)
+	if (idx >= post->jcount)
 		goto out_finished;
 
 	/* skip over any discarded blocks */
-	if (jstart != jentry)
-		jstart = jentry;
+	if (jstart != idx)
+		jstart = idx;
 
-	start_lba = le64_to_lba48(post->jdata[jentry].bval, NULL);
-	post->jdata[jentry].bval = lba48_to_le64(GC_READ, start_lba);
+	/* schedule the first block */
+	start_lba = le64_to_lba48(mcd->maps[idx].bval, NULL);
+	mcd->maps[idx].bval = lba48_to_le64(GC_READ, start_lba);
 	nblks = 1;
-	jentry++;
+	idx++;
 
-	while (jentry <= post->jcount && (nblks+fill) < GC_MAX_STRIPE) {
-		u64 dm_s = le64_to_lba48(post->jdata[jentry].tlba, NULL);
-		u64 lba = le64_to_lba48(post->jdata[jentry].bval, NULL);
+	/* add any contiguous blocks */
+	while (idx < post->jcount && (nblks+fill) < GC_MAX_STRIPE) {
+		u64 dm_s = le64_to_lba48(mcd->maps[idx].tlba, NULL);
+		u64 lba = le64_to_lba48(mcd->maps[idx].bval, NULL);
 
 		if (Z_LOWER48 == dm_s || Z_LOWER48 == lba) {
-			jentry++;
+			idx++;
 			continue;
 		}
 
-		post->jdata[jentry].bval = lba48_to_le64(GC_READ, lba);
+		mcd->maps[idx].bval = lba48_to_le64(GC_READ, lba);
 
 		/* if the block is contiguous add it to the read */
 		if (lba == (start_lba + nblks)) {
@@ -5689,7 +5691,7 @@ static int z_zone_gc_read(struct gc_state *gc_entry)
 			start_lba = lba;
 			nblks = 1;
 		}
-		jentry++;
+		idx++;
 	}
 
 	/* Issue a copy of 'nblks' blocks */
@@ -5709,7 +5711,7 @@ out_finished:
 
 	spin_lock_irqsave(&znd->gc_lock, flags);
 	gc_entry->nblks = fill;
-	gc_entry->r_ptr = jentry;
+	gc_entry->r_ptr = idx;
 	if (fill > 0)
 		set_bit(DO_GC_WRITE, &gc_entry->gc_flags);
 	else
@@ -5734,22 +5736,20 @@ static int z_zone_gc_write(struct gc_state *gc_entry, u32 stream_id)
 	struct dm_target *ti = znd->ti;
 	struct io_4k_block *io_buf = znd->gc_io_buf;
 	struct map_cache *post = &znd->gc_postmap;
+	struct gc_map_cache_data *mcd = post->mcd;
 	unsigned long flags;
 	u32 aq_flags = Z_AQ_GC | Z_AQ_STREAM_ID | stream_id;
 	u64 lba;
 	u32 nblks;
 	u32 out = 0;
 	int err = 0;
-	int jentry;
+	int idx;
 
 	spin_lock_irqsave(&znd->gc_lock, flags);
-	jentry = gc_entry->w_ptr;
+	idx = gc_entry->w_ptr;
 	nblks = gc_entry->nblks;
 	spin_unlock_irqrestore(&znd->gc_lock, flags);
 
-	if (!jentry)
-		jentry++;
-
 	MutexLock(&post->cached_lock);
 	while (nblks > 0) {
 		u32 nfound = 0;
@@ -5782,15 +5782,13 @@ static int z_zone_gc_write(struct gc_state *gc_entry, u32 stream_id)
 		}
 		out += nfound;
 
-		while ((jentry <= post->jcount) && (added < nfound)) {
+		while ((idx < post->jcount) && (added < nfound)) {
 			u16 rflg;
-			u64 orig = le64_to_lba48(
-					post->jdata[jentry].bval, &rflg);
-			u64 dm_s = le64_to_lba48(
-					post->jdata[jentry].tlba, NULL);
+			u64 orig = le64_to_lba48(mcd->maps[idx].bval, &rflg);
+			u64 dm_s = le64_to_lba48(mcd->maps[idx].tlba, NULL);
 
 			if ((Z_LOWER48 == dm_s || Z_LOWER48 == orig)) {
-				jentry++;
+				idx++;
 
 				if (rflg & GC_READ) {
 					Z_ERR(znd, "ERROR: %" PRIx64
@@ -5802,10 +5800,10 @@ static int z_zone_gc_write(struct gc_state *gc_entry, u32 stream_id)
 				continue;
 			}
 			rflg &= ~GC_READ;
-			post->jdata[jentry].bval = lba48_to_le64(rflg, lba);
+			mcd->maps[idx].bval = lba48_to_le64(rflg, lba);
 			lba++;
 			added++;
-			jentry++;
+			idx++;
 		}
 		nblks -= nfound;
 	}
@@ -5815,7 +5813,7 @@ static int z_zone_gc_write(struct gc_state *gc_entry, u32 stream_id)
 out:
 	spin_lock_irqsave(&znd->gc_lock, flags);
 	gc_entry->nblks = 0;
-	gc_entry->w_ptr = jentry;
+	gc_entry->w_ptr = idx;
 	spin_unlock_irqrestore(&znd->gc_lock, flags);
 	mutex_unlock(&post->cached_lock);
 
@@ -5834,22 +5832,23 @@ static int gc_finalize(struct gc_state *gc_entry)
 	int err = 0;
 	struct zdm *znd = gc_entry->znd;
 	struct map_cache *post = &znd->gc_postmap;
-	int jentry;
+	struct gc_map_cache_data *mcd = post->mcd;
+	int idx;
 
 	MutexLock(&post->cached_lock);
-	for (jentry = post->jcount; jentry > 0; jentry--) {
-		u64 dm_s = le64_to_lba48(post->jdata[jentry].tlba, NULL);
-		u64 lba = le64_to_lba48(post->jdata[jentry].bval, NULL);
+	for (idx = 0; idx < post->jcount; idx++) {
+		u64 dm_s = le64_to_lba48(mcd->maps[idx].tlba, NULL);
+		u64 lba = le64_to_lba48(mcd->maps[idx].bval, NULL);
 
 		if (dm_s != Z_LOWER48 || lba != Z_LOWER48) {
 			Z_ERR(znd, "GC: Failed to move %" PRIx64
 				   " from %"PRIx64" [%d]",
-			      dm_s, lba, jentry);
+			      dm_s, lba, idx);
 			err = -EIO;
 		}
 	}
 	mutex_unlock(&post->cached_lock);
-	post->jcount = jentry;
+	post->jcount = 0;
 	post->jsorted = 0;
 
 	return err;
@@ -5896,16 +5895,17 @@ static int z_zone_gc_metadata_update(struct gc_state *gc_entry)
 {
 	struct zdm *znd = gc_entry->znd;
 	struct map_cache *post = &znd->gc_postmap;
+	struct gc_map_cache_data *mcd = post->mcd;
 	u32 used = post->jcount;
 	int err = 0;
-	int jentry;
+	int idx;
 
-	for (jentry = post->jcount; jentry > 0; jentry--) {
+	for (idx = 0; idx < post->jcount; idx++) {
 		int discard = 0;
 		int mapping = 0;
 		struct map_pg *mapped = NULL;
-		u64 dm_s = le64_to_lba48(post->jdata[jentry].tlba, NULL);
-		u64 lba = le64_to_lba48(post->jdata[jentry].bval, NULL);
+		u64 dm_s = le64_to_lba48(mcd->maps[idx].tlba, NULL);
+		u64 lba = le64_to_lba48(mcd->maps[idx].bval, NULL);
 		struct mpinfo mpi;
 
 		if ((znd->s_base <= dm_s) && (dm_s < znd->md_end)) {
@@ -5947,14 +5947,14 @@ static int z_zone_gc_metadata_update(struct gc_state *gc_entry)
 		MutexLock(&post->cached_lock);
 		if (discard == 1) {
 			Z_ERR(znd, "Dropped: %" PRIx64 " ->  %"PRIx64,
-			      le64_to_cpu(post->jdata[jentry].tlba),
-			      le64_to_cpu(post->jdata[jentry].bval));
+			      le64_to_cpu(mcd->maps[idx].tlba),
+			      le64_to_cpu(mcd->maps[idx].bval));
 
-			post->jdata[jentry].tlba = MC_INVALID;
-			post->jdata[jentry].bval = MC_INVALID;
+			mcd->maps[idx].tlba = MC_INVALID;
+			mcd->maps[idx].bval = MC_INVALID;
 		}
-		if (post->jdata[jentry].tlba  == MC_INVALID &&
-		    post->jdata[jentry].bval == MC_INVALID) {
+		if (mcd->maps[idx].tlba  == MC_INVALID &&
+		    mcd->maps[idx].bval == MC_INVALID) {
 			used--;
 		} else if (lba) {
 			increment_used_blks(znd, lba, 1);
@@ -6874,13 +6874,13 @@ out:
 }
 
 /**
- * compare_lba() - Compare map page on lba.
+ * wset_cmp_wr() - Compare map page on lba.
  * @x1: map page
  * @x2: map page
  *
  * Return -1, 0, or 1 if x1 < x2, equal, or >, respectivly.
  */
-static int compare_lba(const void *x1, const void *x2)
+static int wset_cmp_wr(const void *x1, const void *x2)
 {
 	const struct map_pg *v1 = *(const struct map_pg **)x1;
 	const struct map_pg *v2 = *(const struct map_pg **)x2;
@@ -6890,18 +6890,18 @@ static int compare_lba(const void *x1, const void *x2)
 }
 
 /**
- * cmp_current_lba() - Compare map page on lba.
+ * wset_cmp_rd() - Compare map page on lba.
  * @x1: map page
  * @x2: map page
  *
  * Return -1, 0, or 1 if x1 < x2, equal, or >, respectivly.
  */
-static int cmp_current_lba(const void *x1, const void *x2)
+static int wset_cmp_rd(const void *x1, const void *x2)
 {
 	const struct map_pg *v1 = *(const struct map_pg **)x1;
 	const struct map_pg *v2 = *(const struct map_pg **)x2;
 	int cmp = (v1->last_write < v2->last_write) ? -1
-		: ((v1->last_write > v2->last_write) ? 1 : compare_lba(x1, x2));
+		: ((v1->last_write > v2->last_write) ? 1 : wset_cmp_wr(x1, x2));
 
 	return cmp;
 }
@@ -6967,7 +6967,7 @@ static int _pool_write(struct zdm *znd, struct map_pg **wset, int count)
 		goto out;
 
 	if (count > 1)
-		sort(wset, count, sizeof(*wset), compare_lba, NULL);
+		sort(wset, count, sizeof(*wset), wset_cmp_wr, NULL);
 
 	for (iter = 0; iter < count; iter++) {
 		expg = wset[iter];
@@ -7020,7 +7020,7 @@ static int _pool_read(struct zdm *znd, struct map_pg **wset, int count)
 		goto out;
 
 	if (count > 1)
-		sort(wset, count, sizeof(*wset), cmp_current_lba, NULL);
+		sort(wset, count, sizeof(*wset), wset_cmp_rd, NULL);
 
 	for (iter = 0; iter < count; iter++) {
 		expg = wset[iter];
@@ -7050,17 +7050,117 @@ out:
 }
 
 /**
+ * md_journal_add_map() - Add an entry to the map cache block mapping.
+ * @znd: ZDM Instance
+ * @addr: Address being added to journal.
+ * @lba: bLBA addr is being mapped to (0 to delete the map)
+ *
+ * Add a new journal wb entry.
+ */
+static int md_journal_add_map(struct zdm *znd, u64 addr, u64 lba)
+{
+	struct map_cache *mcache = &znd->jrnl_map;
+	struct jrnl_map_cache_data *mcd = mcache->mcd;
+	struct md_journal *jrnl = &znd->jrnl;
+	int err = 0;
+	u32 entry;
+	int at;
+
+	if (addr < znd->data_lba) {
+		int next;
+
+		SpinLock(&jrnl->wb_alloc);
+		for (next = 0; next < jrnl->size; next++) {
+			if (le32_to_cpu(jrnl->wb[next]) == addr) {
+				jrnl->wb[next] = MZTEV_UNUSED;
+				if (jrnl->in_use > 0)
+					jrnl->in_use--;
+			}
+		}
+		spin_unlock(&jrnl->wb_alloc);
+
+		/* lba 0 => find addr and delete it */
+		if (lba == 0) {
+			MutexLock(&mcache->cached_lock);
+			memcache_sort(znd, mcache);
+			at = _bsrch_tlba(mcache, addr);
+			if (at != -1) {
+				struct map_cache_entry *mce = &mcd->maps[at];
+				u64 tlba = le64_to_lba48(mce->tlba, NULL);
+				u64 blba = le64_to_lba48(mce->bval, NULL);
+
+				mc_delete_entry_locked(mcache, at);
+				lba = blba;
+				addr = 0u;
+			}
+			mutex_unlock(&mcache->cached_lock);
+		}
+		entry = lba - WB_JRNL_BASE;
+
+		/* record the entry in the WB Journal */
+		if (entry < jrnl->size) {
+			SpinLock(&jrnl->wb_alloc);
+			if (jrnl->wb[entry] != MZTEV_UNUSED && addr == 0u &&
+			    jrnl->in_use > 0)
+				jrnl->in_use--;
+
+			if (addr == 0u)
+				jrnl->wb[entry] = MZTEV_UNUSED;
+			else
+				jrnl->wb[entry] = cpu_to_le32((u32)addr);
+
+			set_bit(IS_DIRTY, &jrnl->flags);
+			spin_unlock(&jrnl->wb_alloc);
+		} else {
+			lba = 0;
+		}
+
+		/* Add a mapping addr -> lba for quick lookups */
+		if (lba != 0 && addr != 0u) {
+			MutexLock(&mcache->cached_lock);
+			memcache_sort(znd, mcache);
+			at = _bsrch_tlba(mcache, addr);
+			if (at != -1) {
+				struct map_cache_entry *mce = &mcd->maps[at];
+				u64 tlba = le64_to_lba48(mce->tlba, NULL);
+				u64 blba = le64_to_lba48(mce->bval, NULL);
+				u32 again = blba - WB_JRNL_BASE;
+
+				if (again < jrnl->size) {
+					SpinLock(&jrnl->wb_alloc);
+					if (jrnl->wb[entry] != MZTEV_UNUSED &&
+					    jrnl->in_use > 0)
+						jrnl->in_use--;
+					jrnl->wb[entry] = MZTEV_UNUSED;
+					set_bit(IS_DIRTY, &jrnl->flags);
+					spin_unlock(&jrnl->wb_alloc);
+				}
+				/* update existing map */
+				mce->bval = lba48_to_le64(0, lba);
+			} else {
+				/* add a new map */
+				mcache_insert(znd, mcache, addr, lba);
+			}
+			mutex_unlock(&mcache->cached_lock);
+		}
+	}
+
+	return err;
+}
+
+/**
  * journal_alloc() - Grab the next available LBA for the journal.
  * @znd: ZDM Instance
+ * @addr: Address being reserved.
  * @nblks: Number of blocks desired.
  * @num: Number of blocks allocated.
  *
  * Return: lba or 0 on failure.
  */
-static u64 journal_alloc(struct zdm *znd, u32 nblks, u32 *num)
+static u64 journal_alloc(struct zdm *znd, u64 addr, u32 nblks, u32 *num)
 {
 	struct md_journal *jrnl = &znd->jrnl;
-	u64 tlba = 0ul;
+	u64 blba = 0ul;
 	u32 next;
 	int retry = 2;
 
@@ -7071,7 +7171,7 @@ static u64 journal_alloc(struct zdm *znd, u32 nblks, u32 *num)
 			if (jrnl->wb[next] != MZTEV_UNUSED)
 				continue;
 
-			tlba = WB_JRNL_BASE + next;
+			blba = WB_JRNL_BASE + next;
 			jrnl->wb[next] = MZTEV_NF; /* in flight */
 			jrnl->in_use++;
 			jrnl->wb_next = next + 1;
@@ -7079,15 +7179,24 @@ static u64 journal_alloc(struct zdm *znd, u32 nblks, u32 *num)
 			goto out_unlock;
 		}
 		next = 0;
-	} while (tlba == 0 && --retry > 0);
+	} while (blba == 0 && --retry > 0);
 
 out_unlock:
+
+//	for (next = 0; next < jrnl->size; next++) {
+//		if (le32_to_cpu(jrnl->wb[next]) == addr) {
+//			Z_ERR(znd, "DUPE: %llx at %lx",
+//				addr, next + WB_JRNL_BASE);
+//			jrnl->wb[next] = MZTEV_UNUSED;
+//		}
+//	}
+
 	spin_unlock(&jrnl->wb_alloc);
 
 	if ((jrnl->in_use * 100 / jrnl->size) > 50)
 		set_bit(DO_SYNC, &znd->flags);
 
-	return tlba;
+	return blba;
 }
 
 /**
@@ -7112,7 +7221,7 @@ static u64 z_metadata_lba(struct zdm *znd, struct map_pg *map, u32 *num)
 	if (map->lba < znd->data_lba) {
 		if (test_bit(WB_JRNL_1, &map->flags) ||
 		    test_bit(WB_JRNL_2, &map->flags)) {
-			u64 jrnl_lba = journal_alloc(znd, nblks, num);
+			u64 jrnl_lba = journal_alloc(znd, map->lba, nblks, num);
 
 			if (!jrnl_lba) {
 				Z_ERR(znd, "Out of MD journal space?");
@@ -7184,6 +7293,32 @@ static void pg_update_crc(struct zdm *znd, struct map_pg *pg, __le16 md_crc)
 	}
 }
 
+/**
+ * pg_journal_entry() - Add journal entry and flag in journal status.
+ * @znd: ZDM Instance
+ * @pg: The page of lookup table [or CRC] that was written.
+ */
+static int pg_journal_entry(struct zdm *znd, struct map_pg *pg)
+{
+	int rcode = 0;
+
+	if (pg->lba < znd->data_lba) {
+		u64 blba = 0ul; /* if not in journal clean map entry */
+
+		if (test_bit(WB_JRNL_1, &pg->flags) ||
+		    test_bit(WB_JRNL_2, &pg->flags)) {
+			blba = pg->last_write;
+			set_bit(IN_WB_JOURNAL, &pg->flags);
+		} else {
+			clear_bit(IN_WB_JOURNAL, &pg->flags);
+		}
+
+		rcode = md_journal_add_map(znd, pg->lba, blba);
+		if (rcode)
+			Z_ERR(znd, "%s: MD Journal failed.", __func__);
+	}
+	return rcode;
+}
 
 /**
  * pg_written() - Handle accouting related to lookup table page writes
@@ -7191,7 +7326,6 @@ static void pg_update_crc(struct zdm *znd, struct map_pg *pg, __le16 md_crc)
  * @error: non-zero if an error occurred.
  *
  * callback from dm_io notify.. cannot hold mutex here, cannot sleep.
- *
  */
 static int pg_written(struct map_pg *pg, unsigned long error)
 {
@@ -7222,23 +7356,15 @@ static int pg_written(struct map_pg *pg, unsigned long error)
 	clear_bit(W_IN_FLIGHT, &pg->flags);
 	pg_update_crc(znd, pg, md_crc);
 
+	rcode = pg_journal_entry(znd, pg);
+	if (rcode)
+		goto out;
+
 /*
  * NOTE: If we reach here it's a problem.
  *       TODO: mutex free path for adding a map entry ...
  */
 
-	if (pg->lba < znd->data_lba) {
-		u64 blba = 0ul;
-
-		if (test_bit(WB_JRNL_1, &pg->flags) ||
-		    test_bit(WB_JRNL_2, &pg->flags))
-			blba = pg->last_write;
-
-		rcode = md_journal_add_map(znd, pg->lba, blba, CRIT);
-		if (rcode)
-			goto out;
-	}
-
 	if (pg->last_write < znd->data_lba)
 		goto out;
 
@@ -7254,7 +7380,6 @@ out:
 	return rcode;
 }
 
-
 /**
  * on_pg_written() - A block of map table was written.
  * @error: Any error code that occurred during the I/O.
@@ -7376,20 +7501,13 @@ static int write_if_dirty(struct zdm *znd, struct map_pg *pg, int wq, int snc)
 		if (crc_md_le16(pg->data.addr, Z_CRC_4K) == md_crc)
 			clear_bit(IS_DIRTY, &pg->flags);
 		clear_bit(W_IN_FLIGHT, &pg->flags);
-		if (lba < znd->data_lba) {
-			u64 blba = 0ul; /* if not in journal clean map entry */
 
-			if (test_bit(WB_JRNL_1, &pg->flags) ||
-			    test_bit(WB_JRNL_2, &pg->flags))
-				blba = lba;
+		rcode = pg_journal_entry(znd, pg);
+		if (rcode)
+			goto out;
 
-			rcwrt = md_journal_add_map(znd, dm_s, blba, NORMAL);
-			if (rcwrt) {
-				Z_ERR(znd, "%s: MD Journal failed.", __func__);
-				rcode = rcwrt;
-			}
+		if (pg->lba < znd->data_lba)
 			goto out;
-		}
 
 		rcwrt = z_mapped_addmany(znd, dm_s, lba, nf, NORMAL);
 		if (rcwrt) {
@@ -7450,6 +7568,11 @@ static int _sync_dirty(struct zdm *znd, int bit_type, int sync, int drop)
 	while (&expg->zltlst != &znd->zltpool) {
 		ref_pg(expg);
 
+		if (test_and_clear_bit(WB_RE_CACHE, &expg->flags)) {
+			set_bit(IS_DIRTY, &expg->flags);
+			clear_bit(IS_FLUSH, &expg->flags);
+		}
+
 		if (sync && is_dirty(expg, bit_type)) {
 			if (entries < MAX_WSET) {
 				ref_pg(expg);
@@ -7706,6 +7829,7 @@ static int read_pg(struct zdm *znd, struct map_pg *pg, u64 lba48, gfp_t gfp,
 		      pg->lba, lba48,
 		      le16_to_cpu(check),
 		      le16_to_cpu(expect));
+		dump_stack();
 	}
 	rcode = 1;
 
@@ -7891,9 +8015,9 @@ static int move_to_map_tables(struct zdm *znd, struct map_cache *mcache)
 	struct map_pg *rmtbl = NULL;
 	struct map_addr maddr = { .dm_s = 0ul };
 	struct map_addr rev = { .dm_s = 0ul };
+	struct map_cache_data *mcd = get_mcd(mcache);
 	u64 lut_s = BAD_ADDR;
 	u64 lut_r = BAD_ADDR;
-	int jentry;
 	int err = 0;
 	int is_fwd = 1;
 
@@ -7902,12 +8026,15 @@ static int move_to_map_tables(struct zdm *znd, struct map_cache *mcache)
 	 * page the search devolves to a linear lookup.
 	 */
 	mcache_busy(mcache);
-	for (jentry = mcache->jcount; jentry > 0;) {
-		u64 dm_s = le64_to_lba48(mcache->jdata[jentry].tlba, NULL);
-		u64 lba = le64_to_lba48(mcache->jdata[jentry].bval, NULL);
+	while (mcache->jcount > 0) {
+		int idx = mcache->jcount - 1;
+		u64 dm_s = le64_to_lba48(mcd->maps[idx].tlba, NULL);
+		u64 lba = le64_to_lba48(mcd->maps[idx].bval, NULL);
 
 		if (dm_s == Z_LOWER48 || lba == Z_LOWER48) {
-			mcache->jcount = --jentry;
+			if (mcache->jsorted == mcache->jcount)
+				mcache->jsorted--;
+			mcache->jcount--;
 			continue;
 		}
 
@@ -7959,11 +8086,11 @@ static int move_to_map_tables(struct zdm *znd, struct map_cache *mcache)
 		if (err < 0)
 			goto out;
 
-		mcache->jdata[jentry].tlba = MC_INVALID;
-		mcache->jdata[jentry].bval = MC_INVALID;
+		mcd->maps[idx].tlba = MC_INVALID;
+		mcd->maps[idx].bval = MC_INVALID;
 		if (mcache->jsorted == mcache->jcount)
 			mcache->jsorted--;
-		mcache->jcount = --jentry;
+		mcache->jcount--;
 	}
 out:
 	if (smtbl)
-- 
2.8.1

