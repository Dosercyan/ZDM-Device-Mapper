From fc9864ab024fb36d94e9e6da96b89545d2470fd3 Mon Sep 17 00:00:00 2001
From: Shaun Tancheff <shaun.tancheff@seagate.com>
Date: Wed, 9 Sep 2015 14:01:03 -0500
Subject: [PATCH] Initial ZDM for linux v4.2

---
 Documentation/device-mapper/zoned.txt |   67 +
 block/Kconfig                         |    8 +
 block/Makefile                        |    1 +
 block/bio.c                           |   62 +-
 block/blk-zoned-ctrl.c                |  673 +++++
 block/scsi_ioctl.c                    |    5 +
 drivers/ata/libata-scsi.c             |  166 +
 drivers/md/Kconfig                    |   10 +
 drivers/md/Makefile                   |    1 +
 drivers/md/dm-zoned.c                 | 1775 +++++++++++
 drivers/md/dm-zoned.h                 |  466 +++
 drivers/md/libzoned.c                 | 5347 +++++++++++++++++++++++++++++++++
 drivers/scsi/sd.c                     |   81 +-
 include/linux/ata.h                   |   18 +
 include/linux/blk-zoned-ctrl.h        |  222 ++
 include/scsi/scsi.h                   |   12 +
 16 files changed, 8880 insertions(+), 34 deletions(-)
 create mode 100644 Documentation/device-mapper/zoned.txt
 create mode 100644 block/blk-zoned-ctrl.c
 create mode 100644 drivers/md/dm-zoned.c
 create mode 100644 drivers/md/dm-zoned.h
 create mode 100644 drivers/md/libzoned.c
 mode change 100644 => 100755 include/linux/ata.h
 create mode 100644 include/linux/blk-zoned-ctrl.h
 mode change 100644 => 100755 include/scsi/scsi.h

diff --git a/Documentation/device-mapper/zoned.txt b/Documentation/device-mapper/zoned.txt
new file mode 100644
index 0000000..ade9010
--- /dev/null
+++ b/Documentation/device-mapper/zoned.txt
@@ -0,0 +1,67 @@
+Overview of Host Aware ZBC/ZAC Device Mapper
+  - Zone size (256MiB)
+  - Reset WP, Open Zone, Close Zone, Get Zone Info ...
+
+The Zoned DM treats a zoned device as a collection of 1024 zones 256GiB,
+referred to internally as 'megazones' as with zoned devices the last
+megazone may be less than 1024 zones in size. If the last zone is of
+different size it is ignored.
+
+What that means is that drives which report SAME=0 are not supported
+and likely won't be supported within this architecture. However drives
+which report a SAME code of: all same, last differs or same length
+different types, would all be supported by this architecture.
+
+The initial implementation focuses on drives with same sized zones of
+256MB which is 65536 4k blocks. In future the zone size of 256MB will
+be relaxed to allow any size of zone as long as they are all the same.
+
+Megazones:
+	Each megazone is managed independently and partitioned into
+	meta data and data. The Device Mapper Meta Data is logically
+	located in the 2nd and 3rd zones of the megazone. The 1st and
+	2nd zones are reserved for the megazone's minimally relocatable
+	super block which must be the first block of the first or
+	second zone. The most recent is determined by the generation
+	number embedded in the super block. The meta data is sized
+	for two zones and logically located in sectors non-addressable
+	to the upper device. The actual storage of the meta data is pooled
+	with the data using the same mapping scheme.
+	The device mapper internally is a COW device with a 4k per block
+	addressing scheme. There are some fix-ups to handle non-4k aligned
+	requests to support applications which read and write in 512 byte
+	blocks, however it is still desirable to submit patches for these
+	subsystems assuming the respective maintainers are willing to
+	accept such patches.
+
+Address space:
+	The zoned device mapper presents a smaller block device than
+	the amount of data available on the physical media. The extra
+	space is used to hold the meta data needed for managing the
+	data being stored on the drive performing COW block [re]mapping.
+	The 'shrink' is done by appropriately sizing the device via
+	dmsetup.
+	See the z-on.sh and z-off.sh scripts for specific examples.
+
+Map Strategy:
+	Map incoming sector onto device mapper sector space.
+
+Read Strategy:
+	Check each block requested in the bio to determine if the data
+	blocks are consecutively stored on disk. Pass as much per-bio
+	as possible through to the backing block device.
+
+Write Strategy:
+	Allocate space for entire bio on the backing block device
+	redirecting all incoming write requests to the most recently
+	written zone until the zone is filled or the bio is too large
+	to fit and a new zone is opened. Note that if the zone is not
+	filled this zone will likely become used by meta data writes
+	which are typically single blocks.
+
+Sync Strategy:
+	On SYNC bios all the meta data need to restore the zoned device
+	mapper for disk is written to one of the well known zones at
+	the beginning of the mega zone. Data consistency is only
+	'guaranteed' to be on-disk and consistent following sync
+	events [same as ext4].
diff --git a/block/Kconfig b/block/Kconfig
index 161491d..1588a23 100644
--- a/block/Kconfig
+++ b/block/Kconfig
@@ -100,6 +100,14 @@ config BLK_DEV_THROTTLING
 
 	See Documentation/cgroups/blkio-controller.txt for more information.
 
+config BLK_ZONED_CTRL
+	tristate "Support sending zac/zbc commands to zoned devices."
+	default y
+	---help---
+	Support sending close/finish/open/reset wp, and report zones
+	to zoned based devices. Support sending ZAC/ZBC commands to
+	block devices.
+
 config BLK_CMDLINE_PARSER
 	bool "Block device command line partition parser"
 	default n
diff --git a/block/Makefile b/block/Makefile
index 00ecc97..13d4954 100644
--- a/block/Makefile
+++ b/block/Makefile
@@ -14,6 +14,7 @@ obj-$(CONFIG_BOUNCE)	+= bounce.o
 obj-$(CONFIG_BLK_DEV_BSG)	+= bsg.o
 obj-$(CONFIG_BLK_DEV_BSGLIB)	+= bsg-lib.o
 obj-$(CONFIG_BLK_CGROUP)	+= blk-cgroup.o
+obj-$(CONFIG_BLK_ZONED_CTRL)	+= blk-zoned-ctrl.o
 obj-$(CONFIG_BLK_DEV_THROTTLING)	+= blk-throttle.o
 obj-$(CONFIG_IOSCHED_NOOP)	+= noop-iosched.o
 obj-$(CONFIG_IOSCHED_DEADLINE)	+= deadline-iosched.o
diff --git a/block/bio.c b/block/bio.c
index d6e5ba3..45fbc07 100644
--- a/block/bio.c
+++ b/block/bio.c
@@ -1775,6 +1775,64 @@ static inline bool bio_remaining_done(struct bio *bio)
 	return false;
 }
 
+static DEFINE_PER_CPU(struct bio **, bio_end_queue) = { NULL };
+
+static struct bio * unwind_bio_endio(struct bio *bio, int error)
+{
+	struct bio ***bio_end_queue_ptr;
+	struct bio *bio_queue;
+	struct bio *chain_bio = NULL;
+
+	unsigned long flags;
+
+	bio->bi_flags &= ~(1 << BIO_SEG_VALID);
+	bio->bi_phys_segments = error;
+
+	if (error)
+		clear_bit(BIO_UPTODATE, &bio->bi_flags);
+	else if (!test_bit(BIO_UPTODATE, &bio->bi_flags))
+		bio->bi_phys_segments = -EIO;
+
+	local_irq_save(flags);
+	bio_end_queue_ptr = this_cpu_ptr(&bio_end_queue);
+
+	if (*bio_end_queue_ptr) {
+		**bio_end_queue_ptr = bio;
+		*bio_end_queue_ptr = &bio->bi_next;
+		bio->bi_next = NULL;
+	} else {
+		bio_queue = NULL;
+		*bio_end_queue_ptr = &bio_queue;
+
+next_bio:
+		if (bio->bi_end_io == bio_chain_endio) {
+			struct bio *parent = bio->bi_private;
+			bio_put(bio);
+			chain_bio = parent;
+			goto out;
+		}
+
+		if (bio->bi_end_io)
+			bio->bi_end_io(bio, (short)bio->bi_phys_segments);
+
+		if (bio_queue) {
+			bio = bio_queue;
+			bio_queue = bio->bi_next;
+			if (!bio_queue)
+				*bio_end_queue_ptr = &bio_queue;
+			goto next_bio;
+		}
+		*bio_end_queue_ptr = NULL;
+	}
+
+out:
+
+	local_irq_restore(flags);
+
+	return chain_bio;
+}
+
+
 /**
  * bio_endio - end I/O on a bio
  * @bio:	bio
@@ -1813,9 +1871,7 @@ void bio_endio(struct bio *bio, int error)
 			bio_put(bio);
 			bio = parent;
 		} else {
-			if (bio->bi_end_io)
-				bio->bi_end_io(bio, error);
-			bio = NULL;
+			bio = unwind_bio_endio(bio, error);
 		}
 	}
 }
diff --git a/block/blk-zoned-ctrl.c b/block/blk-zoned-ctrl.c
new file mode 100644
index 0000000..c2436d0
--- /dev/null
+++ b/block/blk-zoned-ctrl.c
@@ -0,0 +1,673 @@
+/*
+ * Functions for zone based SMR devices.
+ *
+ * Copyright (C) 2015 Seagate Technology PLC
+ *
+ * Written by:
+ * Shaun Tancheff <shaun.tancheff@seagate.com>
+ * XiaoDong Han <xiaodong.h.han@seagate.com>
+ *
+ * This file is licensed under  the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/bio.h>
+#include <linux/blkdev.h>
+#include <linux/blk-mq.h>
+#include <linux/sched/sysctl.h>
+
+#include "blk.h"
+#include <linux/blk-zoned-ctrl.h>
+#include <linux/vmalloc.h>
+
+/*
+ * for max sense size
+ */
+#include <scsi/scsi.h>
+#include <scsi/scsi_cmnd.h>
+#include <scsi/scsi_eh.h>
+#include <scsi/scsi_dbg.h>
+#include <linux/ata.h>
+
+#define ZBC_TIMEOUT		   (30 * HZ)
+#define ZBC_MAX_RETRIES		     5
+#define CMD_LEN                     16
+#define INQUIRY_CMDLEN  6
+
+/**
+ * ATA pass through 16.
+ */
+#define ZAC_PASS_THROUGH16_OPCODE  ATA_16
+#define ZAC_PASS_THROUGH16_CDB_LEN 16
+
+/**
+ * ATA commands.
+ */
+#define ZAC_ATA_OPCODE_IDENTIFY    ATA_CMD_ID_ATA
+
+/**
+ * zac/zbc common command codes
+ */
+#define ZONE_CLOSE      ATA_SUBCMD_CLOSE_ZONES
+#define ZONE_FINISH     ATA_SUBCMD_FINISH_ZONES
+#define ZONE_OPEN       ATA_SUBCMD_OPEN_ZONES
+#define ZONE_RESET_WP   ATA_SUBCMD_RESET_WP
+
+/**
+ * ZBC zone command
+ */
+#define ZBC_ZONE_ACTION  0x94
+
+/** ----------------------------------------------------------------- */
+/** ----------------------------------------------------------------- */
+/** ------------------- SMR ZONED DRIVE SUPPORT --------------------- */
+/** ----------------------------------------------------------------- */
+/** ----------------------------------------------------------------- */
+
+static inline void _len_to_cmd_zbc(u8 * cmd, u32 _len)
+{
+	u32 len = cpu_to_be32(_len);
+	memcpy(cmd, &len, sizeof(len));
+}
+
+static inline void _lba_to_cmd_zbc(u8 * cmd, u64 _lba)
+{
+	u64 lba = cpu_to_be64(_lba);
+	memcpy(cmd, &lba, sizeof(lba));
+}
+
+static inline u16 zc_get_word(u8 * buf)
+{
+	u16 w = buf[1];
+	w <<= 8;
+	w |= buf[0];
+	return w;
+}
+
+/* NOTE: this is basically scsi_execute */
+int blk_cmd_execute(struct request_queue *queue,
+			   const unsigned char *cmd,
+			   int data_direction,
+			   void *buffer,
+			   unsigned bufflen,
+			   unsigned char *sense,
+			   int timeout,
+			   int retries,
+			   u64 flags,
+			   int *resid)
+{
+        struct request *req;
+        int write = (data_direction == DMA_TO_DEVICE);
+        int ret = DRIVER_ERROR << 24;
+
+        req = blk_get_request(queue, write, __GFP_WAIT);
+        if (IS_ERR(req))
+                return ret;
+        blk_rq_set_block_pc(req);
+
+        if (bufflen &&  blk_rq_map_kern(queue, req,
+                                        buffer, bufflen, __GFP_WAIT))
+                goto out;
+
+        req->cmd_len = COMMAND_SIZE(cmd[0]);
+        memcpy(req->cmd, cmd, req->cmd_len);
+        req->sense = sense;
+        req->sense_len = 0;
+        req->retries = retries;
+        req->timeout = timeout;
+        req->cmd_flags |= flags | REQ_QUIET | REQ_PREEMPT;
+
+        /*
+         * head injection *required* here otherwise quiesce won't work
+         */
+        blk_execute_rq(req->q, NULL, req, 1);
+
+        /*
+         * Some devices (USB mass-storage in particular) may transfer
+         * garbage data together with a residue indicating that the data
+         * is invalid.  Prevent the garbage from being misinterpreted
+         * and prevent security leaks by zeroing out the excess data.
+         */
+        if (unlikely(req->resid_len > 0 && req->resid_len <= bufflen))
+                memset(buffer + (bufflen - req->resid_len), 0, req->resid_len);
+
+        if (resid)
+                *resid = req->resid_len;
+        ret = req->errors;
+ out:
+        blk_put_request(req);
+
+        return ret;
+}
+EXPORT_SYMBOL(blk_cmd_execute);
+
+int blk_cmd_with_sense(struct gendisk *disk,
+	u8 * cmd, int data_direction,
+	u8 * buf, u32 buf_len, u8 * sense_buffer)
+{
+	struct request_queue *queue = disk->queue;
+	int rc;
+	struct scsi_sense_hdr sshdr = { 0 };
+
+	if (!sense_buffer) {
+		pr_err("scsi cmd exec: sense buffer is NULL\n");
+		return -1;
+	}
+
+	rc = blk_cmd_execute(queue, cmd, data_direction, buf, buf_len,
+		sense_buffer, ZBC_TIMEOUT, ZBC_MAX_RETRIES, 0, NULL);
+
+	pr_debug("%s: %s -> 0x%08x"
+		" [h:%02x d:%02x m:%02x s:%02x]\n", __func__,
+		disk->disk_name, rc,
+			host_byte(rc),
+			driver_byte(rc),
+			msg_byte(rc),
+			status_byte(rc));
+
+	scsi_normalize_sense(sense_buffer, SCSI_SENSE_BUFFERSIZE, &sshdr);
+	if (host_byte(rc)
+	    || (    driver_byte(rc)
+		&& (driver_byte(rc) != DRIVER_SENSE) )
+	    || (    status_byte(rc)
+		&& (status_byte(rc) != CHECK_CONDITION)) ) {
+		pr_err("exec scsi cmd failed,opcode:%d\n", cmd[0]);
+		if (driver_byte(rc) & DRIVER_SENSE) {
+			pr_err("%s: %s", __func__, disk->disk_name );
+		}
+		return -1;
+	} else if (   (driver_byte(rc) == DRIVER_SENSE)
+		   && ((cmd[0] == ATA_16) || (cmd[0] == ATA_12))) {
+		if (sense_buffer[21] != 0x50) {
+			pr_err("%s: ATA pass through command failed\n",
+				__func__);
+			return -1;
+		}
+	} else if (rc) {
+		if (   (driver_byte(rc) == DRIVER_SENSE)
+		    && (status_byte(rc) == CHECK_CONDITION)
+		    && (0 != sense_buffer[0])) {
+			pr_err("%s: Something else failed\n", __func__);
+			return -1;
+		}
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(blk_cmd_with_sense);
+
+int blk_zoned_report(struct gendisk *disk,
+			u64 start_lba,
+			u8 opt,
+			u8 * buf,
+			size_t bufsz)
+{
+	int ret = 0;
+	u8 cmd[CMD_LEN] = {0};
+	u8 sense_buf[SCSI_SENSE_BUFFERSIZE] = {0};
+
+	cmd[0] = REPORT_ZONES;
+	cmd[1] = ATA_SUBCMD_REP_ZONES;
+
+	_lba_to_cmd_zbc(&cmd[2],  start_lba);
+	_len_to_cmd_zbc(&cmd[10], (u32)bufsz);
+
+	cmd[14] = opt;
+
+	pr_debug("%s: "
+		"%02x:%02x "
+		"lba:%02x%02x%02x%02x%02x%02x%02x%02x "
+		"len:%02x%02x%02x%02x %02x %02x\n",
+		__func__,
+		cmd[0],  cmd[1],
+		cmd[2],  cmd[3],  cmd[4],  cmd[5],
+		cmd[6],  cmd[7],  cmd[8],  cmd[9],
+		cmd[10], cmd[11], cmd[12], cmd[13],
+		cmd[14], cmd[15] );
+
+	ret = blk_cmd_with_sense(disk, cmd, DMA_FROM_DEVICE, buf, bufsz,
+					&sense_buf[0]);
+	return ret;
+}
+EXPORT_SYMBOL(blk_zoned_report);
+
+int blk_zoned_inquiry(struct gendisk *disk, u8 evpd, u8 pg_op,
+	u16 mx_resp_len, u8 * buf)
+{
+	int ret = 0;
+	u8 cmd[INQUIRY_CMDLEN] = {0};
+	u8 sense_buf[SCSI_SENSE_BUFFERSIZE] = {0};
+
+	__be16 slen = cpu_to_be16(mx_resp_len);
+
+	if (0xb1 != pg_op) {
+		pr_err("Page Code %02x is wrong, the correct page"
+		       " code is 0xb1\n", pg_op);
+		return -1;
+	}
+
+	cmd[0] = INQUIRY;
+	if (evpd)
+		cmd[1] |= 1;
+	cmd[2] = pg_op;
+	cmd[3] = slen & 0xff;
+	cmd[4] = (slen >> 8) & 0xff;
+
+	pr_debug("%s: cmd: "
+		"%02x:%02x:%02x:%02x:%02x:%02x\n",
+		__func__,
+		cmd[0],  cmd[1], cmd[2],  cmd[3],  cmd[4],  cmd[5]);
+
+	ret = blk_cmd_with_sense(disk, cmd, DMA_FROM_DEVICE,
+					buf, mx_resp_len, &sense_buf[0]);
+	if (ret != 0) {
+		pr_err("%s: inquiry failed\n", disk->disk_name);
+		goto out;
+	}
+
+out:
+	return ret;
+}
+EXPORT_SYMBOL(blk_zoned_inquiry);
+
+static int blk_zoned_cmd(struct gendisk *disk, u64 start_lba, u8 command)
+{
+	int ret = 0;
+	u8 cmd[CMD_LEN] = {0};
+	u8 sense_buf[SCSI_SENSE_BUFFERSIZE] = {0};
+	u8 all_bit = 0;
+
+	pr_debug("zoned cmd (%x): on %s, start_lba %lld\n",
+		command, disk->disk_name, start_lba);
+
+	if (start_lba == ~0ul) {
+		all_bit = 1;
+		start_lba = 0;
+	}
+
+	cmd[0] = ZBC_ZONE_ACTION;
+	cmd[1] = command;
+
+	_lba_to_cmd_zbc(&cmd[2], start_lba);
+
+	pr_debug("%s: "
+		"%02x:%02x "
+		"lba:%02x%02x%02x%02x%02x%02x%02x%02x "
+		"len:%02x%02x%02x%02x %02x %02x\n",
+		__func__,
+		cmd[0],  cmd[1],
+		cmd[2],  cmd[3],  cmd[4],  cmd[5],
+		cmd[6],  cmd[7],  cmd[8],  cmd[9],
+		cmd[10], cmd[11], cmd[12], cmd[13],
+		cmd[14], cmd[15] );
+
+	cmd[14] = all_bit;
+	ret = blk_cmd_with_sense(disk, cmd, DMA_FROM_DEVICE, NULL, 0,
+		&sense_buf[0]);
+	if (ret != 0) {
+		pr_err("%s: zone command %d failed\n",
+			disk->disk_name, command);
+		return -1;
+	}
+	return ret;
+}
+
+int blk_zoned_close(struct gendisk *disk, u64 start_lba)
+{
+	return blk_zoned_cmd(disk, start_lba, ZONE_CLOSE);
+}
+EXPORT_SYMBOL(blk_zoned_close);
+
+int blk_zoned_finish(struct gendisk *disk, u64 start_lba)
+{
+	return blk_zoned_cmd(disk, start_lba, ZONE_FINISH);
+}
+EXPORT_SYMBOL(blk_zoned_finish);
+
+int blk_zoned_open(struct gendisk *disk, u64 start_lba)
+{
+	return blk_zoned_cmd(disk, start_lba, ZONE_OPEN);
+}
+EXPORT_SYMBOL(blk_zoned_open);
+
+int blk_zoned_reset_wp(struct gendisk *disk, u64 start_lba)
+{
+	return blk_zoned_cmd(disk, start_lba, ZONE_RESET_WP);
+}
+EXPORT_SYMBOL(blk_zoned_reset_wp);
+
+static inline void _lba_to_cmd_ata(u8 * cmd, u64 _lba)
+{
+	cmd[1] =  _lba        & 0xff;
+	cmd[3] = (_lba >>  8) & 0xff;
+	cmd[5] = (_lba >> 16) & 0xff;
+	cmd[0] = (_lba >> 24) & 0xff;
+	cmd[2] = (_lba >> 32) & 0xff;
+	cmd[4] = (_lba >> 40) & 0xff;
+}
+
+int blk_zoned_report_ata(struct gendisk *disk, u64 start_lba, u8 opt, u8 * buf, size_t bufsz)
+{
+	int ret = 0;
+	u8 cmd[ZAC_PASS_THROUGH16_CDB_LEN] = { 0 };
+	u8 sense_buf[SCSI_SENSE_BUFFERSIZE] = { 0 };
+
+	cmd[0] = ZAC_PASS_THROUGH16_OPCODE;
+	cmd[1] = (0x4 << 1) | 0x01;
+	cmd[2] = 0x0e;
+
+	cmd[4] = 0x0; /* Function: Report Zones */
+	cmd[3] = opt;
+
+	cmd[5] = (bufsz / 512) >> 8;
+	cmd[6] = (bufsz / 512) & 0xff;
+
+	_lba_to_cmd_ata(&cmd[7], start_lba);
+
+	cmd[13] = 0xa0;
+	cmd[14] = 0x4a;
+	ret = blk_cmd_with_sense(disk, cmd, DMA_FROM_DEVICE, buf, bufsz,
+					&sense_buf[0]);
+	return ret;
+}
+EXPORT_SYMBOL(blk_zoned_report_ata);
+
+int blk_zoned_identify_ata(struct gendisk *disk, zoned_identity_t * identify)
+{
+	int ret = 0;
+	u8 identify_cmd[ZAC_PASS_THROUGH16_CDB_LEN] = { 0 };
+	u8 sense_buf[SCSI_SENSE_BUFFERSIZE] = { 0 };
+	u8 buffer[512] = { 0 };
+	int flag = 0;
+
+	identify->type_id = NOT_ZONED;
+
+	if (NULL == disk) {
+		return -1;
+	}
+
+	identify_cmd[0] = ZAC_PASS_THROUGH16_OPCODE;
+	identify_cmd[1] = (0x4 << 1) | 0x1;
+	identify_cmd[2] = 0xe;
+	identify_cmd[6] = 0x1;
+	identify_cmd[8] = 0x1;
+	identify_cmd[14] = ZAC_ATA_OPCODE_IDENTIFY;
+
+	ret = blk_cmd_with_sense(disk, identify_cmd, DMA_FROM_DEVICE,
+					buffer, 512, &sense_buf[0]);
+	if (ret != 0) {
+		pr_err("%s: identify failed.\n", disk->disk_name);
+		goto out;
+	}
+
+	flag = zc_get_word(&buffer[138]);
+	if ((flag & 0x3) == 0x1) {
+		identify->type_id = HOST_AWARE;
+		pr_debug("%s: is SMR-HostAware\n", disk->disk_name);
+	} else {
+		ret = -1;
+		pr_debug("%s: not HostAware\n", disk->disk_name);
+	}
+
+out:
+	return ret;
+}
+EXPORT_SYMBOL(blk_zoned_identify_ata);
+
+static int _blk_zoned_command_ata(struct gendisk *disk, u64 start_lba, u8 command)
+{
+	int ret = 0;
+	u8 cmd[ZAC_PASS_THROUGH16_CDB_LEN] = { 0 };
+	u8 sense_buf[SCSI_SENSE_BUFFERSIZE] = { 0 };
+
+	pr_debug("zoned command (%x): %s, start_lba %llx\n",
+		command, disk->disk_name, start_lba);
+
+	cmd[0] = ZAC_PASS_THROUGH16_OPCODE;
+	cmd[1] = (0x3 << 1) | 0x01;
+	cmd[4] = command; /* open: 0x03, close: 0x01, finish: 0x02, reset_wp: 0x04 */
+
+	if (start_lba == ~0ul || start_lba == ~1ul) {
+		cmd[3] = 0x1; /* apply command to all zones */
+	} else {
+		_lba_to_cmd_ata(&cmd[7], start_lba);
+	}
+
+	cmd[13] = 1 << 6;
+	cmd[14] = 0x9F;
+
+	ret = blk_cmd_with_sense(disk, cmd, DMA_NONE, NULL, 0, &sense_buf[0]);
+	if (ret != 0) {
+		pr_err("%s: command %d failed\n", disk->disk_name, command);
+		return -1;
+	}
+	return ret;
+}
+
+int blk_zoned_close_ata(struct gendisk *disk, u64 start_lba)
+{
+	return _blk_zoned_command_ata(disk, start_lba, ZONE_CLOSE);
+}
+EXPORT_SYMBOL(blk_zoned_close_ata);
+
+int blk_zoned_finish_ata(struct gendisk *disk, u64 start_lba)
+{
+	return _blk_zoned_command_ata(disk, start_lba, ZONE_FINISH);
+}
+EXPORT_SYMBOL(blk_zoned_finish_ata);
+
+int blk_zoned_open_ata(struct gendisk *disk, u64 start_lba)
+{
+	return _blk_zoned_command_ata(disk, start_lba, ZONE_OPEN);
+}
+EXPORT_SYMBOL(blk_zoned_open_ata);
+
+int blk_zoned_reset_wp_ata(struct gendisk *disk, u64 start_lba)
+{
+	return _blk_zoned_command_ata(disk, start_lba, ZONE_RESET_WP);
+}
+EXPORT_SYMBOL(blk_zoned_reset_wp_ata);
+
+int _inquiry_ioctl(struct gendisk *disk, void __user *parg)
+{
+	int error = 0;
+	size_t result_size = 0;
+	size_t alloc_size = PAGE_SIZE;
+	zoned_inquiry_t * inq = kmalloc(alloc_size, GFP_KERNEL);
+	u8 extended;
+
+	if (!inq) {
+		error = -ENOMEM;
+		goto out;
+	}
+	if (copy_from_user(inq, parg, sizeof(*inq))) {
+		error = -EFAULT;
+		goto out;
+	}
+	result_size = inq->mx_resp_len + offsetof(zoned_inquiry_t, result);
+	if (result_size > alloc_size ) {
+		void * tmp;
+		alloc_size = result_size;
+		tmp = krealloc(inq, alloc_size, GFP_KERNEL);
+		if (!tmp) {
+			error = -ENOMEM;
+			goto out;
+		}
+		inq = tmp;
+	}
+
+	extended = inq->evpd & 0x7f;
+	if (inq->evpd & ZOPT_USE_ATA_PASS) {
+		zoned_identity_t ident;
+		pr_debug("%s: using ata passthrough.\n", __func__ );
+		error = blk_zoned_identify_ata(disk, &ident);
+		inq->result[8] = ident.type_id << 4;
+	} else {
+		error = blk_zoned_inquiry(disk, extended,   inq->pg_op,
+					  inq->mx_resp_len, inq->result);
+	}
+	if (error) {
+		error = -EFAULT;
+		goto out;
+	}
+	if (copy_to_user(parg, inq, result_size)) {
+		error = -EFAULT;
+		goto out;
+	}
+
+out:
+	if (inq) {
+		kfree(inq);
+	}
+	return error;
+}
+EXPORT_SYMBOL(_inquiry_ioctl);
+
+int _zone_close_ioctl(struct gendisk *disk, unsigned long arg)
+{
+	int error = -EFAULT;
+	if (arg & 1) {
+		if (arg != ~0ul) {
+			arg &= ~1ul; /* ~1 :: 0xFF...FE */
+		}
+		error = blk_zoned_close_ata(disk, arg);
+	} else {
+		if (arg == ~1ul) {
+			arg = ~0ul;
+		}
+		error = blk_zoned_close(disk, arg);
+	}
+
+	return error;
+}
+EXPORT_SYMBOL(_zone_close_ioctl);
+
+int _zone_finish_ioctl(struct gendisk *disk, unsigned long arg)
+{
+	int error = -EFAULT;
+	if (arg & 1) {
+		if (arg != ~0ul) {
+			arg &= ~1ul; /* ~1 :: 0xFF...FE */
+		}
+		error = blk_zoned_finish_ata(disk, arg);
+	} else {
+		if (arg == ~1ul) {
+			arg = ~0ul;
+		}
+		error = blk_zoned_finish(disk, arg);
+	}
+
+	return error;
+}
+EXPORT_SYMBOL(_zone_finish_ioctl);
+
+int _zone_open_ioctl(struct gendisk *disk, unsigned long arg)
+{
+	int error = -EFAULT;
+	if (arg & 1) {
+		if (arg != ~0ul) {
+			arg &= ~1ul; /* ~1 :: 0xFF...FE */
+		}
+		error = blk_zoned_open_ata(disk, arg);
+	} else {
+		if (arg == ~1ul) {
+			arg = ~0ul;
+		}
+		error = blk_zoned_open(disk, arg);
+	}
+
+	return error;
+}
+EXPORT_SYMBOL(_zone_open_ioctl);
+
+int _reset_wp_ioctl(struct gendisk *disk, unsigned long arg)
+{
+	int error = -EFAULT;
+	if (arg & 1) {
+		if (arg != ~0ul) {
+			arg &= ~1ul; /* ~1 :: 0xFF...FE */
+		}
+		error = blk_zoned_reset_wp_ata(disk, arg);
+	} else {
+		if (arg == ~1ul) {
+			arg = ~0ul;
+		}
+		error = blk_zoned_reset_wp(disk, arg);
+	}
+
+	return error;
+}
+EXPORT_SYMBOL(_reset_wp_ioctl);
+
+int _report_zones_ioctl(struct gendisk *disk, void __user *parg)
+{
+	int error = -EFAULT;
+	int is_vm = 0;
+	struct bdev_zone_report_ioctl_t * zone_iodata = NULL;
+	u32 alloc_size = max(PAGE_SIZE, sizeof(*zone_iodata));
+	u8 opt = 0;
+
+	zone_iodata = kmalloc(alloc_size, GFP_KERNEL);
+	if (!zone_iodata) {
+		error = -ENOMEM;
+		goto report_zones_out;
+	}
+	if (copy_from_user(zone_iodata, parg, sizeof(*zone_iodata))) {
+		error = -EFAULT;
+		goto report_zones_out;
+	}
+	if (zone_iodata->data.in.return_page_count > alloc_size) {
+		void * tmp;
+		alloc_size = zone_iodata->data.in.return_page_count;
+		if (alloc_size < KMALLOC_MAX_SIZE) {
+			tmp = krealloc(zone_iodata, alloc_size, GFP_KERNEL);
+			if (!tmp) {
+				error = -ENOMEM;
+				goto report_zones_out;
+			}
+			zone_iodata = tmp;
+		} else {
+			/* too large for kmalloc, fallback to vmalloc */
+			is_vm = 1;
+			tmp = zone_iodata;
+			zone_iodata = vzalloc(alloc_size);
+			if (zone_iodata) {
+				memcpy(zone_iodata, tmp,
+					sizeof(*zone_iodata));
+			}
+			kfree(tmp);
+			if (!zone_iodata) {
+				error = -ENOMEM;
+				goto report_zones_out;
+			}
+		}
+	}
+	opt = zone_iodata->data.in.report_option & 0x7F;
+	if (zone_iodata->data.in.report_option & ZOPT_USE_ATA_PASS) {
+		error = blk_zoned_report_ata(disk,
+			zone_iodata->data.in.zone_locator_lba,
+			opt, (u8*)&zone_iodata->data.out, alloc_size );
+	} else {
+		error = blk_zoned_report(disk,
+			zone_iodata->data.in.zone_locator_lba,
+			opt, (u8*)&zone_iodata->data.out, alloc_size );
+	}
+	if (error) {
+		goto report_zones_out;
+	}
+	if (copy_to_user(parg, zone_iodata, alloc_size)) {
+		error = -EFAULT;
+	}
+report_zones_out:
+	if (zone_iodata) {
+		if (is_vm)
+			vfree(zone_iodata);
+		else
+			kfree(zone_iodata);
+	}
+	return error;
+}
+EXPORT_SYMBOL(_report_zones_ioctl);
diff --git a/block/scsi_ioctl.c b/block/scsi_ioctl.c
index dda653c..b5eed6e 100644
--- a/block/scsi_ioctl.c
+++ b/block/scsi_ioctl.c
@@ -705,6 +705,11 @@ int scsi_verify_blk_ioctl(struct block_device *bd, unsigned int cmd)
 	case SG_GET_RESERVED_SIZE:
 	case SG_SET_RESERVED_SIZE:
 	case SG_EMULATED_HOST:
+	case SCSI_IOCTL_INQUIRY:
+	case SCSI_IOCTL_REPORT_ZONES:
+	case SCSI_IOCTL_RESET_WP:
+	case SCSI_IOCTL_OPEN_ZONE:
+	case SCSI_IOCTL_CLOSE_ZONE:
 		return 0;
 	case CDROM_GET_CAPABILITY:
 		/* Keep this until we remove the printk below.  udev sends it
diff --git a/drivers/ata/libata-scsi.c b/drivers/ata/libata-scsi.c
index 0d7f0da..39e2be9 100644
--- a/drivers/ata/libata-scsi.c
+++ b/drivers/ata/libata-scsi.c
@@ -65,6 +65,7 @@ static struct ata_device *__ata_scsi_find_dev(struct ata_port *ap,
 					const struct scsi_device *scsidev);
 static struct ata_device *ata_scsi_find_dev(struct ata_port *ap,
 					    const struct scsi_device *scsidev);
+static void scsi_16_lba_len(const u8 *cdb, u64 *plba, u32 *plen);
 
 #define RW_RECOVERY_MPAGE 0x1
 #define RW_RECOVERY_MPAGE_LEN 12
@@ -1443,6 +1444,159 @@ static unsigned int ata_scsi_flush_xlat(struct ata_queued_cmd *qc)
 }
 
 /**
+ *	ata_scsi_reset_wp_xlat - Translate SCSI Reset Write Pointer command
+ *	@qc: Storage for translated ATA taskfile
+ *
+ *	Sets up an ATA taskfile to issue Reset Write Pointers Ext command.
+ *	May need change when zac specs is available.
+ *
+ *	LOCKING:
+ *	spin_lock_irqsave(host lock)
+ *
+ *	RETURNS:
+ *	Zero on success, non-zero on error.
+ */
+static unsigned int ata_scsi_reset_wp_xlat(struct ata_queued_cmd *qc)
+{
+	struct scsi_cmnd *scmd = qc->scsicmd;
+	struct ata_taskfile *tf = &qc->tf;
+	const u8 *cdb = scmd->cmnd;
+	u8 sa; /* service action */
+	u8 all_bit;
+
+	if (scmd->cmd_len < 16)
+		goto invalid_fld;
+
+	sa = cdb[1] & 0x1f;
+
+	if (!(sa == ATA_SUBCMD_CLOSE_ZONES || 
+	      sa == ATA_SUBCMD_FINISH_ZONES || 
+	      sa == ATA_SUBCMD_OPEN_ZONES || 
+	      sa == ATA_SUBCMD_RESET_WP)) 
+		goto invalid_fld;
+
+	all_bit = cdb[14] & 0x01;
+	if (!all_bit) {
+		struct ata_device *dev = qc->dev;
+		u64 max_lba = dev->n_sectors;     /* Maximal LBA supported */
+		u64 slba;
+		u32 slen;
+
+		scsi_16_lba_len(cdb, &slba, &slen);
+		if (slba > max_lba) {
+			ata_dev_err(dev,
+				"Zone start LBA %llu > %llu (Max LBA)\n",
+				slba, max_lba);
+			goto out_of_range;
+		}
+
+		tf->hob_lbah = (slba >> 40) & 0xff;
+		tf->hob_lbam = (slba >> 32) & 0xff;
+		tf->hob_lbal = (slba >> 24) & 0xff;
+		tf->lbah = (slba >> 16) & 0xff;
+		tf->lbam = (slba >> 8) & 0xff;
+		tf->lbal = slba & 0xff;
+	}
+
+
+	tf->flags |= ATA_TFLAG_DEVICE | ATA_TFLAG_LBA48;
+	tf->protocol = ATA_PROT_NODATA;
+
+	tf->command = ATA_CMD_ZONE_MAN_OUT;
+	tf->feature = sa;
+	tf->hob_feature = all_bit;
+
+	return 0;
+
+ invalid_fld:
+	ata_scsi_set_sense(scmd, ILLEGAL_REQUEST, 0x24, 0x0);
+	/* "Invalid field in cbd" */
+	return 1;
+ out_of_range:
+	ata_scsi_set_sense(scmd, ILLEGAL_REQUEST, 0x21, 0x0);
+	/* LBA out of range */
+	return 1;
+}
+
+/**
+ *	ata_scsi_report_zones_xlat - Translate SCSI Report Zones command
+ *	@qc: Storage for translated ATA taskfile
+ *
+ *	Sets up an ATA taskfile to issue Report Zones Ext command.
+ *	May need change when zac specs is updated.
+ *
+ *	LOCKING:
+ *	spin_lock_irqsave(host lock)
+ *
+ *	RETURNS:
+ *	Zero on success, non-zero on error.
+ */
+static unsigned int ata_scsi_report_zones_xlat(struct ata_queued_cmd *qc)
+{
+	struct ata_device *dev = qc->dev;
+	struct scsi_cmnd *scmd = qc->scsicmd;
+	struct ata_taskfile *tf = &qc->tf;
+	const u8 *cdb = scmd->cmnd;
+	u64 max_lba = dev->n_sectors;     /* Maximal LBA supported */
+	u64 slba;       /* Start LBA in scsi command */
+	u32 alloc_len;  /* Alloc length in bytes in scsi command */
+	u8 reporting_option;
+
+	if (scmd->cmd_len < 16){
+		ata_dev_err(dev, "ZAC Error: Command length is less than 16\n");
+		goto invalid_fld;
+	}
+	if (unlikely(!dev->dma_mode)){
+		ata_dev_err(dev, "ZAC Error: No DMA mode is set\n");
+		goto invalid_fld;
+	}
+	if (!scsi_sg_count(scmd)){
+		ata_dev_err(dev, "ZAC Error: SCSI sg count is zero\n");
+		goto invalid_fld;
+	}
+	scsi_16_lba_len(cdb, &slba, &alloc_len);
+	if (slba > max_lba) {
+		ata_dev_err(dev, "Zone start LBA %llu > %llu (Max LBA)\n", slba, max_lba);
+		goto out_of_range;
+	}
+
+	reporting_option = cdb[14] & 0x3f;
+
+	tf->flags |= ATA_TFLAG_DEVICE | ATA_TFLAG_LBA48 | ATA_TFLAG_ISADDR;
+	tf->protocol = ATA_PROT_DMA;
+
+	tf->command = ATA_CMD_ZONE_MAN_IN;
+
+	tf->hob_lbah = (slba >> 40) & 0xff;
+	tf->hob_lbam = (slba >> 32) & 0xff;
+	tf->hob_lbal = (slba >> 24) & 0xff;
+	tf->lbah = (slba >> 16) & 0xff;
+	tf->lbam = (slba >> 8) & 0xff;
+	tf->lbal = slba & 0xff;
+
+	tf->feature = 0x00;
+	tf->hob_feature = reporting_option;
+
+	alloc_len    /= 512; /* bytes in scsi, blocks in ata */
+	tf->nsect     = alloc_len & 0xff;
+	tf->hob_nsect = alloc_len >> 8;
+
+	ata_qc_set_pc_nbytes(qc);
+
+	return 0;
+
+ invalid_fld:
+	ata_scsi_set_sense(scmd, ILLEGAL_REQUEST, 0x24, 0x0);
+	/* "Invalid field in cbd" */
+	return 1;
+ out_of_range:
+	ata_scsi_set_sense(scmd, ILLEGAL_REQUEST, 0x21, 0x0);
+	/* LBA out of range */
+	return 1;
+}
+
+
+/**
  *	scsi_6_lba_len - Get LBA and transfer length
  *	@cdb: SCSI command to translate
  *
@@ -2232,12 +2386,17 @@ static unsigned int ata_scsiop_inq_b1(struct ata_scsi_args *args, u8 *rbuf)
 {
 	int form_factor = ata_id_form_factor(args->id);
 	int media_rotation_rate = ata_id_rotation_rate(args->id);
+	bool zac_ha = ata_drive_zac_ha(args->id);
 
 	rbuf[1] = 0xb1;
 	rbuf[3] = 0x3c;
 	rbuf[4] = media_rotation_rate >> 8;
 	rbuf[5] = media_rotation_rate;
 	rbuf[7] = form_factor;
+	if (zac_ha) {
+		rbuf[8] &= 0xcf;
+		rbuf[8] |= 0x10;  /* SBC4: 0x01 for zoned host aware device */
+	}
 
 	return 0;
 }
@@ -3415,6 +3574,13 @@ static inline ata_xlat_func_t ata_get_xlat_func(struct ata_device *dev, u8 cmd)
 
 	case START_STOP:
 		return ata_scsi_start_stop_xlat;
+
+	case RESET_WP:
+		return ata_scsi_reset_wp_xlat;
+
+	case REPORT_ZONES:
+		return ata_scsi_report_zones_xlat;
+	  break;
 	}
 
 	return NULL;
diff --git a/drivers/md/Kconfig b/drivers/md/Kconfig
index bfec3bd..3ed88ba 100644
--- a/drivers/md/Kconfig
+++ b/drivers/md/Kconfig
@@ -335,6 +335,16 @@ config DM_ERA
          over time.  Useful for maintaining cache coherency when using
          vendor snapshots.
 
+config DM_ZONED
+       tristate "SMR zoned target (EXPERIMENTAL)"
+       depends on BLK_DEV_DM
+       default n
+       select DM_PERSISTENT_DATA
+       select DM_BIO_PRISON
+       ---help---
+         dm-zoned implements COW and GC by to align on zone boundaries.
+         Forward writing within zones, garbage collection within zones .. etc.
+
 config DM_MIRROR
        tristate "Mirror target"
        depends on BLK_DEV_DM
diff --git a/drivers/md/Makefile b/drivers/md/Makefile
index 462f443..a692a79 100644
--- a/drivers/md/Makefile
+++ b/drivers/md/Makefile
@@ -59,6 +59,7 @@ obj-$(CONFIG_DM_CACHE_SMQ)	+= dm-cache-smq.o
 obj-$(CONFIG_DM_CACHE_CLEANER)	+= dm-cache-cleaner.o
 obj-$(CONFIG_DM_ERA)		+= dm-era.o
 obj-$(CONFIG_DM_LOG_WRITES)	+= dm-log-writes.o
+obj-$(CONFIG_DM_ZONED)		+= dm-zoned.o
 
 ifeq ($(CONFIG_DM_UEVENT),y)
 dm-mod-objs			+= dm-uevent.o
diff --git a/drivers/md/dm-zoned.c b/drivers/md/dm-zoned.c
new file mode 100644
index 0000000..51be8f6
--- /dev/null
+++ b/drivers/md/dm-zoned.c
@@ -0,0 +1,1775 @@
+/*
+ * Kernel Device Mapper for abstracting ZAC/ZBC devices as normal
+ * block devices for linux file systems.
+ *
+ * Copyright (C) 2015 Seagate Technology PLC
+ *
+ * Written by:
+ * Shaun Tancheff <shaun.tancheff@seagate.com>
+ *
+ * This file is licensed under  the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include "dm.h"
+#include <linux/dm-io.h>
+#include <linux/init.h>
+#include <linux/mempool.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/random.h>	/* uuid */
+#include <linux/crc32c.h>	/* crc32c */
+#include <linux/crc16.h>
+#include <linux/sort.h>		/* sort [heapsort impl] */
+#include <linux/ctype.h>	/* isdigit() */
+#include <linux/blk-zoned-ctrl.h>
+#include <linux/timer.h>
+#include <linux/delay.h>
+
+#include "dm-zoned.h"
+
+#define PRIu64 "llu"
+#define PRIx64 "llx"
+
+static inline char * _zdisk(zoned_t *znd)
+{
+	return znd->bdev_name;
+}
+
+#define Z_ERR(znd, fmt, arg...) \
+	pr_err("dm-zoned(%s): " fmt "\n", _zdisk(znd), ## arg)
+
+#define Z_INFO(znd, fmt, arg...) \
+        printk("dm-zoned(%s): " fmt "\n", _zdisk(znd), ## arg)
+
+#define Z_DBG(znd, fmt, arg...) \
+	pr_debug("dm-zoned(%s): " fmt "\n", _zdisk(znd), ## arg)
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static int dev_is_congested(struct dm_dev *dev, int bdi_bits);
+static int zoned_is_congested(struct dm_target_callbacks *cb, int bdi_bits);
+static int zoned_constructor(struct dm_target *ti, unsigned argc, char **argv);
+static void do_io_work(struct work_struct *work);
+static int block_io(zoned_t *, enum dm_io_mem_type, void *, sector_t,
+		    unsigned int, int, int);
+static int zoned_map_write(megazone_t*, struct bio*, z_map_addr_t*);
+static int zoned_map_read(zoned_t * znd, struct bio *bio);
+static int zoned_map(struct dm_target *ti, struct bio *bio);
+static sector_t get_dev_size(struct dm_target *ti);
+static int zoned_iterate_devices(struct dm_target *ti,
+				 iterate_devices_callout_fn fn, void *data);
+static int zoned_merge(struct dm_target *ti, struct bvec_merge_data *bvm,
+		       struct bio_vec *biovec, int max_size);
+static void zoned_io_hints(struct dm_target *ti, struct queue_limits *limits);
+
+static int is_zoned_inquiry(zoned_t * znd, int trim, int ata);
+static int dmz_reset_wp(megazone_t * megaz, u64 z_id);
+static int dmz_open_zone(megazone_t * megaz, u64 z_id);
+static int dmz_close_zone(megazone_t * megaz, u64 z_id);
+static int dmz_report_zones(zoned_t *, u64, struct bdev_zone_report_result_t *);
+
+
+static void activity_timeout(unsigned long data);
+static void zoned_destroy(zoned_t *);
+static int gc_can_cherrypick(megazone_t * megaz);
+static void bg_work_task(struct work_struct *work);
+static void on_timeout_activity(zoned_t * znd);
+
+static inline struct inode *get_bdev_bd_inode(zoned_t * znd)
+{
+	return znd->dev->bdev->bd_inode;
+}
+
+#include "libzoned.c"
+
+static int dev_is_congested(struct dm_dev *dev, int bdi_bits)
+{
+	struct request_queue *q = bdev_get_queue(dev->bdev);
+	return bdi_congested(&q->backing_dev_info, bdi_bits);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_is_congested(struct dm_target_callbacks *cb, int bdi_bits)
+{
+	zoned_t *zoned = container_of(cb, zoned_t, callbacks);
+	int backing = dev_is_congested(zoned->dev, bdi_bits);
+	if (zoned->gc_backlog > 1) {
+		backing |= 1 << WB_async_congested; // Was: BDI_
+		backing |= 1 << WB_sync_congested;  // Was: BDI_
+	}
+	return backing;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void set_discard_support(struct gendisk *disk, int trim)
+{
+	DMINFO("dm-zoned(%s) - Discard Support: %s", trim ? "on" : "off",
+		disk->disk_name);
+
+	if (disk && disk->queue && trim) {
+		disk->queue->limits.discard_granularity = PAGE_SIZE;
+		disk->queue->limits.max_discard_sectors = 1 << 22;
+		disk->queue->limits.discard_zeroes_data = 1;
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void discard_support(zoned_t * znd, int trim)
+{
+	struct gendisk *disk = znd->dev->bdev->bd_disk;
+	if (disk->queue) {
+		set_discard_support(disk, trim);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int is_zoned_inquiry(zoned_t * znd, int trim, int ata)
+{
+	struct gendisk *disk = znd->dev->bdev->bd_disk;
+
+	if (disk->queue) {
+		u8 extended = 1;
+		u8 page_op = 0xb1;
+		u8 *buf = NULL;
+		u16 sz = 64;
+		int wp_err;
+
+		set_discard_support(disk, trim);
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+		if (ata) {
+			zoned_identity_t ident;
+			wp_err = blk_zoned_identify_ata(disk, &ident);
+			if (!wp_err) {
+				if (ident.type_id == HOST_AWARE) {
+					znd->zinqtype = Z_TYPE_SMR_HA;
+					znd->ata_passthrough = 1;
+				}
+			}
+			return 0;
+		}
+
+		buf = ZDM_ALLOC(znd, Z_C4K, PG_01); /* zoned inq */
+		if (!buf) {
+			return -ENOMEM;
+		}
+
+		wp_err = blk_zoned_inquiry(disk, extended, page_op, sz, buf);
+		if (!wp_err) {
+			znd->zinqtype = buf[Z_VPD_INFO_BYTE] >> 4 & 0x03;
+			if (znd->zinqtype != Z_TYPE_SMR_HA &&
+			    buf[4] == 0x17 && buf[5] == 0x5c) {
+				Z_ERR(znd, "Forcing ResetWP capability ... ");
+				znd->zinqtype = Z_TYPE_SMR_HA;
+				znd->ata_passthrough = 0;
+			}
+		}
+#else
+	#warning "CONFIG_BLK_ZONED_CTRL required."
+#endif
+		if (buf) {
+			ZDM_FREE(znd, buf, Z_C4K, PG_01);
+		}
+	}
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_map_discard(zoned_t * znd, struct bio *bio)
+{
+	u64 lba     = 0;
+	int rcode   = DM_MAPIO_SUBMITTED;
+	u64 s_up    = bio->bi_iter.bi_sector >> 3;
+	u64 blks    = bio->bi_iter.bi_size / Z_C4K;
+	u64 count;
+	z_map_addr_t maddr;
+	megazone_t *megaz = NULL;
+	int err;
+
+	for (count = 0; count < blks; count++) {
+		u64 s_map = s_up + count;
+
+		map_addr_onto_dm_zoned(znd, s_map, &maddr);
+		megaz = &znd->z_mega[maddr.mz_id];
+
+		mutex_lock(&megaz->mz_io_mutex);
+		lba = z_lookup(megaz, &maddr);
+		if (lba) {
+			Z_DBG(znd, "TRIM: FS: %llx -> dm_s: %llx lba: %llx",
+				s_map, maddr.dm_s, lba);
+			err = z_mapped_discard(megaz, maddr.dm_s, lba);
+		}
+		mutex_unlock(&megaz->mz_io_mutex);
+
+		if (err == -EBUSY) {
+			Z_ERR(znd, "Discard target %llx in flight."
+				   " Flushing GC queue.",
+				maddr.dm_s);
+			flush_workqueue(znd->gc_wq);
+			Z_ERR(znd, "GC queue flushed. Continue discarding.");
+			err = 0;
+			continue;
+		} else {
+			rcode = err;
+			goto out;
+		}
+
+		if ( (count & 0xFFFF) == 0 ) {
+			if (test_bit(DO_JOURNAL_MOVE, &megaz->flags) ||
+			    test_bit(DO_MEMPOOL, &megaz->flags)) {
+				if (!test_bit(DO_METAWORK_QD, &megaz->flags)) {
+					set_bit(DO_METAWORK_QD, &megaz->flags);
+					queue_work(znd->meta_wq, &megaz->meta_work);
+					flush_workqueue(znd->meta_wq);
+				}
+			}
+
+		}
+	}
+out:
+	if (rcode == DM_MAPIO_SUBMITTED) {
+		bio_endio(bio, 0);
+	}
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int dmz_reset_wp(megazone_t * megaz, u64 z_id)
+{
+	// start of MZ:
+	int wp_err = 0;
+
+// FUTURE: Check zone 'type' flag is ZONED [and non conventional].
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+	if (megaz->znd->zinqtype == Z_TYPE_SMR_HA) {
+		struct gendisk *disk = megaz->znd->dev->bdev->bd_disk;
+		u64 mapped_zoned = z_id + megaz->znd->first_zone;
+		u64 lba = Z_BLKSZ * ((megaz->mega_nr * 1024) + mapped_zoned);
+		u64 s_addr = lba * Z_BLOCKS_PER_DM_SECTOR;
+
+#if EXTRA_DEBUG
+		Z_ERR(megaz->znd, "%s: Reset WP: z_id: %llu -> zone %llu "
+		      "[0x%llx]", __func__, z_id, s_addr / 0x80000, s_addr);
+#endif
+
+		if (megaz->znd->ata_passthrough) {
+			wp_err = blk_zoned_reset_wp_ata(disk, s_addr);
+		} else {
+			wp_err = blk_zoned_reset_wp(disk, s_addr);
+		}
+		if (wp_err) {
+			Z_ERR(megaz->znd, "Reset WP: %llu -> %d failed.",
+			       s_addr, wp_err);
+			Z_ERR(megaz->znd, "Disabling Reset WP capability");
+
+			megaz->znd->zinqtype = 0;
+		}
+	}
+#endif /* CONFIG_BLK_ZONED_CTRL */
+	return wp_err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int dmz_open_zone(megazone_t * megaz, u64 z_id)
+{
+	// start of MZ:
+	int wp_err = 0;
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+	if (megaz->znd->zinqtype == Z_TYPE_SMR_HA) {
+		struct gendisk *disk = megaz->znd->dev->bdev->bd_disk;
+		u64 mapped_zoned = z_id + megaz->znd->first_zone;
+		u64 lba = Z_BLKSZ * ((megaz->mega_nr * 1024) + mapped_zoned);
+		u64 s_addr = lba * Z_BLOCKS_PER_DM_SECTOR;
+
+		if (megaz->znd->ata_passthrough) {
+			wp_err = blk_zoned_open_ata(disk, s_addr);
+		} else {
+			wp_err = blk_zoned_open(disk, s_addr);
+		}
+		if (wp_err) {
+			Z_ERR(megaz->znd, "Open Zone: %llx -> %d failed.",
+			       s_addr, wp_err);
+			Z_ERR(megaz->znd, "ZAC/ZBC support disabled.");
+			megaz->znd->zinqtype = 0;
+		}
+	}
+#endif /* CONFIG_BLK_ZONED_CTRL */
+	return wp_err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int dmz_close_zone(megazone_t * megaz, u64 z_id)
+{
+	// start of MZ:
+	int wp_err = 0;
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+	if (megaz->znd->zinqtype == Z_TYPE_SMR_HA) {
+		struct gendisk *disk = megaz->znd->dev->bdev->bd_disk;
+		u64 mapped_zoned = z_id + megaz->znd->first_zone;
+		u64 lba = Z_BLKSZ * ((megaz->mega_nr * 1024) + mapped_zoned);
+		u64 s_addr = lba * Z_BLOCKS_PER_DM_SECTOR;
+
+		if (megaz->znd->ata_passthrough) {
+			wp_err = blk_zoned_close_ata(disk, s_addr);
+		} else {
+			wp_err = blk_zoned_close(disk, s_addr);
+		}
+		if (wp_err) {
+			Z_ERR(megaz->znd, "Open Zone: %llu -> %d failed.",
+			       s_addr, wp_err);
+			Z_ERR(megaz->znd, "Disabling Reset WP capability");
+			megaz->znd->zinqtype = 0;
+		}
+	}
+#endif /* CONFIG_BLK_ZONED_CTRL */
+	return wp_err;
+}
+
+static inline size_t max_report_size(size_t bufsz)
+{
+	return (bufsz - sizeof(struct bdev_zone_report_result_t)) /
+	                sizeof(struct bdev_zone_descriptor_entry_t);
+}
+
+static int dmz_report_zones(zoned_t * znd, u64 z_id, struct bdev_zone_report_result_t * out)
+{
+	// start of MZ:
+	int wp_err = -ENOTSUPP;
+
+#if GC_MAX_STRIPE < REPORT_BUFFER
+  #error "Should have enough buffer space for 4096 zones per call"
+#endif
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+	if (znd->zinqtype == Z_TYPE_SMR_HA) {
+		struct gendisk *disk = znd->dev->bdev->bd_disk;
+		u64 lba = (z_id + znd->first_zone) * Z_BLKSZ;
+		u64 s_addr = lba * Z_BLOCKS_PER_DM_SECTOR;
+		u8 * buf = (u8 *) znd->gc_io_buf;
+		size_t bufsz = REPORT_BUFFER * Z_C4K;
+		u8  opt = ZOPT_NON_SEQ_AND_RESET;
+                int err;
+
+		if (znd->ata_passthrough) {
+			err = blk_zoned_report_ata(disk, s_addr, opt, buf, bufsz);
+		} else {
+			err = blk_zoned_report(disk, s_addr, opt, buf, bufsz);
+		}
+
+		if (!err) {
+			u32 max_count = max_report_size(bufsz);
+			out = (struct bdev_zone_report_result_t *)buf;
+
+			if (max_count > be32_to_cpu(out->descriptor_count)) {
+				out->descriptor_count = cpu_to_be32(max_count);
+			}
+		} else {
+			Z_ERR(znd, "Open Zone: %llu -> %d failed.",
+			       s_addr, err);
+			Z_ERR(znd, "Disabling Reset WP capability");
+			znd->zinqtype = 0;
+		}
+	}
+#endif /* CONFIG_BLK_ZONED_CTRL */
+	return wp_err;
+}
+
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void zoned_actual_size(struct dm_target *ti, zoned_t * zoned)
+{
+	u64 size = i_size_read(zoned->dev->bdev->bd_inode);	// size in bytes.
+
+	zoned->nr_blocks = size / 4096;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_integrity_check(zoned_t * znd)
+{
+	int rc = 0;
+	if (znd->z_mega) {
+		u32 iter;
+		for (iter = 0; iter < znd->mega_zones_count; iter++) {
+			megazone_t *megaz = &znd->z_mega[iter];
+
+			set_bit(DO_META_CHECK, &megaz->flags);
+			queue_work(znd->meta_wq, &megaz->meta_work);
+		}
+
+		flush_workqueue(znd->meta_wq);
+
+		for (iter = 0; iter < znd->mega_zones_count; iter++) {
+			megazone_t *megaz = &znd->z_mega[iter];
+			if (megaz->meta_result) {
+				rc = megaz->meta_result;
+			}
+		}
+	}
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * <data dev> <format|check|force>
+ */
+static int zoned_constructor(struct dm_target *ti, unsigned argc, char **argv)
+{
+	int create = 0;
+	int force = 0;
+	int check = 0;
+	int zbc_probe = 1;
+	int zac_probe = 1;
+	int packed_meta = 1;
+	int trim = 1;
+	int r;
+	zoned_t *zoned;
+	long long starting_zone_nr = 0;
+	long long mz_md_provision = MZ_METADATA_ZONES;
+	int preserve_z0 = 0;
+
+	BUILD_BUG_ON(Z_C4K != (sizeof(z_mapcache_entry_t) * Z_UNSORTED));
+	BUILD_BUG_ON(Z_C4K != (sizeof(io_4k_t)) );
+	BUILD_BUG_ON(Z_C4K != (sizeof(mz_key_block_t)) );
+	BUILD_BUG_ON(SYNC_IO_SZ < (sizeof(mz_sync_io_t)) );
+
+	if (argc < 1) {
+		ti->error = "Invalid argument count";
+		return -EINVAL;
+	}
+
+	for (r = 1; r < argc; r++) {
+		if (isdigit(*argv[r])) {
+			int krc = kstrtoll(argv[r], 0, &starting_zone_nr);
+			if (krc != 0) {
+				DMERR("Failed to parse %s: %d", argv[r], krc);
+				starting_zone_nr = 0;
+			}
+		}
+		if (!strcasecmp("no_z0", argv[r])) {
+			preserve_z0 = 1;
+		}
+		if (!strcasecmp("create", argv[r])) {
+			create = 1;
+		}
+		if (!strcasecmp("load", argv[r])) {
+			create = 0;
+		}
+		if (!strcasecmp("check", argv[r])) {
+			check = 1;
+		}
+		if (!strcasecmp("force", argv[r])) {
+			force = 1;
+		}
+		if (!strcasecmp("nozbc", argv[r])) {
+			zbc_probe = 0;
+		}
+		if (!strcasecmp("nozac", argv[r])) {
+			zac_probe = 0;
+		}
+		if (!strcasecmp("meta=hm", argv[r])) {
+			packed_meta = 0;
+		}
+		if (!strcasecmp("discard", argv[r])) {
+			trim = 1;
+		}
+		if (!strcasecmp("nodiscard", argv[r])) {
+			trim = 0;
+		}
+		if (!strncasecmp("reserve=", argv[r], 8)) {
+			long long mz_resv;
+			int krc = kstrtoll(argv[r] + 8, 0, &mz_resv);
+			if (krc == 0) {
+				if (mz_resv > mz_md_provision) {
+					mz_md_provision = mz_resv;
+				}
+			} else {
+				DMERR("Reserved 'FAILED TO PARSE.' %s: %d", argv[r]+8, krc);
+				mz_resv = 0;
+			}
+		}
+	}
+	zoned = ZDM_ALLOC(NULL, sizeof(*zoned), KM_00);
+	if (!zoned) {
+		ti->error = "Error allocating zoned structure";
+		return -ENOMEM;
+	}
+
+	zoned->ti = ti;
+	ti->private = zoned;
+	zoned->first_zone = starting_zone_nr;
+	zoned->packed_meta = packed_meta;
+	zoned->mz_provision = mz_md_provision;
+
+	r = dm_get_device(ti, argv[0], FMODE_READ | FMODE_WRITE, &zoned->dev);
+	if (r) {
+		ti->error = "Error opening backing device";
+		zoned_destroy(zoned);
+		return -EINVAL;
+	}
+
+	if (zoned->dev->bdev) {
+		bdevname(zoned->dev->bdev, zoned->bdev_name);
+	}
+
+	if (0 == starting_zone_nr) {
+		zoned->preserve_z0 = preserve_z0;
+	} else {
+		Z_INFO(zoned, "First zone on device: %llx", starting_zone_nr);
+	}
+
+	/*
+	 * Set if this target needs to receive flushes regardless of
+	 * whether or not its underlying devices have support.
+	 */
+	ti->flush_supported = true;
+
+	/*
+	 * Set if this target needs to receive discards regardless of
+	 * whether or not its underlying devices have support.
+	 */
+	ti->discards_supported = true;
+
+	/*
+	 * Set if the target required discard bios to be split
+	 * on max_io_len boundary.
+	 */
+	ti->split_discard_bios = false;
+
+	/*
+	 * Set if this target does not return zeroes on discarded blocks.
+	 */
+	ti->discard_zeroes_data_unsupported = false;
+	/*
+	 * Set if this target wants discard bios to be sent.
+	 */
+	ti->num_discard_bios = 1;
+
+	if (! trim ) {
+		ti->discards_supported = false;
+		ti->num_discard_bios = 0;
+	}
+
+	zoned_actual_size(ti, zoned);
+
+	zoned->callbacks.congested_fn = zoned_is_congested;
+	dm_table_add_target_callbacks(ti->table, &zoned->callbacks);
+	r = zoned_init(ti, zoned);
+	if (r) {
+		ti->error = "Error in zoned init";
+		zoned_destroy(zoned);
+		return -EINVAL;
+	}
+	if (zbc_probe) {
+		Z_ERR(zoned, "Checking for ZONED support %s", trim ? "with trim" : "");
+		is_zoned_inquiry(zoned, trim, 0);
+	} else if (zac_probe) {
+		Z_ERR(zoned, "Checking for ZONED [ATA PASSTHROUGH] support %s", trim ? "with trim" : "");
+		is_zoned_inquiry(zoned, trim, 1);
+	} else {
+		Z_ERR(zoned, "No PROBE");
+		discard_support(zoned, trim);
+	}
+
+	r = megazone_init(zoned);
+	if (r) {
+		ti->error = "Error in zoned init megazone";
+		zoned_destroy(zoned);
+		return -EINVAL;
+	}
+	r = zoned_init_disk(ti, zoned, create, check, force);
+	if (r) {
+		ti->error = "Error in zoned init from disk";
+		zoned_destroy(zoned);
+		return -EINVAL;
+	}
+	r = megazone_wp_sync(zoned);
+	if (r) {
+		ti->error = "Error in zoned re-sync WP";
+		zoned_destroy(zoned);
+		return -EINVAL;
+	}
+
+	if (check) {
+		// for each megaz, for each non-zero entry in the crc_md table, load the
+		// page  (and check the crc).
+		zoned_integrity_check(zoned);
+	}
+
+	mod_timer(&zoned->timer, jiffies + msecs_to_jiffies(5000));
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void zoned_dtr(struct dm_target *ti)
+{
+	zoned_t *znd = ti->private;
+
+	if (znd->z_superblock) {
+		mz_key_block_t *key_blk = znd->z_superblock;
+		z_super_t *sblock = &key_blk->sblock;
+
+		sblock->flags = cpu_to_le32(0);
+		sblock->csum = sb_crc32(sblock);
+	}
+
+	megazone_destroy(znd);
+	zoned_destroy(znd);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Read or write a chunk aligned and sized block of data from a device.
+ */
+static void do_io_work(struct work_struct *work)
+{
+	struct z_io_req_t *req = container_of(work, struct z_io_req_t, work);
+	struct dm_io_request *io_req = req->io_req;
+	unsigned long error_bits = 0;
+
+	req->result = dm_io(io_req, 1, req->where, &error_bits);
+
+	if (error_bits) {
+		DMERR("ERROR: dm_io error: %lx", error_bits);
+	}
+
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int block_io(zoned_t * znd,
+		    enum dm_io_mem_type dtype,
+		    void *data,
+		    sector_t block, unsigned int nDMsect, int rw, int queue)
+{
+	struct dm_io_region where = {
+		.bdev = znd->dev->bdev,
+		.sector = block,
+		.count = nDMsect,
+	};
+	struct dm_io_request io_req = {
+		.bi_rw = rw,
+		.mem.type = dtype,
+		.mem.offset = 0,
+		.mem.ptr.vma = data,
+		.client = znd->io_client,
+		.notify.fn = NULL,
+		.notify.context = NULL,
+	};
+
+	switch (dtype) {
+		case DM_IO_KMEM:
+			io_req.mem.ptr.addr = data;
+		break;
+		case DM_IO_BIO:
+			io_req.mem.ptr.bio = data;
+			where.count = nDMsect;
+		break;
+		case DM_IO_VMA:
+			io_req.mem.ptr.vma = data;
+		break;
+		default:
+			Z_ERR(znd, "page list not handled here ..  see dm-io.");
+		break;
+	}
+
+	if (queue) {
+		struct z_io_req_t req;
+
+		/*
+		 * Issue the synchronous I/O from a different thread
+		 * to avoid generic_make_request recursion.
+		 */
+		INIT_WORK_ONSTACK(&req.work, do_io_work);
+		req.where = &where;
+		req.io_req = &io_req;
+		queue_work(znd->io_wq, &req.work);
+		flush_workqueue(znd->io_wq);
+		destroy_work_on_stack(&req.work);
+
+		return req.result;
+	} else {
+		unsigned long error_bits = 0;
+		int rcode = dm_io(&io_req, 1, &where, &error_bits);
+
+		if (error_bits) {
+			Z_ERR(znd, "ERROR: dm_io error: %lx", error_bits);
+		}
+
+		return rcode;
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * count -> count in 4k sectors.
+ */
+static int read_block(struct dm_target *ti, enum dm_io_mem_type dtype,
+		      void *data, u64 lba, unsigned int count, int queue)
+{
+	zoned_t *znd = ti->private;
+	sector_t block = lba * Z_BLOCKS_PER_DM_SECTOR;
+	unsigned int nDMsect = count * Z_BLOCKS_PER_DM_SECTOR;
+	int rc;
+
+	BUG_ON(lba >= znd->nr_blocks);
+
+	if (lba >= znd->nr_blocks) {
+		Z_ERR(znd, "Error reading past end of media: %llx.", lba);
+		rc = -EIO;
+		return rc;
+	}
+
+	rc = block_io(znd, dtype, data, block, nDMsect, READ, queue);
+	if (rc) {
+		Z_ERR(znd, "read error: %d -- R: %llx [%u dm sect] (Q:%d)",
+			rc, lba, nDMsect, queue);
+		dump_stack();
+	}
+
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * count -> count in 4k sectors.
+ */
+static int write_block(struct dm_target *ti, enum dm_io_mem_type dtype,
+		       void *data, u64 lba, unsigned int count, int queue)
+{
+	zoned_t *znd = ti->private;
+	sector_t block = lba * Z_BLOCKS_PER_DM_SECTOR;
+	unsigned int nDMsect = count * Z_BLOCKS_PER_DM_SECTOR;
+
+	int rc;
+
+	BUG_ON(lba >= znd->nr_blocks);
+
+	rc = block_io(znd, dtype, data, block, nDMsect, WRITE, queue);
+	if (rc) {
+		Z_ERR(znd, "write error: %d W: %llx [%u dm sect] (Q:%d)",
+			rc, lba, nDMsect, queue);
+		dump_stack();
+	}
+
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zm_cow(megazone_t * megaz, struct bio *bio, z_map_addr_t * maddr,
+		  u32 blks, u64 origin)
+{
+	struct dm_target *ti = megaz->znd->ti;
+	int count = 1;
+	int use_wq = 1;
+	unsigned int bytes = bio_cur_bytes(bio);
+	u8 *data = bio_data(bio);
+	u8 *io = NULL;
+	u16 ua_off = bio->bi_iter.bi_sector & 0x0007;
+	u16 ua_size = bio->bi_iter.bi_size & 0x0FFF;	/* in bytes */
+	u32 mapped = 0;
+	u64 disk_lba = 0;
+
+	if (!megaz->cow_block) {
+		megaz->cow_block = ZDM_ALLOC(megaz->znd, Z_C4K, PG_02); /* dm_io */
+	}
+	io = megaz->cow_block;
+
+	disk_lba = z_acquire(megaz, Z_AQ_NORMAL, blks, &mapped);
+
+#if EXTRA_DEBUG
+	Z_DBG(megaz->znd, "%s: %s [%s] [%s] s:%llu+%u sz:%u [%llu]",
+		 __func__, "W",
+		 bio->bi_rw & REQ_META ? "meta" : "data",
+		 bio->bi_rw & REQ_PRIO ? "high" : "norm",
+		 maddr->dm_s, ua_off, bio->bi_iter.bi_size, disk_lba);
+#endif
+
+	if (!disk_lba || !mapped)
+		return -ENOSPC;
+
+	if (!io)
+		return -EIO;
+
+	while (bytes) {
+		int rd;
+		unsigned int iobytes = Z_C4K;
+
+		/* ---------------------------------------------------------- */
+		if (origin) {
+			if (maddr->dm_s != megaz->cow_addr) {
+				Z_ERR(megaz->znd, "Copy block from %llx <= %llx",
+					origin, maddr->dm_s);
+				rd = read_block(ti, DM_IO_KMEM, io, origin,
+						count, use_wq);
+				if (rd) {
+					return -EIO;
+				}
+				megaz->cow_addr = maddr->dm_s;
+			} else {
+				Z_ERR(megaz->znd, "Cached block from %llx <= %llx",
+					origin, maddr->dm_s);
+			}
+		} else {
+			memset(io, 0, Z_C4K);
+		}
+
+		if (ua_off) {
+			iobytes -= ua_off * 512;
+		}
+		if (bytes < iobytes) {
+			iobytes = bytes;
+		}
+		Z_ERR(megaz->znd, "Moving %u bytes from origin [offset:%u]",
+		      iobytes, ua_off * 512);
+
+		memcpy(io + (ua_off * 512), data, iobytes);
+
+		/* ---------------------------------------------------------- */
+
+		rd = write_block(ti, DM_IO_KMEM, io, disk_lba, count, use_wq);
+		if (rd) {
+			return -EIO;
+		}
+
+		rd = z_mapped_addmany(megaz, maddr->dm_s, disk_lba, mapped);
+		if (rd) {
+			Z_ERR(megaz->znd, "%s: Journal MANY failed.", __func__);
+			return -EIO;
+		}
+
+		data += iobytes;
+		bytes -= iobytes;
+		ua_size -= (ua_size > iobytes) ? iobytes : ua_size;
+		ua_off = 0;
+		disk_lba++;
+
+		if (bytes && (ua_size || ua_off)) {
+			map_addr_calc(maddr->dm_s + 1, maddr);
+			origin = z_lookup(megaz, maddr);
+		}
+	}
+
+	// ? do I have to end the bio?
+	bio_endio(bio, 0);
+
+	return DM_MAPIO_SUBMITTED;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+#define BIO_CACHE_SECTORS (SYNC_CACHE_PAGES * Z_BLOCKS_PER_DM_SECTOR)
+
+static int zm_write_cache(zoned_t * znd, io_dm_t * sync_cache, u64 lba, u32 * _cached)
+{
+	int use_wq    = 1;
+	int cached    = *_cached;
+	int blks      = cached / 8;
+	int sectors   = blks * 8;
+	int remainder = cached - sectors;
+	int err;
+
+	err = write_block(znd->ti, DM_IO_KMEM, sync_cache, lba, blks, use_wq);
+	if (!err) {
+		if (remainder) {
+			memcpy(sync_cache[0].data, sync_cache[sectors].data, remainder * 512 );
+		}
+		*_cached = remainder;
+	}
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zm_write_pages(megazone_t * megaz, struct bio *bio, z_map_addr_t * maddr)
+{
+	io_dm_t * sync_cache = (io_dm_t *)megaz->sync_cache;
+	u32 blks     = dm_div_up(bio->bi_iter.bi_size, Z_C4K);
+	u64 lba      = 0;
+	u32 cached   = 0; /* total: SYNC_CACHE_PAGES * 8 */
+	u32 written  = 0;
+	int avail    = 0;
+	int err;
+
+	struct bvec_iter start;
+        struct bvec_iter iter;
+        struct bio_vec bv;
+
+/* USE: megaz->sync_cache for dumping bio pages to disk ... */
+
+	start = bio->bi_iter; /* struct implicit copy */
+	do {
+		u64 map_origin = 0;
+		u32 mcount = 0;
+		u32 mapped = 0;
+
+		lba = z_acquire(megaz, Z_AQ_NORMAL, blks - written, &mapped);
+		if (!lba && mapped) {
+			lba = z_acquire(megaz, Z_AQ_NORMAL, mapped, &mapped);
+		}
+		if (lba && mapped) {
+			avail += mapped * 8; /* claimed pages in dm blocks */
+		}
+		if (!avail) {
+			return -ENOSPC;
+		}
+
+		map_origin = lba;
+
+		// peal off some pages ...
+		__bio_for_each_segment(bv, bio, iter, start) {
+			void * src;
+
+			if (avail <= 0) {
+				Z_DBG(megaz->znd, "%s: TBD: Close Z# %llu",
+					__func__, map_origin >> 16);
+				start = iter;
+				break;
+			}
+
+			src = kmap_atomic(bv.bv_page);
+			memcpy(sync_cache[cached].data,
+				src + bv.bv_offset, bv.bv_len);
+			kunmap_atomic(src);
+			cached += bv.bv_len / 512;
+			avail  -= bv.bv_len / 512;
+
+			/* if there is less than 1 4k block in out cache,
+			 * send the available blocks to disk */
+			if ( cached >= (BIO_CACHE_SECTORS - 8) ) {
+				int blks = cached / 8;
+				err = zm_write_cache(megaz->znd, sync_cache, lba, &cached);
+				if (err) {
+					Z_ERR(megaz->znd, "%s: bio-> %" PRIx64
+					      " [%d of %d blks] -> %d",
+					      __func__, lba, cached, blks, err);
+					bio_endio(bio, err);
+					goto out;
+				}
+				lba     += blks;
+				written += blks;
+				mcount  += blks;
+			}
+		}
+		if ( (cached / 8) > 0 ) {
+			int blks = cached / 8;
+			err = zm_write_cache(megaz->znd, sync_cache, lba, &cached);
+			if (err) {
+				Z_ERR(megaz->znd, "%s: bio-> %" PRIx64
+				      " [%d of %d blks] -> %d",
+				      __func__, lba, cached, blks, err);
+				bio_endio(bio, err);
+				goto out;
+			}
+			lba     += blks;
+			written += blks;
+			mcount  += blks;
+		}
+
+		err = z_mapped_addmany(megaz, maddr->dm_s, map_origin, mcount);
+		if (err) {
+			Z_ERR(megaz->znd, "%s: Journal MANY failed.", __func__);
+			err = -EIO;
+			bio_endio(bio, err);
+			return err;
+		}
+
+		if (written < blks) {
+			map_addr_calc(maddr->dm_s + written, maddr);
+		}
+
+		if (written == blks && cached > 0) {
+			Z_ERR(megaz->znd, "%s: cached: %d un-written blocks!!",
+			      __func__, cached );
+		}
+
+	} while (written < blks);
+	bio_endio(bio, 0);
+
+out:
+	return DM_MAPIO_SUBMITTED;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static int zoned_map_write(megazone_t * megaz, struct bio *bio, z_map_addr_t * maddr)
+{
+	u32 blks     = dm_div_up(bio->bi_iter.bi_size, Z_C4K);
+	u16 ua_off   = bio->bi_iter.bi_sector & 0x0007;
+	u16 ua_size  = bio->bi_iter.bi_size & 0x0FFF;	/* in bytes */
+	if (ua_size || ua_off) {
+		u64 origin = z_lookup(megaz, maddr);
+		if (origin) {
+			return zm_cow(megaz, bio, maddr, blks, origin);
+		}
+	}
+	return zm_write_pages(megaz, bio, maddr);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_map_read(zoned_t * znd, struct bio *bio)
+{
+	int rcode = DM_MAPIO_REMAPPED;
+	u64 ua_off = bio->bi_iter.bi_sector & 0x0007;
+	u64 ua_size = bio->bi_iter.bi_size & 0x0FFF;	/* in bytes */
+	u64 s_up = bio->bi_iter.bi_sector >> 3;
+	u64 blks = dm_div_up(bio->bi_iter.bi_size, Z_C4K);
+	z_map_addr_t maddr;
+	u64 start_lba;
+	megazone_t *megaz = NULL;
+
+	map_addr_onto_dm_zoned(znd, s_up, &maddr);
+	megaz = &znd->z_mega[maddr.mz_id];
+	start_lba = z_lookup(megaz, &maddr);
+
+	if (start_lba) {
+		u64 sz;
+
+		bio->bi_iter.bi_sector = start_lba << 3;
+		if (ua_off) {
+			bio->bi_iter.bi_sector += ua_off;
+		}
+
+		for (sz = 1; sz < blks; sz++) {
+			u64 next_lba;
+
+			map_addr_onto_dm_zoned(znd, s_up+sz, &maddr);
+			megaz = &znd->z_mega[maddr.mz_id];
+			next_lba = z_lookup(megaz, &maddr);
+			if (next_lba != (start_lba + sz)) {
+				unsigned nsect = sz * 8;
+				if (ua_size) {
+					unsigned ua_blocks = ua_size / 512;
+					nsect -= 8;
+					nsect += ua_blocks;
+				}
+				Z_DBG(megaz->znd,
+					"NON SEQ @ %llx + %llu [%llx] [%llx]",
+					 maddr.dm_s, sz, start_lba, next_lba);
+				dm_accept_partial_bio(bio, nsect);
+				return rcode;
+			}
+		}
+
+		if (ua_off || ua_size) {
+			Z_ERR(megaz->znd, "zoned_map_read: Submitting"
+			      " bio: sector: %lx bytes: %u",
+			      bio->bi_iter.bi_sector, bio->bi_iter.bi_size);
+		}
+#if EXTRA_DEBUG
+		Z_DBG(znd, "%s: %s [%s] [%s] s:%llx sz:%llu [%llx]",
+			 __func__, "R",
+			 bio->bi_rw & REQ_META ? "meta" : "data",
+			 bio->bi_rw & REQ_PRIO ? "high" : "norm",
+			 maddr.dm_s, blks, start_lba);
+#endif
+
+		generic_make_request(bio);
+		rcode = DM_MAPIO_SUBMITTED;
+	} else {
+		/* drop read-ahead if not marked as used */
+		if (READA == bio_rw(bio)) {
+			return -EIO;
+		} else {
+			/* return 0's for deleted/unused blocks */
+			Z_DBG(znd, "%s: R s:%lx -> %llx sz:%llu (zero fill)",
+				 __func__, bio->bi_iter.bi_sector,
+				 maddr.dm_s, blks);
+
+			zero_fill_bio(bio);
+			bio_endio(bio, 0);
+			return DM_MAPIO_SUBMITTED;
+		}
+	}
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 mz_final(zoned_t *znd, struct bio * bio)
+{
+	u64 blks = dm_div_up(bio->bi_iter.bi_size, Z_C4K);
+	u64 s_up = bio->bi_iter.bi_sector >> 3;
+	z_map_addr_t maddr;
+
+	if (blks > 0) {
+		s_up += (blks - 1);
+	}
+	map_addr_onto_dm_zoned(znd, s_up, &maddr);
+
+	return maddr.mz_id;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+/*
+ * Return the number of 4k sectors available
+ */
+static u32 mz_bio_blocks(zoned_t *znd, struct bio * bio, u64 mz_id)
+{
+	u64 s_up = bio->bi_iter.bi_sector >> 3;
+	u32 blks = dm_div_up(bio->bi_iter.bi_size, Z_C4K);
+	u32 count;
+	z_map_addr_t maddr;
+
+	for (count = 0; count < blks; count++) {
+		map_addr_onto_dm_zoned(znd, s_up+count, &maddr);
+		if (mz_id != maddr.mz_id) {
+			break;
+		}
+	}
+	return count;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_map(struct dm_target *ti, struct bio *bio)
+{
+	zoned_t *znd = ti->private;
+	bool is_write = (bio_data_dir(bio) == WRITE);
+	sector_t sector_nr = bio->bi_iter.bi_sector / Z_BLOCKS_PER_DM_SECTOR;
+	int rcode = DM_MAPIO_REMAPPED;
+	z_map_addr_t maddr;
+	struct request_queue *q;
+	megazone_t *megaz = NULL;
+	int force_sync_now = 0;
+	struct block_device *bdev = bio->bi_bdev;
+
+	/* map to backing device ... NOT dm-zoned device */
+	bio->bi_bdev = znd->dev->bdev;
+
+	q = bdev_get_queue(bio->bi_bdev);
+	q->queue_flags |= QUEUE_FLAG_NOMERGES;
+
+	/* sector is from the upper layer (fs, gparted, etc) */
+	map_addr_onto_dm_zoned(znd, sector_nr, &maddr);
+
+	if (maddr.dm_s >= znd->nr_blocks) {
+		Z_ERR(znd, "%s requested %lu -> %llu is too large for device (%llu)",
+		      __func__, sector_nr, maddr.dm_s, znd->nr_blocks);
+		return -ENOSPC;
+	}
+
+	megaz = &znd->z_mega[maddr.mz_id];
+	mutex_lock(&megaz->mz_io_mutex);
+
+	// check for SYNC flag
+	if (bio->bi_rw & REQ_SYNC) {
+		set_bit(DO_SYNC, &megaz->flags);
+		force_sync_now = 1;
+	}
+
+	Z_DBG(znd, "%s: s:%lu sz:%u -> %s [%llu]", __func__,
+		 sector_nr, bio->bi_iter.bi_size,
+		 is_write ? "W" : "R", maddr.mz_id);
+
+	if (bio->bi_iter.bi_size) {
+		if (bio->bi_rw & REQ_DISCARD) {
+			znd->gc_mz_pref = maddr.mz_id;
+			mutex_unlock(&megaz->mz_io_mutex);
+			rcode = zoned_map_discard(znd, bio);
+			mutex_lock(&megaz->mz_io_mutex);
+		} else if (is_write) {
+			znd->gc_mz_pref = maddr.mz_id;
+			if (mz_final(znd, bio) != maddr.mz_id) {
+				u32 accept;
+
+				accept = mz_bio_blocks(znd, bio, maddr.mz_id);
+				/*
+				 * accept number of 4k blocks -> 512 blocks
+				 * and have the upper layer remap them back
+				 * to ZDM.
+				 */
+				bio->bi_bdev = bdev;
+				dm_accept_partial_bio(bio, accept << 3);
+				rcode = DM_MAPIO_REMAPPED;
+				Z_ERR(znd, "ReMap to self [crossing MZ] %u", accept);
+				mutex_unlock(&megaz->mz_io_mutex);
+				return rcode;
+			} else {
+				rcode = zoned_map_write(megaz, bio, &maddr);
+			}
+		} else {
+			rcode = zoned_map_read(znd, bio);
+		}
+		megaz->age = jiffies;
+	}
+
+	if (test_bit(DO_SYNC, &megaz->flags) ||
+	    test_bit(DO_JOURNAL_MOVE, &megaz->flags) ||
+	    test_bit(DO_MEMPOOL, &megaz->flags)) {
+		if (!test_bit(DO_METAWORK_QD, &megaz->flags)) {
+			set_bit(DO_METAWORK_QD, &megaz->flags);
+			queue_work(znd->meta_wq, &megaz->meta_work);
+		}
+	}
+
+	mutex_unlock(&megaz->mz_io_mutex);
+
+	if (force_sync_now) {
+		flush_workqueue(znd->meta_wq);
+	} else if (megaz->z_gc_free < 5) {
+
+		Z_ERR(znd, "MZ#%u space is tight: free %d",
+		      megaz->mega_nr, megaz->z_gc_free);
+
+		gc_compact_check(megaz, 0);
+
+		if ( test_and_set_bit(ZF_NO_GC_DELAY, &znd->flags) ) {
+			if (work_pending(&znd->gc_work)) {
+				flush_workqueue(znd->gc_wq);
+			}
+		}
+	}
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int _do_mem_purge(megazone_t * megaz, u64 mem_time)
+{
+	int do_work = 0;
+
+	if (time_before64(megaz->age, mem_time)) {
+		if (megaz->incore_count > 3) {
+			set_bit(DO_MEMPOOL, &megaz->flags);
+			if (!work_pending(&megaz->meta_work)) {
+				do_work = 1;
+			}
+		}
+	}
+	return do_work;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int gc_can_cherrypick(megazone_t * megaz)
+{
+	int delay = 0;
+	int z_gc = megaz->z_data - 1;
+
+	for (; z_gc < megaz->z_count; z_gc++) {
+		const int is_ready = (megaz->z_ptrs[z_gc] & Z_WP_GC_READY) ? 1 : 0;
+		const u32 wp = megaz->z_ptrs[z_gc] & Z_WP_VALUE_MASK;
+		const u32 nfree = megaz->zfree_count[z_gc];
+		if (is_ready && (wp == Z_BLKSZ) && (nfree == Z_BLKSZ)) {
+			if (gc_compact_check(megaz, delay)) {
+				return 1;
+			}
+		}
+	}
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void on_timeout_activity(zoned_t * znd)
+{
+	int gc_idle = 0;
+	u64 gc_time = msecs_to_jiffies(1000);
+	u64 mem_time = msecs_to_jiffies(5000);
+	u64 tnow = jiffies;
+	unsigned long flags;
+	megazone_t *megaz;
+	u32 iter;
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	if (!znd->gc_active) {
+		gc_idle = 1;
+	}
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+
+	gc_time = (gc_time < tnow) ? tnow - gc_time : 0;
+	mem_time = (mem_time < tnow) ? tnow - mem_time : 0;
+
+	if (!znd->z_mega) {
+		return;
+	}
+
+	megaz = &znd->z_mega[znd->gc_mz_pref];
+	if (gc_can_cherrypick(megaz)) {
+		gc_idle = 0;
+	}
+	for (iter = 0; iter < znd->mega_zones_count; iter++) {
+		megaz = &znd->z_mega[iter];
+		if (megaz) {
+			if (gc_idle && time_before64(megaz->age, gc_time)) {
+				if (gc_can_cherrypick(megaz)) {
+					gc_idle = 0;
+				}
+			}
+		}
+	}
+	for (iter = 0; iter < znd->mega_zones_count; iter++) {
+		megaz = &znd->z_mega[iter];
+		if (megaz) {
+			if (gc_idle && time_before64(megaz->age, gc_time)) {
+				int delay = 0;
+				if (gc_compact_check(megaz, delay)) {
+					gc_idle = 0;
+				}
+			}
+			if (_do_mem_purge(megaz, mem_time)) {
+				queue_work(znd->meta_wq, &megaz->meta_work);
+			}
+		}
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void bg_work_task(struct work_struct *work)
+{
+	zoned_t *znd;
+	if (!work) {
+		return;
+	}
+	znd = container_of(work, zoned_t, bg_work);
+	on_timeout_activity(znd);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void activity_timeout(unsigned long data)
+{
+	zoned_t *znd = (zoned_t *) data;
+
+	if (!work_pending(&znd->bg_work)) {
+		queue_work(znd->bg_wq, &znd->bg_work);
+	}
+	if (! test_bit(ZF_FREEZE, &znd->flags) ) {
+		mod_timer(&znd->timer, jiffies + msecs_to_jiffies(1000));
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static sector_t get_dev_size(struct dm_target *ti)
+{
+	zoned_t *znd = ti->private;
+
+	u64 sz = i_size_read(get_bdev_bd_inode(znd));	/* size in bytes. */
+        u64 lut_resv;
+	lut_resv = (znd->mega_zones_count * znd->mz_provision);
+	if (znd->preserve_z0) {
+		lut_resv++;
+	}
+
+	Z_DBG(znd, "%s device size: %llu (/8) -> blocks: %llu"
+		 " -> zones -> %llu mz: %llu",
+		 __func__, sz, sz / 4096, (sz / 4096) / 65536,
+		 ((sz / 4096) / 65536) / 1024);
+
+	sz -= (lut_resv * Z_SMR_SZ_BYTES);
+
+	Z_DBG(znd, "%s backing device size: %llu (4k blocks)", __func__, sz);
+
+	/* NOTE: `sz` should match `ti->len` when the dm_table
+	 *       is setup correctly */
+
+	return to_sector(sz);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_iterate_devices(struct dm_target *ti,
+				 iterate_devices_callout_fn fn, void *data)
+{
+	zoned_t *zoned = ti->private;
+	int rc = fn(ti, zoned->dev, 0, get_dev_size(ti), data);
+
+	Z_DBG(zoned, "%s: %p -> rc: %d", __func__, fn, rc);
+
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Follow the backing device limits for READ [and DISCARD].
+ * Limit WRITE requests to the current zone max [Enforced in ->map()]
+ */
+static int zoned_merge(struct dm_target *ti, struct bvec_merge_data *bvm,
+		       struct bio_vec *biovec, int max_size)
+{
+	zoned_t *znd = ti->private;
+	struct request_queue *q = bdev_get_queue(znd->dev->bdev);
+	sector_t sector_nr = bvm->bi_sector / Z_BLOCKS_PER_DM_SECTOR;
+	megazone_t *megaz = NULL;
+	z_map_addr_t maddr;
+	z_wptr_t wptr = 0;
+	u32 avail = 0;
+	int zmax = 4096;
+	int bdev_max = 4096;
+	int in_max = max_size;
+
+	map_addr_onto_dm_zoned(znd, sector_nr, &maddr);
+
+	bvm->bi_bdev = znd->dev->bdev;
+	bvm->bi_sector = maddr.dm_s * Z_BLOCKS_PER_DM_SECTOR;
+
+	if (q->merge_bvec_fn) {
+		bdev_max = q->merge_bvec_fn(q, bvm, biovec);
+		if (max_size > bdev_max) {
+			max_size = bdev_max;
+		}
+	}
+
+	megaz = &znd->z_mega[maddr.mz_id];
+	wptr = megaz->z_ptrs[megaz->z_current];
+
+	if (wptr < Z_BLKSZ) {
+		avail = Z_BLKSZ - wptr;
+	}
+	if (avail > 25) {
+		avail = 25; /* arbitrary I/O limit in 4k blocks.*/
+	}
+	avail *= 4096;
+	if (avail) {
+		zmax = avail;
+	}
+	if (max_size > zmax) {
+		max_size = zmax;
+	}
+
+	Z_DBG(znd, "%s: S:%lx sz:%u rw:%lx -> rc: [in:%d, bv:%d, av:%d] %d",
+		__func__,
+		bvm->bi_sector, bvm->bi_size, bvm->bi_rw,
+		in_max, bdev_max, zmax, max_size);
+
+
+	return max_size;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void zoned_io_hints(struct dm_target *ti, struct queue_limits *limits)
+{
+	u64 io_opt_sectors = limits->io_opt >> SECTOR_SHIFT;
+
+	/*
+	 * If the system-determined stacked limits are compatible with the
+	 * zoned device's blocksize (io_opt is a factor) do not override them.
+	 */
+	if (io_opt_sectors < 8 || do_div(io_opt_sectors, 8)) {
+		blk_limits_io_min(limits, 0);
+		blk_limits_io_opt(limits, 8 << SECTOR_SHIFT);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void zoned_status(struct dm_target *ti, status_type_t type,
+			 unsigned status_flags, char *result, unsigned maxlen)
+{
+	zoned_t *znd = (zoned_t *) ti->private;
+
+	switch (type) {
+	case STATUSTYPE_INFO:
+		result[0] = '\0';
+		break;
+
+	case STATUSTYPE_TABLE:
+		scnprintf(result, maxlen, "%s Z#%llu", znd->dev->name,
+			 znd->first_zone);
+		break;
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_ioctl_fwd(struct dm_dev *dev, unsigned int cmd,
+			   unsigned long arg)
+{
+	int r = scsi_verify_blk_ioctl(NULL, cmd);
+
+	if (0 == r) {
+		r = __blkdev_driver_ioctl(dev->bdev, dev->mode, cmd, arg);
+	}
+
+	return r;
+}
+
+static int do_ioc_wpstat(zoned_t * znd, unsigned long arg, int what)
+{
+	void __user *parg = (void __user *)arg;
+	int error = -EFAULT;
+	struct zdm_ioc_request * req;
+
+	req = kzalloc(sizeof(*req), GFP_KERNEL);
+
+	if (!req) {
+		error = -ENOMEM;
+		goto out;
+	}
+
+	if (copy_from_user(req, parg, sizeof(*req))) {
+		goto out;
+	}
+	if (req->megazone_nr < znd->mega_zones_count) {
+		megazone_t *megaz = &znd->z_mega[req->megazone_nr];
+		u32 reply_sz =
+		    req->result_size < Z_C4K ? req->result_size : Z_C4K;
+		void *send_what = what ? megaz->z_ptrs : megaz->zfree_count;
+
+		if (copy_to_user(parg, send_what, reply_sz)) {
+			error = -EFAULT;
+		}
+		error = 0;
+	}
+out:
+	if (req) {
+		kfree(req);
+	}
+
+	return error;
+}
+
+static void fill_ioc_status(megazone_t * megaz, struct zdm_ioc_status *status)
+{
+	int entry;
+
+	memset(status, 0, sizeof(*status));
+	status->mc_entries = megaz->mc_entries;
+
+	for (entry = 0; entry < megaz->z_count; entry++) {
+		u32 used = megaz->z_ptrs[entry] & Z_WP_VALUE_MASK;
+		u32 avail = Z_BLKSZ - used;
+		u32 discard = (used == Z_BLKSZ) ? megaz->zfree_count[entry] : 0;
+
+		status->b_used += used;
+		status->b_available += avail;
+		status->b_discard += discard;
+	}
+
+	/*  fixed array of ->sectortm and ->reversetm */
+	status->m_used = 2 * ((sizeof(z_mapped_t *) * Z_BLKSZ) / 4096);
+	status->inpool = megaz->znd->memstat;
+	memcpy(status->bins, megaz->znd->bins, sizeof(status->bins));
+	status->mlut_blocks = megaz->incore_count;
+
+	for (entry = 0; entry < MZKY_NCRC; entry++) {
+		if (megaz->stm_crc[entry].crc_pg) {
+			status->crc_blocks++;
+		}
+		if (megaz->rtm_crc[entry].crc_pg) {
+			status->crc_blocks++;
+		}
+	}
+}
+
+static int do_ioc_status(zoned_t * znd, unsigned long arg)
+{
+	void __user *parg = (void __user *)arg;
+	int error = -EFAULT;
+	struct zdm_ioc_request * req;
+	struct zdm_ioc_status * stats;
+
+	req = kzalloc(sizeof(*req), GFP_KERNEL);
+	stats = kzalloc(sizeof(*stats), GFP_KERNEL);
+
+	if (!req || !stats) {
+		error = -ENOMEM;
+		goto out;
+	}
+
+	if (copy_from_user(req, parg, sizeof(*req))) {
+		goto out;
+	}
+	if (req->megazone_nr < znd->mega_zones_count) {
+		megazone_t *megaz = &znd->z_mega[req->megazone_nr];
+
+		if (req->result_size < sizeof(*stats)) {
+			error = -EBADTYPE;
+			goto out;
+		}
+		fill_ioc_status(megaz, stats);
+		if (copy_to_user(parg, stats, sizeof(*stats))) {
+			error = -EFAULT;
+		}
+		error = 0;
+	}
+out:
+
+	if (req) {
+		kfree(req);
+	}
+	if (stats) {
+		kfree(stats);
+	}
+	return error;
+}
+
+static int zoned_ioctl(struct dm_target *ti, unsigned int cmd,
+		       unsigned long arg)
+{
+	int rcode = 0;
+
+	zoned_t *znd = (zoned_t *) ti->private;
+
+	switch (cmd) {
+	case ZDM_IOC_MZCOUNT:
+		rcode = znd->mega_zones_count;
+		break;
+	case ZDM_IOC_WPS:
+		do_ioc_wpstat(znd, arg, 1);
+		break;
+	case ZDM_IOC_FREE:
+		do_ioc_wpstat(znd, arg, 0);
+		break;
+	case ZDM_IOC_STATUS:
+		do_ioc_status(znd, arg);
+		break;
+
+	default:
+		rcode = zoned_ioctl_fwd(znd->dev, cmd, arg);
+		break;
+	}
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static void start_worker(zoned_t * znd)
+{
+	clear_bit(ZF_FREEZE, &znd->flags);
+	atomic_set(&znd->suspended, 0);
+	mod_timer(&znd->timer, jiffies + msecs_to_jiffies(5000));
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static void stop_worker(zoned_t * znd)
+{
+	set_bit(ZF_FREEZE, &znd->flags);
+	atomic_set(&znd->suspended, 1);
+	megazone_flush_all(znd);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static void zoned_postsuspend(struct dm_target *ti)
+{
+	zoned_t *zoned = ti->private;
+	stop_worker(zoned);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static int zoned_preresume(struct dm_target *ti)
+{
+	zoned_t *zoned = ti->private;
+	start_worker(zoned);
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static struct target_type zoned_target = {
+	.name = "zoned",
+	.module = THIS_MODULE,
+	.version = {1, 0, 0},
+	.ctr = zoned_constructor,
+	.dtr = zoned_dtr,
+	.map = zoned_map,
+
+	.postsuspend = zoned_postsuspend,
+	.preresume = zoned_preresume,
+	.status = zoned_status,
+//  .message = zoned_message,
+	.ioctl = zoned_ioctl,
+
+	.iterate_devices = zoned_iterate_devices,
+	.merge = zoned_merge,
+	.io_hints = zoned_io_hints
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static int __init dm_zoned_init(void)
+{
+	int r;
+
+	r = dm_register_target(&zoned_target);
+	if (r) {
+		DMERR("zoned target registration failed: %d", r);
+		return r;
+	}
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static void __exit dm_zoned_exit(void)
+{
+	dm_unregister_target(&zoned_target);
+}
+
+module_init(dm_zoned_init);
+module_exit(dm_zoned_exit);
+
+MODULE_DESCRIPTION(DM_NAME " zoned target for Host Aware/Managed drives.");
+MODULE_AUTHOR("Shaun Tancheff <shaun.tancheff@seagate.com>");
+MODULE_LICENSE("GPL");
diff --git a/drivers/md/dm-zoned.h b/drivers/md/dm-zoned.h
new file mode 100644
index 0000000..8f38634
--- /dev/null
+++ b/drivers/md/dm-zoned.h
@@ -0,0 +1,466 @@
+/*
+ * Kernel Device Mapper for abstracting ZAC/ZBC devices as normal
+ * block devices for linux file systems.
+ *
+ * Copyright (C) 2015 Seagate Technology PLC
+ *
+ * Written by:
+ * Shaun Tancheff <shaun.tancheff@seagate.com>
+ *
+ * This file is licensed under  the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#ifndef _DM_ZONED_H
+#define _DM_ZONED_H
+
+#define ZDM_IOC_MZCOUNT 0x5a4e0001
+#define ZDM_IOC_WPS     0x5a4e0002
+#define ZDM_IOC_FREE    0x5a4e0003
+#define ZDM_IOC_STATUS  0x5a4e0004
+
+#define DM_MSG_PREFIX "zoned"
+
+#define ZDM_RESERVED_ZNR         0
+#define ZDM_CRC_STASH_ZNR        1 /* first 64 blocks */
+#define ZDM_REVERSE_MAP_ZNR      2
+#define ZDM_SECTOR_MAP_ZNR       3
+#define ZDM_DATA_START_ZNR       4
+
+#define Z_WP_GC_FULL            (1 << 31)
+#define Z_WP_GC_ACTIVE          (1 << 30)
+#define Z_WP_GC_READY           (1 << 29)
+#define Z_WP_NON_SEQ            (1 << 28)
+
+#define Z_WP_GC_PENDING         (Z_WP_GC_FULL|Z_WP_GC_ACTIVE)
+
+//#define Z_WP_TYPE_CONV          (1 << 27)
+//#define Z_WP_TYPE_CONV          (1 << 26)
+//#define Z_WP_TYPE_CONV          (1 << 25)
+//#define Z_WP_TYPE_CONV          (1 << 24)
+
+#define Z_WP_VALUE_MASK         (~0u >> 8)
+#define Z_WP_FLAGS_MASK         (~0u << 24)
+
+#define Z_AQ_GC                 (1 << 31)
+#define Z_AQ_META               (1 << 30)
+#define Z_AQ_NORMAL             (0)
+
+typedef u32 z_wptr_t;
+typedef __le32 remap_sector_t;
+
+#define Z_C4K                   (4096ul)
+#define Z_UNSORTED              (Z_C4K / sizeof(z_mapcache_entry_t))
+#define Z_BLOCKS_PER_DM_SECTOR  (Z_C4K/512)
+#define MZ_METADATA_ZONES       (8ul)
+
+#define SUPERBLOCK_LOCATION     0
+#define SUPERBLOCK_MAGIC        0x5a6f4e65	/* ZoNe */
+#define SUPERBLOCK_CSUM_XOR     146538381
+#define MIN_ZONED_VERSION       1
+#define Z_VERSION               1
+#define MAX_ZONED_VERSION       1
+#define INVALID_WRITESET_ROOT   SUPERBLOCK_LOCATION
+
+#define UUID_LEN 16
+
+#define Z_TYPE_SMR		2
+#define Z_TYPE_SMR_HA 		1
+#define Z_VPD_INFO_BYTE		8
+
+enum superblock_flags_t {
+	SB_DIRTY = 1,
+	SB_Z0_RESERVED,
+};
+
+struct z_io_req_t {
+	struct dm_io_region *where;
+	struct dm_io_request *io_req;
+	struct work_struct work;
+	int result;
+};
+
+enum jentry_t {
+	DATA_MAP = 1,
+	ZONE_SWAP = 2,		// used to indicate the location of the 'free' zone.
+	CHECKPOINT = 3,
+	META_MAP = 4,
+
+	DELAYED = 13,
+	DIRTY = 14,
+	DELETED = 15,
+};
+
+enum jmatch_t {
+	MATCH_SECTOR = 0,
+	MATCH_BLOCK,
+};
+
+#define Z_LOWER48 (~0ul >> 16)
+#define Z_UPPER16 (~Z_LOWER48)
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+enum mapped_flags_enum {
+	IS_DIRTY,
+	IS_GC,
+};
+typedef enum mapped_flags_enum mapped_flags_e;
+
+enum work_flags_enum {
+	DO_JOURNAL_MOVE,
+	DO_MEMPOOL,
+	DO_SYNC,
+	DO_SYNC_CRC,
+	DO_JOURNAL_LOAD,
+	DO_META_CHECK,
+	DO_GC_NO_PURGE,
+	DO_METAWORK_QD,
+};
+typedef enum work_flags_enum work_flags_e;
+
+enum gc_flags_enum {
+	DO_GC_NEW,
+	DO_GC_PREPARE,		// -> READ or COMPLETE state
+	DO_GC_WRITE,
+	DO_GC_META,		// -> PREPARE state
+	DO_GC_COMPLETE,
+};
+typedef enum gc_flags_enum gc_flags_e;
+
+enum znd_flags_enum {
+	ZF_PRESERVE_ZONE0,
+	ZF_PACKED_META,
+	ZF_IS_ZONED_TYPE,
+	ZF_USE_ATA_MODE,
+	ZF_NO_GC_DELAY,
+	ZF_FREEZE,
+	// DO_GC_NO_DELAY,
+};
+typedef enum znd_flags_enum znd_flags_e;
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct z_super_t;
+struct zoned;
+struct megazone_type;
+struct z_mapped_type;
+struct z_mapcache_entry_type;
+struct z_map_addr_type;
+struct z_mapcache_type;
+struct mz_crc_block_type;
+struct mz_key_block_type;
+struct mz_sync_io_type;
+struct io_4k_block;
+struct mz_logical_addr_map_type;
+
+typedef struct mz_logical_addr_map_type mz_lam_t;
+typedef struct zoned 			zoned_t;
+typedef struct megazone_type 		megazone_t;
+typedef struct z_mapped_type 		z_mapped_t;
+typedef struct z_mapcache_entry_type 	z_mapcache_entry_t;
+typedef struct z_map_addr_type 		z_map_addr_t;
+typedef struct z_mapcache_type 		z_mapcache_t;
+typedef struct mz_key_block_type 	mz_key_block_t;
+typedef struct mz_crc_block_type 	mz_crc_block_t;
+typedef struct z_super_t 		z_super_t;
+typedef struct mz_sync_io_type          mz_sync_io_t;
+typedef struct io_4k_block              io_4k_t;
+typedef struct io_dm_block              io_dm_t;
+
+struct z_gc_queue_entry_type {
+	megazone_t *megaz;
+	unsigned long gc_flags;
+	u64 r_lba;
+
+	u32 r_ptr;
+	u32 w_ptr;
+
+	u32 nblks;		/* 1-65536 */
+	int result;
+
+	u16 z_gc;
+	u16 tag;
+
+};
+typedef struct z_gc_queue_entry_type z_gc_queue_entry_t;
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct z_mapcache_entry_type {
+	__le64 logical;		/* record type [16 bits] + logical sector # */
+	__le64 physical;	/* csum 16 [16 bits] + 'physical' block lba */
+} __packed;
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct z_mapped_type {
+	struct list_head inpool;
+
+	u64 age;		/* most recent access in jiffies */
+	u64 lba;		/* Z_LOWER48 contains the BLOCK where this
+				 * data originates from .. */
+	unsigned long flags;
+	struct mutex md_lock;
+
+	u32 *mdata;		/* 4k page of table entries */
+	atomic_t refcount;
+
+	u64 last_write;		/* last known position on disk */
+	z_map_addr_t * maddr; /* nomially null */
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct z_map_addr_type {
+	u64 dm_s;		/* full map on dm layer         */
+	u64 z_id;		/* z_id match zone_list_t.z_id  */
+	u64 mz_off;		/* megazone offset              */
+	u64 mz_id;		/* mega zone #                  */
+	u64 offentry;		/* entry in lut (0-1023)        */
+	u64 lut_s;		/* sector table lba  */
+	u64 lut_r;		/* reverse table lba */
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct mz_logical_addr_map_type
+{
+	u64 mz_base;
+	u64 r_base;
+	u64 s_base;
+	u64 sk_low;
+	u64 sk_high;
+	u64 crc_low;
+	u64 crc_hi;
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct z_mapcache_type {
+	struct list_head jlist;
+	z_mapcache_entry_t *jdata;	/* 4k page of data */
+	atomic_t refcount;
+	struct mutex cached_lock;
+	unsigned long no_sort_flag;
+	u32 jcount;
+	u32 jsorted;
+	u32 jsize;
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct mz_crc_block_type {
+	u64 age;		/* most recent access in jiffies */
+	u64 lba;		/* logical home */
+	unsigned long flags;	/* IS_DIRTY flag */
+	atomic_t refcount;	/* REF count (move to flags?) */
+	u64 last_write;
+	struct mutex lock_pg;
+	u16 *crc_pg;		/* attached 4K page: [2048] entries */
+};
+
+#define MZKY_NBLKS  64
+#define MZKY_NCRC   32
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct z_super_t {
+	u8 uuid[UUID_LEN];	/* 16 */
+	__le64 nr_zones;	/*  8 */
+	__le64 magic;		/*  8 */
+	__le64 first_zone;	/*  8 */
+	__le32 version;		/*  4 */
+	__le32 packed_meta;	/*  4 */
+	__le32 flags;		/*  4 */
+	__le32 csum;		/*  4 */
+} __packed;			/* 56 */
+
+#define MAX_CACHE_SYNC 400
+
+/* stm_keys    -  512 - LBA64 for each key page of the Sector Table */
+/* stm_crc_lba -  256 - LBA64 for each CRC page */
+/* stm_crc_pg  -   64 - CRC16 for each CRC page */
+/* rtm_crc_lba -  256 - LBA64 for each CRC page */
+/* rtm_crc_pg  -   64 - CRC16 for each CRC page */
+/* crcs        -  816 - Testing worst case so far - 142 entries. */
+/* reserved    - 2048 */
+/* n_crcs;     -    2 */
+/* zp_crc;     -    2 */
+/* free_crc    -    2 */
+/* sblock;     -   56 */
+/* generation  -    8 */
+/* key_crc     -    2 */
+/* magic       -    8 */
+
+struct mz_key_block_type {
+	u64 sig[2];
+	u64 stm_keys[MZKY_NBLKS];
+	u64 stm_crc_lba[MZKY_NCRC];
+	u16 stm_crc_pg[MZKY_NCRC];
+	u64 rtm_crc_lba[MZKY_NCRC];
+	u16 rtm_crc_pg[MZKY_NCRC];
+	u16 crcs[MAX_CACHE_SYNC];
+	u16 reserved[1020];
+	u32 gc_resv;
+	u32 meta_resv;
+	u16 n_crcs;
+	u16 zp_crc;
+	u16 free_crc;
+	z_super_t sblock;
+	u64 generation;
+	u16 key_crc;
+	u64 magic;
+} __packed;
+
+struct mz_sync_io_type {
+	mz_key_block_t  bmkeys;
+	z_wptr_t        z_ptrs[1024];
+	u32             zfree[1024];
+} __packed;
+
+struct io_4k_block {
+	u8 data[Z_C4K];
+};
+
+struct io_dm_block {
+	u8 data[512];
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct megazone_type {
+	unsigned long flags;
+	zoned_t *znd;
+	struct list_head jlist;	/* journal */
+	struct list_head smtpool;	/* in-use sm table  entries */
+	z_mapped_t **sectortm;
+	z_mapped_t **reversetm;
+	mz_sync_io_t *sync_io;
+	io_4k_t * sync_cache;
+	z_wptr_t *z_ptrs;
+	u32 * zfree_count;
+	z_wptr_t  z_commit[1024];
+	mz_key_block_t *bmkeys;
+
+	mz_lam_t       logical_map;
+
+	mz_crc_block_t stm_crc[MZKY_NCRC];
+	mz_crc_block_t rtm_crc[MZKY_NCRC];
+
+	struct work_struct meta_work;
+	sector_t last_w;
+
+	u8 *cow_block;
+	u64 cow_addr;
+
+	struct mutex mz_io_mutex;	/* for normal i/o */
+
+	struct mutex zp_lock;	/* general lock (block acquire)  */
+	spinlock_t jlock;	/* journal lock */
+	spinlock_t map_pool_lock;	/* smtpool: memory pool lock */
+	struct mutex discard_lock;
+
+	u64 age;		/* most recent access in jiffies */
+
+	u32 mega_nr;
+	u32 z_count;		/* megazone data span: 4-1024 */
+	u32 z_gc_free;		/* current empty zone count */
+
+	u32 z_data;		/* Range: 2->1023 */
+
+	u32 z_current;		/* Range: 2->1023 */
+	u32 z_gc_resv;
+	u32 z_meta_resv;
+	s32 incore_count;
+	int mc_entries;
+	int meta_result;
+	u8 aggressive_gc;
+};
+typedef struct megazone_type megazone_t;
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct zoned {
+	struct dm_target *ti;
+	struct dm_target_callbacks callbacks;
+	struct dm_dev *dev;
+	u64 first_zone;		/* delta from lba 0 .. TBD use report_zones IOCTL */
+
+	unsigned long flags;    /* future: move preserve/packed/zingtype/ata_pass etc */
+	int preserve_z0;
+	int packed_meta;
+
+	// background activity:
+	struct work_struct bg_work;
+	struct workqueue_struct *bg_wq;
+
+	spinlock_t stats_lock;
+
+	// zoned gc:
+	z_gc_queue_entry_t *gc_active;
+	spinlock_t gc_lock;
+	struct work_struct gc_work;
+	struct workqueue_struct *gc_wq;
+	int gc_backlog;
+	void *gc_io_buf;
+
+	// superblock:
+	void *z_superblock;
+	z_super_t *super_block;
+
+	// array of mega-zones:
+	megazone_t *z_mega;
+	struct workqueue_struct *meta_wq;
+
+	u64 device_zone_count;	/* zones on device */
+	u64 mega_zones_count;	/* # of 256G mega-zones */
+	u64 nr_blocks;		/* 4k blocks on backing device */
+
+	z_mapcache_t gc_postmap;
+
+	struct dm_io_client *io_client;
+	struct workqueue_struct *io_wq;
+	struct timer_list timer;
+
+	u32 bins[40];
+	char bdev_name[BDEVNAME_SIZE];
+
+	size_t memstat;
+	atomic_t suspended;
+	u16 gc_mz_pref;
+	u16 mz_provision;
+	u8 zinqtype;
+	u8 ata_passthrough;
+};
+
+struct zdm_ioc_request {
+	u32 result_size;
+	u32 megazone_nr;
+};
+
+struct zdm_ioc_status {
+	u64 b_used;
+	u64 b_available;
+	u64 b_discard;
+	u64 m_used;
+	u64 mc_entries;
+	u64 mlut_blocks;
+	u64 crc_blocks;
+	u64 inpool;
+	u32 bins[40];
+};
+
+#endif /* #define  _DM_ZONED_H */
diff --git a/drivers/md/libzoned.c b/drivers/md/libzoned.c
new file mode 100644
index 0000000..8c3171f
--- /dev/null
+++ b/drivers/md/libzoned.c
@@ -0,0 +1,5347 @@
+/*
+ * Kernel Device Mapper for abstracting ZAC/ZBC devices as normal
+ * block devices for linux file systems.
+ *
+ * Copyright (C) 2015 Seagate Technology PLC
+ *
+ * Written by:
+ * Shaun Tancheff <shaun.tancheff@seagate.com>
+ *
+ * This file is licensed under  the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#define GCC_VERSION (__GNUC__ * 10000 + __GNUC_MINOR__ * 100 + __GNUC_PATCHLEVEL__)
+
+#define PARANOIA                 0
+#define EXTRA_DEBUG              0
+
+#define MZ_MEMPOOL_SZ           64
+#define JOURNAL_MEMCACHE_BLOCKS  4
+#define MEM_PURGE_MSECS        900
+
+/* Always run the GC when this many blocks can be freed */
+#define GC_COMPACT_NORMAL     1024
+/* When less than 20 zones are free use aggressive gc in the megazone */
+#define GC_COMPACT_AGGRESSIVE   32
+
+// For performance tuning:
+//   Q? smaller strips give smoother performance
+//      a single drive I/O is 8 (or 32?) blocks?
+//   A? Does not seem to ...
+#define GC_MAX_STRIPE          256
+#define REPORT_BUFFER           65 /* pages for (at least) 4096 zone info */
+#define SYNC_CACHE_ORDER         4
+#define SYNC_CACHE_PAGES        (1 << SYNC_CACHE_ORDER)
+#define SYNC_IO_ORDER            2
+#define SYNC_IO_SZ             ((1 << SYNC_IO_ORDER) * PAGE_SIZE)
+
+#define MZTEV_UNUSED    0xFFFFFFFFu
+#define MZTEV_NF        0x80000000u
+#define MZTEV_MAX       0x03ffFFFFu
+
+#define REF( v )   atomic_inc( &(v) )
+#define DEREF( v ) atomic_dec( &(v) )
+
+#define Z_TABLE_MAGIC  0x123456787654321Eul
+#define Z_KEY_SIG      0xFEDCBA987654321Ful
+
+#define Z_CRC_4K	    4096
+#define Z_BLKSZ          0x10000
+#define MAX_ZONES_PER_MZ    1024
+#define Z_SMR_SZ_BYTES   (Z_BLKSZ*Z_C4K)
+
+#define GC_READ          (1ul << 15)
+
+/**
+ * 16 bit crc
+ */
+static inline u16 crc16_md(void const *data, size_t len)
+{
+	const u16 init = 0xFFFF;
+	const u8 *p = data;
+	return crc16(init, p, len);
+}
+
+/**
+ * 16 bit CRC converted to little endian
+ */
+static inline u16 crc_md_le16(void const *data, size_t len)
+{
+	u16 crc = crc16_md(data, len);
+	return cpu_to_le16(crc);
+}
+
+/**
+ * 32 bit CRC [NOTE: 32c is HW assisted on Intel]
+ */
+static inline u32 crcpg(void * data)
+{
+	return crc32c(~0u, data, Z_CRC_4K) ^ SUPERBLOCK_CSUM_XOR;
+}
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline u64 le64_to_lba48(u64 enc, u16 * flg)
+{
+	const u64 lba64 = le64_to_cpu(enc);
+	if (flg) {
+		*flg = (lba64 >> 48) & 0xFFFF;
+	}
+	return lba64 & Z_LOWER48;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline u64 lba48_to_le64(u16 flags, u64 lba48)
+{
+	u64 high_bits = flags;
+
+	return cpu_to_le64( (high_bits << 48) | (lba48 & Z_LOWER48));
+}
+
+
+static inline int sb_test_flag(z_super_t * sb, int bit_no)
+{
+	u32 flags = le32_to_cpu(sb->flags);
+	return (flags & (1 << bit_no)) ? 1 : 0;
+}
+
+static inline void sb_set_flag(z_super_t * sb, int bit_no)
+{
+	u32 flags = le32_to_cpu(sb->flags);
+	flags |= (1 << bit_no);
+	sb->flags = cpu_to_le32(flags);
+}
+
+static inline u16 _calc_zone(u64 lba)
+{
+	return (lba >> 16) & 0x3FF;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int is_reverse_table_zone(megazone_t *megaz, z_map_addr_t *maddr)
+{
+	int is_rtm = 0;
+	int zoff = maddr->z_id % 1024;
+	int rtm_zone = ZDM_REVERSE_MAP_ZNR;	/* Z: #2 */
+
+	if (zoff == rtm_zone) {
+		is_rtm = 1;
+	}
+	return is_rtm;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void megazone_fill_lam(megazone_t *megaz, mz_lam_t * lam)
+{
+	lam->mz_base = (megaz->mega_nr * 1024);
+	lam->r_base  = (lam->mz_base + ZDM_REVERSE_MAP_ZNR) * Z_BLKSZ;
+	lam->s_base  = (lam->mz_base + ZDM_SECTOR_MAP_ZNR)  * Z_BLKSZ;
+	lam->crc_low = (lam->mz_base + ZDM_CRC_STASH_ZNR)   * Z_BLKSZ;
+	lam->crc_hi  =  lam->crc_low + (MZKY_NCRC * 2);
+	lam->sk_low  =  lam->s_base  + (ZDM_SECTOR_MAP_ZNR * MZKY_NBLKS);
+	lam->sk_high =  lam->sk_low  +  MZKY_NBLKS;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * Convert an upper layer sector number to the locally managed
+ * data sector number
+ */
+static int map_addr_onto_dm_zoned(zoned_t *, u64 sector_nr, z_map_addr_t * out);
+
+/**
+ * Locate meta data for locally managed sector.
+ */
+static int map_addr_calc(u64 dm_s, z_map_addr_t * out);
+
+/**
+ * Megazone:
+ */
+static int megazone_init(zoned_t * znd);
+static void megazone_destroy(zoned_t * znd);
+static void megazone_flush_all(zoned_t * znd);
+static void megazone_free_all(zoned_t * znd);
+
+static int write_if_dirty(megazone_t * megaz, z_mapped_t * oldest, int use_wq);
+static void gc_work_task(struct work_struct *work);
+static void meta_work_task(struct work_struct *work);
+
+static u64 mcache_greatest_gen(megazone_t *, int, u64 *, u64 *);
+static u64 mcache_find_gen(megazone_t *, u64 base, int, u64 * out);
+static int find_superblock(megazone_t * megaz, int use_wq, int do_init);
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static int do_sync_tables(megazone_t * megaz, int need_table_push);
+static int sync_crc_pages(megazone_t * megaz);
+static int unused_phy(megazone_t * megaz, u64 lba, u64 orig_s);
+static int unused_addr(megazone_t * megaz, u64 dm_s);
+static int move_to_map_tables(megazone_t * megaz, z_mapcache_t * jrnl);
+static int load_page(megazone_t *, z_mapped_t *, u64 lba, int);
+static z_mapped_t *get_map_entry(megazone_t *, z_map_addr_t *, int);
+static z_mapped_t *sector_map_entry(megazone_t * megaz, z_map_addr_t * madr);
+static z_mapped_t *reverse_map_entry(megazone_t * megaz, z_map_addr_t * sm);
+static u64 locate_sector(megazone_t * megaz, z_map_addr_t * maddr);
+static int update_map_entry(megazone_t *, z_mapped_t *, z_map_addr_t *, u64, int);
+static mz_crc_block_t *get_meta_pg_crc(megazone_t *, z_map_addr_t *, int, int);
+static int load_crc_meta_pg(megazone_t *, mz_crc_block_t *, u64, u16, int);
+static z_mapped_t *get_map_table_entry(megazone_t * megaz, u64 lba, int is_map_to);
+static int map_entry_page(megazone_t *, z_mapped_t *, u64 lba, int is_to);
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static int zoned_init(struct dm_target *ti, zoned_t *znd);
+static int read_block(struct dm_target *, enum dm_io_mem_type, void *, u64, unsigned int, int);
+static int write_block(struct dm_target *, enum dm_io_mem_type, void *, u64, unsigned int, int);
+static int fpages(megazone_t * megaz, int allowed_pages);
+static int zoned_create_disk(struct dm_target *ti, zoned_t * znd);
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+// static int zoned_init_from_disk(struct dm_target *ti, zoned_t * znd);
+static int zoned_init_disk(struct dm_target *ti, zoned_t * znd,
+			   int create, int check, int force);
+
+static sector_t jentry_value(z_mapcache_entry_t * e, bool is_block);
+static u64 z_lookup_cache(megazone_t * megaz, z_map_addr_t * sm);
+static u64 z_lookup(megazone_t * megaz, z_map_addr_t * sm);
+
+/* REFACTOR: It is a memcache not a journal */
+
+static int z_mapped_add_one(megazone_t * megaz, u64 dm_s, u64 lba);
+static int z_mapped_discard(megazone_t * megaz, u64 dm_s, u64 lba);
+static int z_mapped_addmany(megazone_t * megaz, u64 dm_s, u64 lba, u64 count);
+static int z_mapped_to_list(megazone_t * megaz, u64 dm_s, u64 lba, int purge);
+
+static int z_mapped_sync(megazone_t * megaz, int do_crc);
+static int z_mapped_init(megazone_t * megaz);
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 z_acquire(megazone_t * megaz, u32 flags, u32 nblks, u32 * nfound);
+static u32 sb_crc32(struct z_super_t *sblock);
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ *  generic-ish n-way alloc/free
+ *  Use kmalloc for small (< 4k) allocations.
+ *  Use vmalloc for multi-page alloctions
+ *  Except:
+ *  Use multipage allocations for dm_io'd pages that a frequently hit.
+ *
+ *  NOTE: ALL allocations are zero'd before returning.
+ *        alloc/free count is tracked for dynamic analysis.
+ */
+
+#define GET_PG_SYNC   0x010000
+#define GET_PG_CACHE  0x020000
+#define GET_ZPG       0x040000
+#define GET_KM        0x080000
+#define GET_VM        0x100000
+
+#define MP_SIO   (GET_PG_SYNC | 23 )
+#define MP_CACHE (GET_PG_CACHE | 24 )
+
+#define PG_01    (GET_ZPG |  1 )
+#define PG_02    (GET_ZPG |  2 )
+#define PG_05    (GET_ZPG |  5 )
+#define PG_06    (GET_ZPG |  6 )
+#define PG_08    (GET_ZPG |  8 )
+#define PG_09    (GET_ZPG |  9 )
+#define PG_10    (GET_ZPG | 10 )
+#define PG_11    (GET_ZPG | 11 )
+#define PG_13    (GET_ZPG | 13 )
+#define PG_17    (GET_ZPG | 17 )
+#define PG_27    (GET_ZPG | 27 )
+
+#define KM_00    (GET_KM  |  0 )
+#define KM_07    (GET_KM  |  7 )
+#define KM_12    (GET_KM  | 12 )
+#define KM_14    (GET_KM  | 14 )
+#define KM_15    (GET_KM  | 15 )
+#define KM_16    (GET_KM  | 16 )
+#define KM_18    (GET_KM  | 18 )
+#define KM_20    (GET_KM  | 20 )
+#define KM_25    (GET_KM  | 25 )
+#define KM_26    (GET_KM  | 26 )
+#define KM_28    (GET_KM  | 28 )
+#define KM_29    (GET_KM  | 29 )
+#define KM_30    (GET_KM  | 30 )
+
+#define VM_03    (GET_VM  |  3 )
+#define VM_04    (GET_VM  |  4 )
+#define VM_21    (GET_VM  | 21 )
+#define VM_22    (GET_VM  | 22 )
+
+#define ZDM_FREE(z, _p, sz, id)    zdm_free( (z), (_p), (sz), (id) ), (_p) = 0
+#define ZDM_ALLOC(z, sz, id)       zdm_alloc((z), (sz), (id) )
+#define ZDM_CALLOC(z, n, sz, id)   zdm_calloc((z), (n), (sz), (id) )
+
+/**
+ * kfree wrapper
+ */
+static void zdm_free(zoned_t * znd, void *p, size_t sz, u32 code)
+{
+        int id    = code & 0x00FFFF;
+        int flag  = code & 0xFF0000;
+
+	if (p) {
+		if (znd) {
+			spin_lock(&znd->stats_lock);
+			if (sz > znd->memstat) {
+				Z_ERR(znd, "Free'd more mem than allocated? %d", id );
+			}
+			if (sz > znd->bins[id]) {
+				Z_ERR(znd, "Free'd more mem than allocated? %d", id );
+				dump_stack();
+			}
+			znd->memstat -= sz;
+			znd->bins[id] -= sz;
+			spin_unlock(&znd->stats_lock);
+		}
+
+		switch (flag) {
+			case GET_PG_SYNC:
+				free_pages((unsigned long)p, SYNC_IO_ORDER);
+				break;
+			case GET_PG_CACHE:
+				free_pages((unsigned long)p, SYNC_CACHE_ORDER);
+				break;
+			case GET_ZPG:
+				free_page((unsigned long)p);
+				break;
+			case GET_KM:
+				kfree(p);
+				break;
+			case GET_VM:
+				vfree(p);
+				break;
+			default:
+				Z_ERR(znd, "zdm_free %p scheme %x not mapped.", p, code);
+				break;
+		}
+
+	} else {
+		Z_ERR(znd, "double zdm_free %p [%d]", p, id);
+		BUG_ON(p);
+	}
+}
+
+static void * zdm_alloc(zoned_t * znd, size_t sz, int code)
+{
+	void * pmem = NULL;
+        int id    = code & 0x00FFFF;
+        int flag  = code & 0xFF0000;
+
+	switch (flag) {
+		case GET_PG_SYNC:
+			pmem = (void *)__get_free_pages(GFP_KERNEL, SYNC_IO_ORDER);
+			if (pmem) {
+				memset(pmem, 0, sz);
+			}
+			break;
+		case GET_PG_CACHE:
+			pmem = (void *)__get_free_pages(GFP_KERNEL, SYNC_CACHE_ORDER);
+			if (pmem) {
+				memset(pmem, 0, sz);
+			}
+			break;
+		case GET_ZPG:
+			pmem = (void *)get_zeroed_page(GFP_KERNEL);
+			break;
+		case GET_KM:
+			pmem = kzalloc(sz, GFP_KERNEL);
+			break;
+		case GET_VM:
+			pmem = vzalloc(sz);
+			break;
+		default:
+			Z_ERR(znd, "zdm alloc scheme for %u unknown.", code);
+			break;
+	}
+	if (!pmem) {
+		Z_ERR(znd, "Out of memory. %d", id);
+		dump_stack();
+	}
+
+	if (znd && pmem) {
+		spin_lock(&znd->stats_lock);
+		znd->memstat += sz;
+		znd->bins[id] += sz;
+		spin_unlock(&znd->stats_lock);
+	}
+
+	return pmem;
+}
+
+static inline void * zdm_calloc(zoned_t * znd, size_t n, size_t sz, int line_no)
+{
+	return zdm_alloc(znd, sz * n, line_no);
+}
+
+/* -------------------------------------------------------------------------- */
+/**
+ *  Bump item to the top of the 'in-use' list.
+ *  The list is culled from the end to keep memory usage sane.
+ */
+static inline void incore_hint(megazone_t * megaz, struct list_head *head,
+			       struct list_head *item)
+{
+	spin_lock(&megaz->map_pool_lock);
+	if (head->next != item) {
+		list_move(item, head);
+	}
+	spin_unlock(&megaz->map_pool_lock);
+}
+
+/* -------------------------------------------------------------------------- */
+/**
+ * spin_lock wrapper for z_mapped_t data
+ */
+static inline void mapped_lock(z_mapped_t * mapped)
+{
+	mutex_lock(&mapped->md_lock);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * spin_unlock wrapper for z_mapped_t data
+ */
+static inline void mapped_unlock(z_mapped_t * mapped)
+{
+	mutex_unlock(&mapped->md_lock);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ *  translate a lookup table entry to a Sector #, or LBA
+ */
+static inline u64 map_value(megazone_t * megaz, u32 delta)
+{
+	u64 old_phy = 0;
+
+	if (delta != MZTEV_UNUSED) {
+		old_phy = (megaz->mega_nr * 1024 * Z_BLKSZ) + delta;
+	}
+	return old_phy;
+}
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ *  Warn if a give LBA is not valid (Esp if beyond a WP)
+ */
+static inline int warn_bad_lba(megazone_t * megaz, u64 lba48)
+{
+	int rcode = 0;
+	u64 zone  = lba48 / Z_BLKSZ;
+	u64 mz_nr = zone / 1024;
+
+	if (mz_nr == megaz->mega_nr) {
+		u32 wp_at;
+		u32 off = lba48 & 0xFFFF;
+
+		zone %= 1024;
+		if (zone < megaz->z_count) {
+			wp_at = megaz->z_ptrs[zone] & Z_WP_VALUE_MASK;
+			if (off >= wp_at) {
+				rcode = 1;
+				Z_ERR(megaz->znd, "LBA %" PRIx64 " is not valid: "
+				      "MZ# %u, off:%x wp:%x",
+					lba48, megaz->mega_nr, off, wp_at);
+				dump_stack();
+			}
+		} else {
+			rcode = 1;
+			Z_ERR(megaz->znd, "LBA is not valid - Z: %" PRIu64
+				" count %u", zone, megaz->z_count);
+		}
+	} else {
+		rcode = 1;
+		Z_ERR(megaz->znd, "Lut %" PRIx64 " is not in MZ %u (got %"
+			PRIu64 ")!!!", lba48, megaz->mega_nr, mz_nr);
+	}
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ *  Encode a Sector # or LBA to a lookup table entry value.
+ */
+static int map_encode(megazone_t * megaz, u64 to_addr, u32 * value)
+{
+	int err = 0;
+	u64 mz_lba = (megaz->mega_nr * 1024 * Z_BLKSZ);
+
+	*value = (u32) MZTEV_UNUSED;
+	if (~0ul != to_addr) {
+		u64 physical = to_addr - mz_lba;
+		*value = (u32) (physical & MZTEV_MAX);
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ *  Teardown a zoned device mapper instance.
+ */
+static void zoned_destroy(zoned_t * znd)
+{
+	if (znd->io_client) {
+		dm_io_client_destroy(znd->io_client);
+	}
+	del_timer_sync(&znd->timer);
+
+	if (znd->io_wq) {
+		destroy_workqueue(znd->io_wq);
+		znd->io_wq = NULL;
+	}
+	if (znd->meta_wq) {
+		destroy_workqueue(znd->meta_wq);
+		znd->meta_wq = NULL;
+	}
+	if (znd->gc_wq) {
+		destroy_workqueue(znd->gc_wq);
+		znd->gc_wq = NULL;
+	}
+	if (znd->bg_wq) {
+		destroy_workqueue(znd->bg_wq);
+		znd->bg_wq = NULL;
+	}
+	if (znd->dev) {
+		dm_put_device(znd->ti, znd->dev);
+		znd->dev = NULL;
+	}
+	if (znd->z_superblock) {
+		ZDM_FREE(znd, znd->z_superblock, Z_C4K, PG_05);
+	}
+	if (znd->gc_io_buf) {
+		ZDM_FREE(znd, znd->gc_io_buf, GC_MAX_STRIPE * Z_C4K, VM_04);
+	}
+	if (znd->gc_postmap.jdata) {
+		size_t sz = Z_BLKSZ * sizeof(*znd->gc_postmap.jdata);
+		ZDM_FREE(znd, znd->gc_postmap.jdata, sz, VM_03 );
+	}
+	ZDM_FREE(NULL, znd, sizeof(*znd), KM_00);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * Initialize a zoned device mapper instance
+ *
+ * Setup the zone pointer table and do a one time calculation of some
+ * basic limits
+ */
+static int zoned_init(struct dm_target *ti, zoned_t * znd)
+{
+	u64 size = i_size_read(get_bdev_bd_inode(znd));	// size in bytes.
+	u64 bdev_nr_sect4k = size / Z_C4K;
+	u64 device_zone_count = bdev_nr_sect4k / Z_BLKSZ;
+	u64 mzcount = dm_div_up(device_zone_count, MAX_ZONES_PER_MZ);
+	u64 remainder = device_zone_count % MAX_ZONES_PER_MZ;
+	u64 lut_resv;
+
+	if (0 < remainder && remainder < 5) {
+		Z_ERR(znd, "Final MZ contains too few zones!");
+		mzcount--;
+	}
+
+	znd->device_zone_count = device_zone_count;
+	znd->mega_zones_count = mzcount;
+	lut_resv = (znd->mega_zones_count * znd->mz_provision);
+	if (znd->preserve_z0) {
+		lut_resv++;
+	}
+
+	set_bit(ZF_NO_GC_DELAY, &znd->flags);
+
+	Z_DBG(znd, "%s: size:%" PRIu64 " zones: %" PRIu64 ", megas %" PRIu64
+		 ", resvd %" PRIu64 " %d", __func__, size,
+		 znd->device_zone_count, znd->mega_zones_count,
+		 lut_resv, __LINE__);
+
+	spin_lock_init(&znd->gc_lock);
+	spin_lock_init(&znd->stats_lock);
+
+	znd->gc_postmap.jdata = ZDM_CALLOC(znd, Z_BLKSZ, sizeof(*znd->gc_postmap.jdata), VM_03);
+	if (!znd->gc_postmap.jdata) {
+		ti->error = "Could not create gc_postmap array";
+		return -ENOMEM;
+	}
+	znd->gc_postmap.jsize = Z_BLKSZ;
+	mutex_init(&znd->gc_postmap.cached_lock);
+
+	znd->io_client = dm_io_client_create();
+	if (!znd->io_client) {
+		return -ENOMEM;
+	}
+
+	znd->meta_wq = create_singlethread_workqueue("znd_meta_wq");
+	if (!znd->meta_wq) {
+		ti->error = "couldn't start header metadata update thread";
+		return -ENOMEM;
+	}
+
+	znd->gc_wq = create_singlethread_workqueue("znd_gc_wq");
+	if (!znd->gc_wq) {
+		ti->error = "couldn't start GC workqueue.";
+		return -ENOMEM;
+	}
+	INIT_WORK(&znd->gc_work, gc_work_task);
+
+	znd->bg_wq = create_singlethread_workqueue("znd_bg_wq");
+	if (!znd->bg_wq) {
+		ti->error = "couldn't start background workqueue.";
+		return -ENOMEM;
+	}
+	INIT_WORK(&znd->bg_work, bg_work_task);
+
+	setup_timer(&znd->timer, activity_timeout, (unsigned long)znd);
+
+	znd->gc_io_buf = ZDM_CALLOC(znd, GC_MAX_STRIPE, Z_C4K, VM_04);
+	if (!znd->gc_io_buf) {
+		ti->error = "couldn't gc io buffer";
+		return -ENOMEM;
+	}
+
+	znd->io_wq = create_singlethread_workqueue("kzoned_dm_io_wq");
+	if (!znd->io_wq) {
+		ti->error = "couldn't start header metadata update thread";
+		return -ENOMEM;
+	}
+	znd->z_superblock = ZDM_ALLOC(znd, Z_C4K, PG_05);
+	if (!znd->z_superblock) {
+		ti->error = "couldn't allocate in-memory superblock";
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Metadata of zoned device mapper (for future backward compatability)
+ */
+static int check_metadata_version(struct z_super_t *sblock)
+{
+	u32 metadata_version = le32_to_cpu(sblock->version);
+	if (metadata_version < MIN_ZONED_VERSION
+	    || metadata_version > MAX_ZONED_VERSION) {
+		DMERR("ZONED metadata version %u found,"
+		      " but only versions between %u and %u supported.",
+		      metadata_version, MIN_ZONED_VERSION, MAX_ZONED_VERSION);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * CRC check for superblock.
+ */
+static u32 sb_crc32(struct z_super_t *sblock)
+{
+	const u32 was = sblock->csum;
+	u32 crc;
+
+	sblock->csum = 0;
+	crc = crc32c(~(u32) 0u, sblock, sizeof(*sblock)) ^ SUPERBLOCK_CSUM_XOR;
+
+	sblock->csum = was;
+	return cpu_to_le32(crc);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Check the superblock to see if it is valid and not corrupt.
+ */
+static int sb_check(struct z_super_t *sblock)
+{
+	__le32 csum_le;
+
+	if (le64_to_cpu(sblock->magic) != SUPERBLOCK_MAGIC) {
+		DMERR("sb_check failed: magic %x: wanted %x",
+		      le32_to_cpu(sblock->magic), SUPERBLOCK_MAGIC);
+		return -EILSEQ;
+	}
+
+	csum_le = sb_crc32(sblock);
+	if (csum_le != sblock->csum) {
+		DMERR("sb_check failed: csum %u: wanted %u",
+		      csum_le, sblock->csum);
+		return -EILSEQ;
+	}
+
+	return check_metadata_version(sblock);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Initialize the on-disk format of a zoned device mapper.
+ */
+static int zoned_create_disk(struct dm_target *ti, zoned_t * znd)
+{
+	struct z_super_t *sblock = znd->super_block;
+
+	memset(sblock, 0, sizeof(*sblock));
+	generate_random_uuid(sblock->uuid);
+	sblock->magic = cpu_to_le64(SUPERBLOCK_MAGIC);
+	sblock->version = cpu_to_le32(Z_VERSION);
+	if (znd->preserve_z0) {
+		sb_set_flag(sblock, SB_Z0_RESERVED);
+	}
+	sblock->first_zone = cpu_to_le64(znd->first_zone);
+	sblock->packed_meta = cpu_to_le32(znd->packed_meta);
+
+	if (znd->packed_meta) {
+		int iter;
+		int locations = znd->mega_zones_count * 3;
+		u64 incr = 512ul;
+		u64 lba = 0;
+		void * data = ZDM_ALLOC(znd, Z_C4K, PG_06);
+		struct dm_target * ti = znd->ti;
+
+		Z_ERR(znd, "Clear sblocks (because: packed)");
+		if (znd->preserve_z0) {
+			lba = Z_BLKSZ;
+		}
+		if (znd->mega_zones_count >= 32) {
+			Z_ERR(znd, "TODO: Use lba of mz #32.");
+		}
+
+		for (iter = 0; iter < locations; iter++) {
+			int rc = write_block(ti, DM_IO_KMEM, data, lba, 1, 1);
+			if (rc) {
+				Z_ERR(znd, "%s: clear sb @ %" PRIx64
+				       " failed:  %d", __func__, lba, rc);
+			}
+			lba += incr;
+		}
+		ZDM_FREE(znd, data, Z_C4K, PG_06);
+	}
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Repair an otherwise good device mapper instance that was not cleanly removed.
+ */
+static int zoned_repair(zoned_t * znd)
+{
+	Z_INFO(znd, "Is Dirty .. zoned_repair consistency fixer TODO!!!.");
+	return -ENOMEM;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * HA drives use a single zone for metadata per-instance:
+ *   Note this scheme should be re-visisted for drives > 10.5 TiB
+ */
+static void packed_meta_defaults(zoned_t * znd)
+{
+	if (znd->packed_meta) {
+		u32 iter;
+		Z_ERR(znd, "Updating zone data ranges (because: packed)");
+		for (iter = 0; iter < znd->mega_zones_count; iter++) {
+			megazone_t *megaz = &znd->z_mega[iter];
+
+			megaz->z_data = 0;
+			if (megaz->mega_nr == 0) {
+				megaz->z_data++;
+				if (znd->preserve_z0) {
+					megaz->z_data++;
+				}
+			}
+			// where metadata starts ...
+			megaz->z_current = megaz->z_data;
+		}
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Locate the existing SB on disk and re-load or create the device-mapper
+ * instance based on the existing disk state.
+ */
+static int zoned_init_disk(struct dm_target *ti, zoned_t * znd,
+			   int create, int check, int force)
+{
+	mz_key_block_t *key_blk = znd->z_superblock;
+	megazone_t *megaz = &znd->z_mega[0];
+	int jinit = 1;
+	int n4kblks = 1;
+	int use_worker = 1;
+	int rc = 0;
+	u64 first_zone = znd->first_zone;
+	int packed_meta = znd->packed_meta;
+	int preserve_z0 = znd->preserve_z0;
+
+	memset(key_blk, 0, sizeof(*key_blk));
+
+	if (create && force) {
+		Z_ERR(znd, "Force Creating a clean instance.");
+	} else if (find_superblock(megaz, use_worker, 1)) {
+		u64 sb_lba = 0;
+		u64 generation;
+
+		Z_INFO(znd, "Found existing superblock");
+		if (force) {
+			if (first_zone != znd->first_zone) {
+				Z_ERR(znd, "  (force) override first zone: %"
+					PRIu64  " with %" PRIu64 "",
+				      znd->first_zone, first_zone);
+				jinit = 0;
+				znd->first_zone = first_zone;
+			}
+			if (packed_meta != znd->packed_meta) {
+				Z_ERR(znd, "  (force) override meta=%s with %s",
+				      znd->packed_meta ? "hm" : "ha",
+				      packed_meta ? "hm" : "ha");
+				jinit = 0;
+				znd->packed_meta = packed_meta;
+			}
+			if (preserve_z0 != znd->preserve_z0) {
+				Z_ERR(znd, "  (force) override no_z0 : %d with %d",
+				      znd->preserve_z0, preserve_z0);
+				jinit = 0;
+				znd->preserve_z0 = preserve_z0;
+			}
+		}
+		generation = mcache_greatest_gen(megaz, use_worker, &sb_lba, NULL);
+		Z_DBG(znd, "Generation: %" PRIu64 " @ %" PRIx64, generation, sb_lba);
+
+		rc = read_block(ti, DM_IO_KMEM, key_blk, sb_lba, n4kblks, use_worker);
+		if (rc) {
+			ti->error = "Superblock read error.";
+			return rc;
+		}
+	}
+	znd->super_block = &key_blk->sblock;
+	rc = sb_check(znd->super_block);
+	if (rc) {
+		jinit = 0;
+		if (create) {
+			DMWARN("Check failed .. creating superblock.");
+			zoned_create_disk(ti, znd);
+			znd->super_block->nr_zones =
+			    le64_to_cpu(znd->device_zone_count);
+			DMWARN("in-memory superblock created.");
+		} else {
+			ti->error = "Superblock check failed.";
+			return rc;
+		}
+	}
+
+	if (sb_test_flag(znd->super_block, SB_DIRTY)) {
+		int repair_check = zoned_repair(znd);
+		if (!force) {
+			if (repair_check) {
+				/* repair failed -- don't load from disk */
+				jinit = 0;
+			}
+		} else if (repair_check && jinit) {
+			Z_ERR(znd, "repair failed .. "
+			      "force enabled ... " "loading anyway.");
+		}
+	}
+
+	if (jinit) {
+		u32 iter;
+		Z_ERR(znd, "INIT: Reloading DM Zoned metadata from DISK");
+
+		znd->first_zone = le64_to_cpu(znd->super_block->first_zone);
+		znd->preserve_z0 = sb_test_flag(znd->super_block, SB_Z0_RESERVED);
+		znd->packed_meta = le32_to_cpu(znd->super_block->packed_meta);
+
+		packed_meta_defaults(znd);
+
+		for (iter = 0; iter < znd->mega_zones_count; iter++) {
+			megazone_t *megaz = &znd->z_mega[iter];
+			set_bit(DO_JOURNAL_LOAD, &megaz->flags);
+			queue_work(znd->meta_wq, &megaz->meta_work);
+		}
+		Z_ERR(znd, "Waiting for load to complete.");
+		flush_workqueue(znd->meta_wq);
+	} else {
+		packed_meta_defaults(znd);
+	}
+
+/* On newer GCC this warning -> error breaks the build. */
+
+#pragma GCC diagnostic push
+#if GCC_VERSION > 40804
+#pragma GCC diagnostic ignored "-Wdate-time"
+#endif
+
+	Z_ERR(znd, "ZONED: [" __DATE__ " " __TIME__ "] marking superblock dirty.");
+#pragma GCC diagnostic pop
+	// write the 'dirty' flag back to disk.
+	sb_set_flag(znd->super_block, SB_DIRTY);
+
+	znd->super_block->csum = sb_crc32(znd->super_block);
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline sector_t jentry_value(z_mapcache_entry_t * e, bool is_block)
+{
+	sector_t value = 0;
+
+	if (is_block) {
+		value = le64_to_lba48(e->physical, NULL);
+	} else {
+		value = le64_to_lba48(e->logical, NULL);
+	}
+
+	return value;
+}
+
+// NTS: intel/arm are LE. sparc/mips are BE
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int compare_logical_sectors(const void *x1, const void *x2)
+{
+	const z_mapcache_entry_t *r1 = x1;
+	const z_mapcache_entry_t *r2 = x2;
+	const u64 v1 = le64_to_lba48(r1->logical, NULL);
+	const u64 v2 = le64_to_lba48(r2->logical, NULL);
+
+	return (v1 < v2) ? -1 : ((v1 > v2) ? 1 : 0);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int __find_sector_entry_chunk(z_mapcache_entry_t * data,
+				     s32 count, sector_t find, bool is_block)
+{
+	int at = -1;
+	s32 first = 0;
+	s32 last = count - 1;
+	s32 middle = (first + last) / 2;
+
+	while ((-1 == at) && (first <= last)) {
+		sector_t logical = ~0ul;
+		if (0 <= middle && middle < count) {
+			logical = jentry_value(&data[middle], is_block);
+		}
+
+		if (logical < find)
+			first = middle + 1;
+		else if (logical > find)
+			last = middle - 1;
+		else
+			at = middle;
+		middle = (first + last) / 2;
+	}
+	return at;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 z_lookup_key_range(megazone_t * megaz, z_map_addr_t * maddr)
+{
+	u64 found = 0ul;
+	mz_lam_t * lam = &megaz->logical_map;
+
+	if ((lam->sk_low <= maddr->dm_s) && (maddr->dm_s < lam->sk_high)) {
+		unsigned long flags;
+		int off = maddr->dm_s - lam->sk_low;
+
+		spin_lock_irqsave(&megaz->jlock, flags);
+		found = le64_to_cpu(megaz->bmkeys->stm_keys[off]);
+		spin_unlock_irqrestore(&megaz->jlock, flags);
+	}
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_add_one(megazone_t * megaz, u64 dm_s, u64 lba)
+{
+	int err;
+	int purge = 0;
+	mz_lam_t * lam = &megaz->logical_map;
+	u16 zone = _calc_zone(lba);
+
+	/*
+	 * location of the SLT key sectors need to be
+	 * stashed into the sector lookup table block map
+	 * Does dm_s point in the sector lookup table block map
+	 */
+	if ((lam->sk_low <= dm_s) && (dm_s < lam->sk_high)) {
+		int off = dm_s - lam->sk_low;
+		unsigned long flags;
+		spin_lock_irqsave(&megaz->jlock, flags);
+		megaz->bmkeys->stm_keys[off] = cpu_to_le64(lba);
+		spin_unlock_irqrestore(&megaz->jlock, flags);
+	}
+	do {
+		err = z_mapped_to_list(megaz, dm_s, lba, purge);
+	} while (-EBUSY == err );
+
+	megaz->z_commit[zone]++;
+	if (megaz->z_commit[zone] == Z_BLKSZ) {
+		mutex_lock(&megaz->zp_lock);
+		megaz->z_ptrs[zone] |= Z_WP_GC_READY;
+		mutex_unlock(&megaz->zp_lock);
+	}
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_discard(megazone_t * megaz, u64 dm_s, u64 lba)
+{
+	int purge = 1;
+	int err;
+
+	int do_inflight = 0;
+	zoned_t *znd = megaz->znd;
+	unsigned long flags;
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	if (znd->gc_active) {
+		if (_calc_zone(lba) == znd->gc_active->z_gc) {
+			do_inflight = 1;
+		}
+	}
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+	if (do_inflight) {
+		err = -EBUSY;
+		goto out;
+	}
+
+	/*
+	 * Action: purge from lookup table
+	 *         mark as unused in the sector lookup table
+	 *         mark as unused in the reverse lookup table
+	 *         purge from the *ACTIVE GC TABLE* when active!
+	 *
+	 * NOTE:   called with io mutex held.
+	 */
+ 	do {
+		err = z_mapped_to_list(megaz, dm_s, lba, purge);
+	} while (-EBUSY == err );
+
+	if (!err) {
+		err = unused_addr(megaz, dm_s);
+	}
+	if (!err) {
+		err = unused_phy(megaz, lba, dm_s);
+	}
+out:
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static z_mapcache_t *jalloc(zoned_t * znd)
+{
+	z_mapcache_t *jrnl_first;
+
+	jrnl_first = ZDM_ALLOC(znd, sizeof(*jrnl_first), KM_07);
+	if (jrnl_first) {
+		mutex_init(&jrnl_first->cached_lock);
+		jrnl_first->jcount = 0;
+		jrnl_first->jsorted = 0;
+		jrnl_first->jdata =
+			ZDM_CALLOC(znd, Z_UNSORTED, sizeof(*jrnl_first->jdata), PG_08);
+
+		if (jrnl_first->jdata) {
+			u64 logical = Z_LOWER48;
+			u64 physical = Z_LOWER48;
+
+			jrnl_first->jdata[0].logical = cpu_to_le64(logical);
+			jrnl_first->jdata[0].physical = cpu_to_le64(physical);
+			jrnl_first->jsize = Z_UNSORTED - 1;
+
+		} else {
+			Z_ERR(znd, "%s: in memory journal is out of space.",
+			      __func__);
+			ZDM_FREE(znd, jrnl_first, sizeof(*jrnl_first), KM_07);
+			jrnl_first = NULL;
+		}
+	}
+	return jrnl_first;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline z_mapcache_t *jfirst_entry(megazone_t * megaz)
+{
+	unsigned long flags;
+	struct list_head *_jhead;
+	z_mapcache_t *jrnl;
+
+	spin_lock_irqsave(&megaz->jlock, flags);
+	_jhead = &(megaz->jlist);
+	jrnl = list_first_entry_or_null(_jhead, typeof(*jrnl), jlist);
+	if (jrnl && (&jrnl->jlist != _jhead)) {
+		REF(jrnl->refcount);
+	} else {
+		jrnl = NULL;
+	}
+	spin_unlock_irqrestore(&megaz->jlock, flags);
+
+	return jrnl;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void jlist_add(megazone_t * megaz, z_mapcache_t * jrnl)
+{
+	unsigned long flags;
+	struct list_head *_jhead;
+
+	spin_lock_irqsave(&megaz->jlock, flags);
+	_jhead = &(megaz->jlist);
+	list_add(&(jrnl->jlist), _jhead);
+	spin_unlock_irqrestore(&megaz->jlock, flags);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void jderef(megazone_t * megaz, z_mapcache_t * jrnl)
+{
+	unsigned long flags;
+	spin_lock_irqsave(&megaz->jlock, flags);
+	DEREF(jrnl->refcount);
+	spin_unlock_irqrestore(&megaz->jlock, flags);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline z_mapcache_t *jnext_entry(megazone_t * megaz, z_mapcache_t * jrnl)
+{
+	unsigned long flags;
+	struct list_head *_jhead;
+	z_mapcache_t * next;
+
+	spin_lock_irqsave(&megaz->jlock, flags);
+	_jhead = &(megaz->jlist);
+	next = list_next_entry(jrnl, jlist);
+	if (next && (&next->jlist != _jhead)) {
+		REF(next->refcount);
+	} else {
+		next = NULL;
+	}
+	DEREF(jrnl->refcount);
+	spin_unlock_irqrestore(&megaz->jlock, flags);
+
+	return next;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void jcache_sort_no_lock(z_mapcache_t * jrnl)
+{
+	if (jrnl->jsorted < jrnl->jcount) {
+		sort(&jrnl->jdata[1], jrnl->jcount,
+		     sizeof(*jrnl->jdata),
+		     compare_logical_sectors, NULL);
+		jrnl->jsorted = jrnl->jcount;
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void jcache_sort_locked(z_mapcache_t * jrnl)
+{
+	if (jrnl->jsorted < jrnl->jcount) {
+		mutex_lock_nested(&jrnl->cached_lock, SINGLE_DEPTH_NESTING);
+		jcache_sort_no_lock(jrnl);
+		mutex_unlock(&jrnl->cached_lock);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int jlinear_find(z_mapcache_t * jrnl, u64 dm_s)
+{
+	int at = -1;
+	int jentry;
+
+	for (jentry = jrnl->jcount; jentry > 0; jentry--) {
+		u64 logi = le64_to_lba48(jrnl->jdata[jentry].logical, NULL);
+		if ( logi == dm_s) {
+			at = jentry - 1;
+			goto out;
+		}
+	}
+
+out:
+	return at;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 z_lookup_cache(megazone_t * megaz, z_map_addr_t * maddr)
+{
+	z_mapcache_t *jrnl;
+	u64 found = 0ul;
+
+	jrnl = jfirst_entry(megaz);
+	while (jrnl) {
+		int at;
+
+		if (jrnl->no_sort_flag) {
+			at = jlinear_find(jrnl, maddr->dm_s);
+		} else {
+			jcache_sort_locked(jrnl); /* Possible dead-lock if unsorted  */
+			at = __find_sector_entry_chunk(&jrnl->jdata[1], jrnl->jcount,
+						       maddr->dm_s, 0);
+		}
+		if (at != -1) {
+			z_mapcache_entry_t *data = &jrnl->jdata[at + 1];
+			found = le64_to_lba48(data->physical, NULL);
+		}
+		if (found) {
+			jderef(megaz, jrnl);
+			goto out;
+		}
+
+		jrnl = jnext_entry(megaz, jrnl);
+	}
+out:
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int lba_in_zone(z_mapcache_t * jrnl, u16 zone)
+{
+	int jentry;
+
+	if (zone >= MAX_ZONES_PER_MZ) {
+		goto out;
+	}
+
+	for (jentry = jrnl->jcount; jentry > 0; jentry--) {
+		u64 lba = le64_to_lba48(jrnl->jdata[jentry].physical, NULL);
+		if (_calc_zone(lba) == zone ) {
+			return 1;
+		}
+	}
+out:
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int gc_verify_cache(megazone_t * megaz, u16 zone)
+{
+	z_mapcache_t *jrnl = NULL;
+	int err = 0;
+
+	jrnl = jfirst_entry(megaz);
+	while (jrnl) {
+		mutex_lock(&jrnl->cached_lock);
+		if (lba_in_zone(jrnl, zone)) {
+			Z_ERR(megaz->znd, "**ERR**: GC error ... %x "
+				"LBA in cache .. Corrupt", zone );
+			err = 1;
+		}
+		mutex_unlock(&jrnl->cached_lock);
+		jrnl = jnext_entry(megaz, jrnl);
+	}
+	return err;
+}
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int _journal_blocks_to_table(megazone_t * megaz, u16 zone)
+{
+	z_mapcache_t *jrnl = NULL;
+	int err = 0;
+
+	jrnl = jfirst_entry(megaz);
+	while (jrnl) {
+		int try_free = 0;
+		z_mapcache_t *jskip;
+
+		mutex_lock(&jrnl->cached_lock);
+		if (jrnl->jcount == jrnl->jsize) {
+			jcache_sort_no_lock(jrnl);
+			err = move_to_map_tables(megaz, jrnl);
+			if (!err && 0 == jrnl->jcount) {
+				try_free = 1;
+			}
+		} else {
+			if (lba_in_zone(jrnl, zone)) {
+				Z_ERR(megaz->znd,
+					"Moving %d Runts because z: %u",
+					jrnl->jcount, zone);
+
+				jcache_sort_no_lock(jrnl);
+				err = move_to_map_tables(megaz, jrnl);
+			}
+		}
+		mutex_unlock(&jrnl->cached_lock);
+
+		if (err) {
+			jderef(megaz, jrnl);
+			Z_ERR(megaz->znd, "%s: Sector map failed.", __func__);
+			goto out;
+		}
+
+		jskip = jnext_entry(megaz, jrnl);
+		if (try_free) {
+			if (0 == jrnl->refcount.counter) {
+				unsigned long flags;
+				size_t sz = Z_UNSORTED * sizeof(*jrnl->jdata);
+
+				spin_lock_irqsave(&megaz->jlock, flags);
+				list_del(&jrnl->jlist);
+				spin_unlock_irqrestore(&megaz->jlock, flags);
+
+				ZDM_FREE(megaz->znd, jrnl->jdata, sz, PG_08);
+				ZDM_FREE(megaz->znd, jrnl, sizeof(*jrnl), KM_07);
+				jrnl = NULL;
+
+				megaz->mc_entries--;
+			} else {
+				Z_ERR(megaz->znd, "Journal still in use. Not freed");
+			}
+		}
+		jrnl = jskip;
+	}
+out:
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_SYNC(megazone_t * megaz, int do_tables, int do_crc)
+{
+	// For every megaz?
+	int err = 0;
+
+	err = do_sync_tables(megaz, do_tables);
+	if (err) {
+		Z_ERR(megaz->znd, "Uh oh: do_sync_tables -> %d", err);
+		goto out;
+	}
+
+	err = sync_crc_pages(megaz);
+	if (err) {
+		Z_ERR(megaz->znd, "Uh oh. sync_crc_pages -> %d", err);
+		goto out;
+	}
+
+	err = z_mapped_sync(megaz, do_crc);
+	if (err) {
+		Z_ERR(megaz->znd, "Uh oh. z_mapped_sync -> %d", err);
+		goto out;
+	}
+
+out:
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_init_from_journal(megazone_t * megaz)
+{
+	int err = 0;
+	if (test_and_clear_bit(DO_JOURNAL_LOAD, &megaz->flags)) {
+		mutex_lock(&megaz->mz_io_mutex);
+		err = z_mapped_init(megaz);
+		mutex_unlock(&megaz->mz_io_mutex);
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int meta_integrity_test(megazone_t * megaz)
+{
+	int rc = 0;
+	u32 z_used = Z_BLKSZ;
+	u64 entry;
+	u64 s_base = 0x20000 + (megaz->mega_nr * Z_BLKSZ * 1024);
+
+	u32 *data = ZDM_ALLOC(megaz->znd, Z_C4K, PG_09);
+	if (!data)
+		return -ENOMEM;
+
+#if EXTRA_DEBUG
+	Z_ERR(megaz->znd, "MZ# %u integrity check in progress ..", megaz->mega_nr);
+#endif // EXTRA_DEBUG
+
+	/* NOTE: R_lut followed by S_lut */
+	for (entry = 0; entry < 0x20000; entry++) {
+		int crc_okay = 0;
+		int z_id = entry / 64;
+		int use_wq = 0;
+		int is_to;
+		int crce;
+		u16 crc_pgv;
+		z_map_addr_t maddr;
+		mz_crc_block_t *pblock;
+
+		map_addr_calc(s_base + entry, &maddr);
+		crce = (maddr.mz_off & 0xFFFF) % 2048;
+		is_to = !is_reverse_table_zone(megaz, &maddr);
+		pblock = get_meta_pg_crc(megaz, &maddr, is_to, use_wq);
+		if (!pblock) {
+			Z_ERR(megaz->znd, "%s: Out of space for metadata?", __func__);
+			return -ENOSPC;
+		}
+
+		if (!is_to) {
+			if (0 == (entry % 64)) {
+				z_used = Z_BLKSZ;
+			}
+		}
+
+		REF(pblock->refcount);
+
+		mutex_lock(&pblock->lock_pg);
+		crc_pgv = pblock->crc_pg[crce];
+		mutex_unlock(&pblock->lock_pg);
+
+		if (crc_pgv) {
+			u64 lba = z_lookup(megaz, &maddr);
+			if (lba) {
+				int count = 1;
+				int rd;
+
+#if EXTRA_DEBUG
+				Z_DBG(megaz->znd, "Testing MD Block: %" PRIx64
+					 " on disk %" PRIx64,
+					 s_base + entry, lba);
+#endif // EXTRA_DEBUG
+
+				rd = read_block(megaz->znd->ti,
+						DM_IO_KMEM, data, lba,
+						count, use_wq);
+				if (rd) {
+					rc = rd;
+					Z_ERR(megaz->znd, "Integrity ERR: "
+					      "%" PRIx64 " on disk %" PRIx64
+					      " read err %d", s_base + entry,
+					      lba, rd);
+				} else {
+					u16 crc = crc_md_le16(data, Z_CRC_4K);
+					if (crc == crc_pgv) {
+						crc_okay = 1;
+					} else {
+						Z_ERR(megaz->znd, "Integrity ERR: "
+						      "%04x != %04x "
+						      "at lba %" PRIx64 " "
+						      "lmap %" PRIx64,
+						      crc,
+						      crc_pgv,
+						      lba, maddr.dm_s);
+						rc = -EIO;
+					}
+
+				}
+			} else {
+				Z_ERR(megaz->znd, "MZ# %u LBA Not found for: "
+				      "0x%" PRIx64 "+entry:%" PRIx64 " [%"
+				      PRIx64 "] is_to %d",
+				      megaz->mega_nr,
+				      s_base, entry, s_base + entry,
+				      is_to );
+			}
+
+			if (crc_okay && (!is_to)) {
+				int off;
+
+				if (0 == (entry % 64)) {
+					z_used = Z_BLKSZ;
+				}
+
+				for (off = 0; off < 1024; off++) {
+					u64 ORlba =
+					    (megaz->mega_nr * Z_BLKSZ * 1024)
+					    + (z_id * Z_BLKSZ)
+					    + ((entry % 64) * 1024)
+					    + off;
+
+					u32 enc = data[off];
+					if (enc == MZTEV_UNUSED) {
+						z_used--;
+					} else {
+						u64 dm_s =
+						    map_value(megaz, enc);
+						if (dm_s <
+						    megaz->znd->nr_blocks) {
+							z_mapped_t *Smap;
+							z_map_addr_t Saddr;
+							map_addr_calc(dm_s,
+								      &Saddr);
+							Smap =
+							    sector_map_entry
+							    (megaz, &Saddr);
+							if (Smap && Smap->mdata) {
+								Z_DBG(megaz->znd, "lba: %"
+								     PRIx64
+								     " okay",
+								     ORlba);
+							} else {
+								Z_ERR(megaz->znd, "lba: %"
+								      PRIx64
+								      " ERROR",
+								      ORlba);
+							}
+						} else {
+							Z_ERR(megaz->znd,
+							      "Invalid rmap entry: %x.",
+							      enc);
+						}
+						BUG_ON(dm_s >=
+						       megaz->znd->nr_blocks);
+					}
+				}
+			}
+		}
+		DEREF(pblock->refcount);
+
+		if (!is_to) {
+			if (63 == (entry % 64)) {
+				// update zone unused count
+				if (0 ==
+				    (megaz->z_ptrs[z_id] & Z_WP_FLAGS_MASK)) {
+					megaz->zfree_count[z_id] =
+					    Z_BLKSZ - z_used;
+				}
+			}
+		}
+	}
+
+	ZDM_FREE(megaz->znd, data, Z_C4K, PG_09);
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_meta_check(megazone_t * megaz)
+{
+	int err = 0;
+	if (test_and_clear_bit(DO_META_CHECK, &megaz->flags)) {
+		mutex_lock(&megaz->mz_io_mutex);
+		err = meta_integrity_test(megaz);
+		mutex_unlock(&megaz->mz_io_mutex);
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_journal_to_table(megazone_t * megaz)
+{
+	int err = 0;
+
+	if (test_and_clear_bit(DO_JOURNAL_MOVE, &megaz->flags)) {
+		mutex_lock(&megaz->mz_io_mutex);
+		err = _journal_blocks_to_table(megaz, MAX_ZONES_PER_MZ);
+		mutex_unlock(&megaz->mz_io_mutex);
+	}
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_free_pages(megazone_t * megaz)
+{
+	int err = 0;
+	if (test_and_clear_bit(DO_MEMPOOL, &megaz->flags)) {
+		int pool_size = MZ_MEMPOOL_SZ * 4;
+		u64 tnow = jiffies_64;
+		u64 mem_time;
+
+		mem_time = msecs_to_jiffies(1500);
+		mem_time = (mem_time < tnow) ? tnow - mem_time : 0;
+		if (time_before64(megaz->age, mem_time)) {
+			pool_size = MZ_MEMPOOL_SZ;
+		}
+
+		mem_time = msecs_to_jiffies(5000);
+		mem_time = (mem_time < tnow) ? tnow - mem_time : 0;
+		if (time_before64(megaz->age, mem_time)) {
+			pool_size = 3;
+		}
+
+		mutex_lock(&megaz->mz_io_mutex);
+		err = fpages(megaz, pool_size);
+		mutex_unlock(&megaz->mz_io_mutex);
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_sync_to_disk(megazone_t * megaz)
+{
+	int err = 0;
+	if (test_and_clear_bit(DO_SYNC, &megaz->flags)) {
+		int do_crc = 0;
+		mutex_lock(&megaz->mz_io_mutex);
+
+		if (test_and_clear_bit(DO_SYNC_CRC, &megaz->flags)) {
+			do_crc = 1;
+		}
+		err = do_SYNC(megaz, 1, do_crc);
+#if PARANOIA
+		if (!err) {
+			err = meta_integrity_test(megaz);
+		}
+#endif
+		mutex_unlock(&megaz->mz_io_mutex);
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void meta_work_task(struct work_struct *work)
+{
+	int err = 0;
+	megazone_t *megaz;
+
+	if (!work) {
+		return;
+	}
+
+	megaz = container_of(work, megazone_t, meta_work);
+
+	if (!megaz) {
+		return;
+	}
+
+	err = do_init_from_journal(megaz);
+
+	/* reduce memory pressure on journal list of arrays
+	   by pushing them into the sector map lookup tables */
+	if (!err) {
+		err = do_journal_to_table(megaz);
+	}
+
+	/* reduce memory pressure on sector map lookup tables
+	   by pushing them onto disc */
+	if (!err) {
+		err = do_free_pages(megaz);
+	}
+
+	/* force a consistent set of meta data out to disk */
+	if (!err) {
+		err = do_sync_to_disk(megaz);
+	}
+
+	if (!err) {
+		err = do_meta_check(megaz);
+	}
+
+	megaz->age = jiffies_64;
+
+	megaz->meta_result = err;
+	clear_bit(DO_METAWORK_QD, &megaz->flags);
+
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_to_list(megazone_t * megaz, u64 dm_s, u64 lba, int purge)
+{
+	z_mapcache_t *jrnl = NULL;
+	z_mapcache_t *jrnl_first = NULL;
+	int handled = 0;
+	int list_count = 0;
+	int err = 0;
+
+	jrnl = jfirst_entry(megaz);
+	while (jrnl) {
+		int at;
+
+		mutex_lock(&jrnl->cached_lock);
+		jcache_sort_no_lock(jrnl);
+		at = __find_sector_entry_chunk(&jrnl->jdata[1], jrnl->jcount,
+					       dm_s, 0);
+		if (at != -1) {
+			z_mapcache_entry_t *data = &jrnl->jdata[at + 1];
+			u64 lba_was = le64_to_lba48(data->physical, NULL);
+			u64 physical = lba & Z_LOWER48;
+
+			if (!purge && (lba != lba_was)) {
+				Z_DBG(megaz->znd, "Remap %" PRIx64 " -> %"PRIx64
+					 " (was %" PRIx64 "->%" PRIx64 ")",
+					 dm_s, lba,
+					 le64_to_lba48(data->logical, NULL),
+					 le64_to_lba48(data->physical, NULL));
+				err = unused_phy(megaz, lba_was, 0);
+				if (1 == err) {
+					err = 0;
+				}
+			}
+			if (purge) {
+				data->logical = cpu_to_le64(Z_LOWER48);
+				data->physical = cpu_to_le64(Z_LOWER48);
+				jrnl->jsorted--;
+				jcache_sort_no_lock(jrnl);
+			} else {
+				data->physical = cpu_to_le64(physical);
+			}
+			handled = 1;
+		} else if (!jrnl_first) {
+			if (jrnl->jcount < jrnl->jsize) {
+				jrnl_first = jrnl;
+			}
+		}
+
+		mutex_unlock(&jrnl->cached_lock);
+
+		if (handled) {
+			jderef(megaz, jrnl);
+			goto out;
+		}
+
+		jrnl = jnext_entry(megaz, jrnl);
+		list_count++;
+	}
+
+	if (purge) {
+		/* nothing was found, nothing to add */
+		goto out;
+	}
+
+	/* ------------------------------------------------------------------ */
+	/* ------------------------------------------------------------------ */
+	if (jrnl_first) {
+		REF(jrnl_first->refcount);	// hold a reference count
+	} else {
+		jrnl_first = jalloc(megaz->znd);
+		if (jrnl_first) {
+			REF(jrnl_first->refcount);
+			jlist_add(megaz, jrnl_first);
+		} else {
+			Z_ERR(megaz->znd, "%s: in memory journal is "
+			      "out of space.", __func__);
+			err = -ENOMEM;
+			goto out;
+		}
+
+		if (list_count > JOURNAL_MEMCACHE_BLOCKS) {
+			set_bit(DO_JOURNAL_MOVE, &megaz->flags);
+		}
+		megaz->mc_entries = list_count + 1;
+	}
+
+	/* ------------------------------------------------------------------ */
+	/* ------------------------------------------------------------------ */
+
+	if (jrnl_first) {
+		mutex_lock(&jrnl_first->cached_lock);
+
+		if (jrnl_first->jcount < jrnl_first->jsize) {
+			u16 idx = ++jrnl_first->jcount;
+			jrnl_first->jdata[idx].logical = lba48_to_le64(0, dm_s);
+			jrnl_first->jdata[idx].physical = lba48_to_le64(0, lba);
+		} else {
+			Z_ERR(megaz->znd, "%s: cached bin out of space!",
+			      __func__);
+			err = -EBUSY;
+		}
+		mutex_unlock(&jrnl_first->cached_lock);
+		jderef(megaz, jrnl_first);
+	}
+out:
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void sb_merge(megazone_t * megaz)
+{
+	z_super_t *sblk = &megaz->bmkeys->sblock;
+	memcpy(sblk, megaz->znd->super_block, sizeof(*sblk));
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline u64 next_generation(megazone_t * megaz)
+{
+	u64 generation = le64_to_cpu(megaz->bmkeys->generation);
+
+	if (0 == generation) {
+		generation = 2;
+	}
+	generation++;
+	if (0 == generation) {
+		generation++;
+	}
+
+	return generation;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_sync(megazone_t * megaz, int do_crc)
+{
+	struct dm_target *ti = megaz->znd->ti;
+	z_mapcache_t *jrnl;
+	int nblks = 1;
+	int use_wq = 0;
+	int rc = 1;
+	int jwrote = 0;
+	int cached = 0;
+	int cindex = 0;
+	int do_reset_wp = 0;
+	int z_id = 0;
+	int need_sync_io = 1;
+	int sync_io_blocks = sizeof(*megaz->sync_io) / Z_C4K;
+	u64 lba = 0;
+	u64 generation = next_generation(megaz);
+
+	io_4k_t * sync_cache = megaz->sync_cache;
+
+	if (megaz->znd->packed_meta) {
+		u64 modulo = 3;
+		u64 incr = 512;
+		if (megaz->znd->preserve_z0) {
+			z_id++;
+			lba = Z_BLKSZ;
+		}
+		if (megaz->mega_nr >= 32) {
+			Z_ERR(megaz->znd, "TODO: Use lba of mz #32.");
+		} else {
+			lba += (modulo * incr) * megaz->mega_nr;
+		}
+		lba += (generation % modulo) * incr;
+	} else {
+		lba = Z_BLKSZ * (megaz->mega_nr * 1024);
+		if (generation & 1) {
+			lba += Z_BLKSZ;
+			z_id++;	// Zn   -> Zn+1
+		}
+		if (megaz->znd->preserve_z0 && megaz->mega_nr == 0) {
+			z_id++;
+			lba = Z_BLKSZ;
+		}
+	}
+
+	if (0 == lba) {
+		lba++;
+	}
+
+	megaz->bmkeys->generation = cpu_to_le64(generation);
+	megaz->bmkeys->gc_resv = megaz->z_gc_resv;
+	megaz->bmkeys->meta_resv = megaz->z_meta_resv;
+
+	sb_merge(megaz);
+
+	jrnl = jfirst_entry(megaz);
+	while (jrnl) {
+		u64 phy = le64_to_lba48(jrnl->jdata[0].physical, NULL);
+		u16 jcount = jrnl->jcount & 0xFFFF;
+		jrnl->jdata[0].physical = lba48_to_le64(jcount, phy);
+
+		megaz->bmkeys->crcs[cindex] = crc_md_le16(jrnl->jdata, Z_CRC_4K);
+		cindex++;
+
+		memcpy(sync_cache[cached].data, jrnl->jdata, Z_C4K);
+		cached++;
+
+		if ( cached == SYNC_CACHE_PAGES) {
+			rc = write_block(ti, DM_IO_KMEM,
+					 sync_cache, lba, cached, use_wq);
+			if (rc) {
+				Z_ERR(megaz->znd, "%s: cache-> %" PRIu64
+				      " [%d blks] %p -> %d",
+				      __func__, lba, nblks, jrnl->jdata, rc);
+				jderef(megaz, jrnl);
+				goto out;
+			}
+			lba    += cached;
+			jwrote += cached;
+			cached  = 0;
+		}
+		jrnl = jnext_entry(megaz, jrnl);
+	}
+
+	jwrote += cached;
+	if (jwrote > 20) {
+		Z_ERR(megaz->znd, "MZ#%u **WARNING** large map cache %d",
+		       megaz->mega_nr, jwrote);
+	}
+
+	if (megaz->znd->packed_meta) {
+		megazone_t *mz0 = &megaz->znd->z_mega[0];
+		mutex_lock(&mz0->zp_lock);
+		mz0->z_ptrs[z_id] |= lba + 2;
+		mz0->zfree_count[z_id] = 0;
+		mutex_unlock(&mz0->zp_lock);
+	}
+
+	mutex_lock(&megaz->zp_lock);
+	if (!megaz->znd->packed_meta) {
+		megaz->z_ptrs[z_id] = jwrote + sync_io_blocks;
+		if (generation & 1) {
+			z_id--;	// Zn+1 -> Zn
+		} else {
+			z_id++;	// Zn   -> Zn+1
+		}
+		if (megaz->z_ptrs[z_id]) {
+			do_reset_wp = 1;
+			megaz->z_ptrs[z_id] = 0;
+		}
+	}
+
+	megaz->bmkeys->n_crcs = cpu_to_le16(jwrote);
+	megaz->bmkeys->zp_crc = crc_md_le16(megaz->z_ptrs, Z_CRC_4K);
+	megaz->bmkeys->free_crc = crc_md_le16(megaz->zfree_count, Z_CRC_4K);
+	megaz->bmkeys->key_crc = 0;
+	megaz->bmkeys->key_crc = crc_md_le16(megaz->bmkeys, Z_CRC_4K);
+
+	if ( cached < (SYNC_CACHE_PAGES - 3)) {
+		memcpy(sync_cache[cached].data, megaz->bmkeys, Z_C4K);
+		cached++;
+		memcpy(sync_cache[cached].data, megaz->z_ptrs, Z_C4K);
+		cached++;
+		memcpy(sync_cache[cached].data, megaz->zfree_count, Z_C4K);
+		cached++;
+		need_sync_io = 0;
+	}
+
+	if (cached > 0) {
+		rc = write_block(ti, DM_IO_KMEM, sync_cache, lba, cached, use_wq);
+		if (rc) {
+			Z_ERR(megaz->znd, "%s: Jrnl-> %" PRIu64
+			      " [%d blks] %p -> %d",
+			      __func__, lba, cached, sync_cache, rc);
+			mutex_unlock(&megaz->zp_lock);
+			goto out;
+		}
+		lba += cached;
+	}
+
+	if (need_sync_io) {
+		void * data = megaz->sync_io;
+		nblks = sync_io_blocks;
+
+		// Z_ERR(megaz->znd, "SYNC: [S] Write %d blocks to %" PRIx64 "", nblks, lba);
+
+		rc = write_block(ti, DM_IO_KMEM, data, lba, nblks, use_wq);
+		if (rc) {
+			Z_ERR(megaz->znd, "%s: WPs -> %" PRIu64 " [%d blks] %p -> %d",
+			       __func__, lba, nblks, data, rc);
+			mutex_unlock(&megaz->zp_lock);
+			goto out;
+		}
+	}
+	mutex_unlock(&megaz->zp_lock);
+
+	if (do_reset_wp) {
+		dmz_reset_wp(megaz, z_id); // reset the 'other' alternate zone.
+	}
+
+out:
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int is_key_page(void *_data)
+{
+	int is_key = 0;
+	mz_key_block_t *data = _data;
+
+	/* Starts with Z_KEY_SIG and ends with magic */
+
+	if (Z_KEY_SIG == le64_to_cpu(data->sig[1])) {
+		if (Z_TABLE_MAGIC == le64_to_cpu(data->magic)) {
+			u16 crc_value = data->key_crc;
+			u16 crc_check;
+
+			data->key_crc = 0;
+			crc_check = crc_md_le16(data, Z_CRC_4K);
+			data->key_crc = crc_value;
+
+			if (crc_check == crc_value) {
+				is_key = 1;
+			}
+		}
+	}
+	return is_key;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void zoned_personality(zoned_t * znd, z_super_t * sblock)
+{
+	znd->first_zone = le64_to_cpu(sblock->first_zone);
+	znd->preserve_z0 = sb_test_flag(sblock, SB_Z0_RESERVED);
+	znd->packed_meta = le32_to_cpu(sblock->packed_meta);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int find_superblock_at(megazone_t * megaz, u64 lba, int use_wq,
+			      int do_init)
+{
+	struct dm_target *ti = megaz->znd->ti;
+	int found = 0;
+	int nblks = 1;
+	int rc = -ENOMEM;
+
+	u32 count = 0;
+	u64 *data = ZDM_ALLOC(megaz->znd, Z_C4K, PG_10);
+	if (!data) {
+		Z_ERR(megaz->znd, "No memory for finding generation ..");
+		return 0;
+	}
+
+	if (0 == lba) {
+		lba++;
+	}
+
+	do {
+		rc = read_block(ti, DM_IO_KMEM, data, lba, nblks, use_wq);
+		if (rc) {
+			Z_ERR(megaz->znd, "%s: read @%" PRIu64 " [%d blks] %p -> %d",
+			       __func__, lba, nblks, data, rc);
+			goto out;
+		}
+		if (is_key_page(data)) {
+			mz_key_block_t *kblk = (mz_key_block_t *) data;
+			z_super_t *sblock = &kblk->sblock;
+			int err = sb_check(sblock);
+			if (!err) {
+				found = 1;
+				if (do_init) {
+					zoned_personality(megaz->znd, sblock);
+				}
+			}
+			goto out;
+		}
+		if (0 == data[0] && 0 == data[1]) {
+			// No SB here.
+			Z_ERR(megaz->znd, "FGen: Invalid block %" PRIx64 "?", lba);
+			goto out;
+		}
+
+		lba++;
+		count++;
+		if (count > MAX_CACHE_SYNC) {
+			Z_ERR(megaz->znd, "FSB: Too deep to be useful.");
+			goto out;
+		}
+	} while (!found);
+
+out:
+	ZDM_FREE(megaz->znd, data, Z_C4K, PG_10);
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int find_superblock(megazone_t * megaz, int use_wq, int do_init)
+{
+	int found = 0;
+	u64 lba;
+	for (lba = 0; lba < 0x30000; lba += Z_BLKSZ) {
+		found = find_superblock_at(megaz, lba, use_wq, do_init);
+		if (found) {
+			break;
+		}
+	}
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 mcache_find_gen(megazone_t * megaz, u64 lba, int use_wq,
+				    u64 * sb_lba)
+{
+	struct dm_target *ti = megaz->znd->ti;
+	u64 generation = 0;
+	int nblks = 1;
+	int rc = 1;
+	int done = 0;
+	u32 count = 0;
+	u64 *data = ZDM_ALLOC(megaz->znd, Z_C4K, PG_11);
+	if (!data) {
+		Z_ERR(megaz->znd, "No memory for finding generation ..");
+		return 0;
+	}
+	do {
+		rc = read_block(ti, DM_IO_KMEM, data, lba, nblks, use_wq);
+
+		if (rc) {
+			Z_ERR(megaz->znd, "%s: Jrnl-> %" PRIu64 " [%d blks] %p -> %d",
+			       __func__, lba, nblks, data, rc);
+			goto out;
+		}
+		if (is_key_page(data)) {
+			mz_key_block_t *kblk = (mz_key_block_t *) data;
+			generation = le64_to_cpu(kblk->generation);
+
+			done = 1;
+			if (sb_lba) {
+				*sb_lba = lba;
+			}
+			goto out;
+		}
+		if (0 == data[0] && 0 == data[1]) {
+			// No SB here.
+			Z_ERR(megaz->znd, "FGen: Invalid block %" PRIx64 "?", lba);
+			goto out;
+		}
+
+		lba++;
+		count++;
+		if (count > MAX_CACHE_SYNC) {
+			Z_ERR(megaz->znd, "FGen: Too deep to be useful.");
+			goto out;
+		}
+	} while (!done);
+
+out:
+	ZDM_FREE(megaz->znd, data, Z_C4K, PG_11);
+	return generation;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int cmp_gen(u64 left, u64 right)
+{
+	int result = 0;
+
+	if (left != right) {
+		u64 delta = (left > right) ? left - right : right - left;
+		result = -1;
+
+		if (delta > 1) {
+			if (left == ~0ul) {
+				result = 1;
+			}
+		} else {
+			if (right > left) {
+				result = 1;
+			}
+		}
+	}
+
+	return result;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 mcache_greatest_gen(megazone_t * megaz, int use_wq, u64 * sb, u64 * at_lba)
+{
+	u64 lba = 0;
+	zoned_t *znd = megaz->znd;
+	u64 gen_no[3] = { 0ul, 0ul, 0ul };
+	u64 gen_lba[3] = { 0ul, 0ul, 0ul };
+	u64 gen_sb[3] = { 0ul, 0ul, 0ul };
+
+	u64 incr = Z_BLKSZ;
+	int locations = 2;
+	int pick = 0;
+	int idx;
+
+	if (znd->packed_meta) {
+		locations = ARRAY_SIZE(gen_lba);
+		incr = 512ul;
+		if (megaz->znd->preserve_z0) {
+			lba = Z_BLKSZ;
+		}
+		if (megaz->mega_nr >= 32) {
+			Z_ERR(megaz->znd, "TODO: Use lba of mz #32.");
+		} else {
+			lba += (locations * incr) * megaz->mega_nr;
+		}
+	} else {
+		lba = Z_BLKSZ * (megaz->mega_nr * 1024);
+		if (megaz->znd->preserve_z0 && megaz->mega_nr == 0) {
+			lba = Z_BLKSZ;
+		}
+	}
+
+	for (idx = 0; idx < locations; idx++) {
+		u64 *pAt = &gen_sb[idx];
+		gen_lba[idx] = lba;
+		gen_no[idx] = mcache_find_gen(megaz, lba, use_wq, pAt);
+		if (gen_no[idx]) {
+			pick = idx;
+		}
+		lba += incr;
+	}
+
+	for (idx = 0; idx < locations; idx++) {
+		if (cmp_gen(gen_no[pick], gen_no[idx]) > 0) {
+			pick = idx;
+		}
+	}
+
+	if (gen_no[pick]) {
+		if (at_lba) {
+			*at_lba = gen_lba[pick];
+		}
+		if (sb) {
+			*sb = gen_sb[pick];
+		}
+	}
+
+	return gen_no[pick];
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_init(megazone_t * megaz)
+{
+	struct dm_target *ti = megaz->znd->ti;
+	int nblks = 1;
+	int use_wq = 0;
+	int rc = 1;
+	int done = 0;
+	int jfound = 0;
+	int cindex = 0;
+	struct list_head hjload;
+	u64 lba = 0;
+	u64 generation;
+	u16 crc_chk;
+	io_4k_t * sync_cache = megaz->sync_cache;
+
+	INIT_LIST_HEAD(&hjload);
+
+	generation = mcache_greatest_gen(megaz, use_wq, NULL, &lba);
+	if ( 0 == generation ) {
+		return -ENODATA;
+	}
+
+	if (0 == lba) {
+		lba++;
+	}
+
+	do {
+		z_mapcache_t *jrnl = jalloc(megaz->znd);
+
+		if (!jrnl) {
+			return -ENOMEM;
+		}
+		rc = read_block(ti, DM_IO_KMEM,
+				jrnl->jdata, lba, nblks, use_wq);
+		if (rc) {
+			Z_ERR(megaz->znd, "%s: Jrnl-> %" PRIu64 " [%d blks] %p -> %d",
+			       __func__, lba, nblks, jrnl->jdata, rc);
+
+			return rc;
+		}
+		lba++;
+
+		if (is_key_page(jrnl->jdata)) {
+			size_t sz = Z_UNSORTED * sizeof(*jrnl->jdata);
+			memcpy(megaz->bmkeys, jrnl->jdata, Z_C4K);
+			jrnl->jcount = 0;
+			done = 1;
+			ZDM_FREE(megaz->znd, jrnl->jdata, sz, PG_08);
+			ZDM_FREE(megaz->znd, jrnl, sizeof(*jrnl), KM_07);
+			jrnl = NULL;
+		} else {
+			u16 jcount;
+			(void)le64_to_lba48(jrnl->jdata[0].physical, &jcount);
+			jrnl->jcount = jcount;
+			list_add(&(jrnl->jlist), &hjload);
+			jfound++;
+		}
+
+		if (jfound > MAX_CACHE_SYNC) {
+			return -EIO;
+		}
+
+	} while (!done);
+
+	crc_chk = megaz->bmkeys->key_crc;
+	megaz->bmkeys->key_crc = 0;
+	megaz->bmkeys->key_crc = crc_md_le16(megaz->bmkeys, Z_CRC_4K);
+
+	if (crc_chk != megaz->bmkeys->key_crc) {
+		Z_ERR(megaz->znd, "Bad Block Map KEYS!");
+		Z_ERR(megaz->znd, "MZ#%u Key CRC: Ex: %04x vs %04x <- calculated",
+		      megaz->mega_nr, le16_to_cpu(crc_chk),
+		      le16_to_cpu(megaz->bmkeys->key_crc));
+		rc = -EIO;
+	}
+
+	if (jfound != le16_to_cpu(megaz->bmkeys->n_crcs)) {
+		Z_ERR(megaz->znd, "MZ#%u mcache entries: found = %u, expected = %u",
+			megaz->mega_nr, jfound,
+			le16_to_cpu(megaz->bmkeys->n_crcs));
+		rc = -EIO;
+	}
+
+	if ((crc_chk == megaz->bmkeys->key_crc) && !list_empty(&hjload)) {
+		z_mapcache_t *jrnl;
+		z_mapcache_t *jsafe;
+
+		list_for_each_entry_safe(jrnl, jsafe, &hjload, jlist) {
+			u16 crc = crc_md_le16(jrnl->jdata, Z_CRC_4K);
+
+			Z_DBG(megaz->znd, "MZ#%u JRNL CRC: %u: %04x [vs %04x] (c:%d)",
+			      megaz->mega_nr, cindex, le16_to_cpu(crc),
+			      le16_to_cpu(megaz->bmkeys->crcs[cindex]),
+			      jrnl->jcount);
+
+			if (crc == megaz->bmkeys->crcs[cindex]) {
+				jlist_add(megaz, jrnl);
+			} else {
+				Z_ERR(megaz->znd, "Bad Journal Entries!! MZ#%u JRNL "
+				      "CRC: %u: %04x [vs %04x] (c:%d)",
+				      megaz->mega_nr, cindex, le16_to_cpu(crc),
+				      le16_to_cpu(megaz->bmkeys->crcs[cindex]),
+				      jrnl->jcount);
+				rc = -EIO;
+			}
+			cindex++;
+		}
+	}
+
+	mutex_lock(&megaz->zp_lock);
+
+	/*
+	 * Read last know write printers
+	 */
+	rc = read_block(ti, DM_IO_KMEM, sync_cache, lba, nblks, use_wq);
+	if (rc) {
+		Z_DBG(megaz->znd, "%s: WPs -> %" PRIu64 " [%d blks] %p -> %d",
+			 __func__, lba, nblks, megaz->z_ptrs, rc);
+	}
+	memcpy(megaz->z_ptrs, sync_cache, Z_C4K);
+	memcpy(megaz->z_commit, sync_cache, Z_C4K);
+
+	crc_chk = crc_md_le16(megaz->z_ptrs, Z_CRC_4K);
+	if (crc_chk != megaz->bmkeys->zp_crc) {
+		Z_ERR(megaz->znd, "MZ#%u WPs CRC: Ex %04x vs %04x  <- calculated",
+		      megaz->mega_nr, le16_to_cpu(megaz->bmkeys->zp_crc),
+		      le16_to_cpu(crc_chk));
+		Z_ERR(megaz->znd, "Bad zone pointers!!");
+		set_bit(DO_META_CHECK, &megaz->flags);
+	}
+	lba++;
+
+	/*
+	 * Read last calculates free counters
+	 */
+	rc = read_block(ti, DM_IO_KMEM, sync_cache, lba, nblks, use_wq);
+	if (rc) {
+		Z_DBG(megaz->znd, "%s: WPs -> %" PRIu64 " [%d blks] %p -> %d",
+			 __func__, lba, nblks, megaz->zfree_count, rc);
+	}
+	memcpy(megaz->zfree_count, sync_cache, Z_C4K);
+
+	crc_chk = crc_md_le16(megaz->zfree_count, Z_CRC_4K);
+	if (crc_chk != megaz->bmkeys->free_crc) {
+		Z_ERR(megaz->znd, "Bad zone free counters!!");
+		Z_ERR(megaz->znd, "MZ#%u FreeCount CRC: Ex %04x vs %04x  <- calculated",
+		      megaz->mega_nr, le16_to_cpu(megaz->bmkeys->free_crc),
+		      le16_to_cpu(crc_chk));
+		set_bit(DO_META_CHECK, &megaz->flags);
+	}
+
+	megaz->z_gc_resv = megaz->bmkeys->gc_resv;
+	megaz->z_meta_resv = megaz->bmkeys->meta_resv;
+
+	mutex_unlock(&megaz->zp_lock);
+
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_addmany(megazone_t * megaz, u64 dm_s, u64 lba, u64 count)
+{
+	sector_t blk;
+
+#if EXTRA_DEBUG
+	if (dm_s < 0x40000) {
+		Z_ERR(megaz->znd, "%s: %08" PRIx64 " -> %08" PRIx64
+		      " [%" PRIu64 " entries]", __func__, dm_s, lba, count);
+	}
+#endif // EXTRA_DEBUG
+	for (blk = 0; blk < count; blk++) {
+		int rc;
+		rc = z_mapped_add_one(megaz, dm_s + blk, lba + blk);
+		if (rc) {
+			return rc;
+		}
+	}
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/**
+ * Lookup a logical sector address to find the disk LBA
+ *
+ *   z_lookup                 (u64)
+ *     locate_sector          *, u64
+ *       sector_map_entry     (int)
+ *         get_map_entry      *, u64, *, int
+ *           get_map_table_entry *, *, u64, u64
+ *           load_page        u64, int, int, int, int, *, u16, int
+ *             z_lookup
+ *
+ *
+ *
+ *
+ */
+static u64 z_lookup(megazone_t * megaz, z_map_addr_t * maddr)
+{
+	u64 found = z_lookup_key_range(megaz, maddr);
+	if (!found) {
+		found = z_lookup_cache(megaz, maddr);
+	}
+	if (!found) {
+		found = locate_sector(megaz, maddr);
+	}
+	return found;
+}
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static z_mapped_t *load_map_entry(megazone_t * megaz, u64 lba, int is_map_to)
+{
+	z_mapped_t *mapped = get_map_table_entry(megaz, lba, is_map_to);
+	if (mapped) {
+		if (!mapped->mdata) {
+			int rc = map_entry_page(megaz, mapped, lba, is_map_to);
+			if (rc < 0) {
+				megaz->meta_result = rc;
+			}
+		}
+	} else {
+		Z_ERR(megaz->znd, "%s: No table for page# %" PRIx64 ".",
+		      __func__, lba);
+	}
+	return mapped;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int metadata_dirty_fling(megazone_t *megaz, u64 dm_s)
+{
+	zoned_t *znd = megaz->znd;
+	mz_lam_t * lam = &megaz->logical_map;
+	z_mapped_t *Smap = NULL;
+	int is_flung = 0;
+
+	Smap = NULL;
+	if ((lam->r_base <= dm_s) && dm_s < (lam->r_base + Z_BLKSZ)) {
+		Smap = load_map_entry(megaz, dm_s, 0);
+		if (!Smap) {
+			Z_ERR(znd, "Failed to fling: %" PRIx64, dm_s);
+		}
+	} else if ((lam->s_base <= dm_s) && dm_s < (lam->s_base + Z_BLKSZ)) {
+		Smap = load_map_entry(megaz, dm_s, 1);
+		if (!Smap) {
+			Z_ERR(znd, "Failed to fling: %" PRIx64, dm_s);
+		}
+	}
+	if (Smap) {
+		is_flung = 1;
+
+		REF(Smap->refcount);
+		mapped_lock(Smap);
+		Smap->age = jiffies_64;
+		set_bit(IS_DIRTY, &Smap->flags);
+		mapped_unlock(Smap);
+		DEREF(Smap->refcount);
+
+		if (Smap->lba != dm_s) {
+			Z_ERR(znd, "Excess churn? lba %"PRIx64
+			           " [last: %"PRIx64"]", dm_s, Smap->lba );
+		}
+	}
+
+	if ((lam->crc_low <= dm_s) && (dm_s < lam->crc_hi)) {
+		int off = dm_s - lam->crc_low;
+		mz_crc_block_t *pblock = NULL;
+		u64 lba;
+		u16 crc;
+
+		if (off < MZKY_NCRC) {
+			pblock = &megaz->rtm_crc[off];
+			lba = le64_to_cpu(megaz->bmkeys->rtm_crc_lba[off]);
+			crc = megaz->bmkeys->rtm_crc_pg[off];
+		} else {
+			off -= MZKY_NCRC;
+			pblock = &megaz->stm_crc[off];
+			lba = le64_to_cpu(megaz->bmkeys->stm_crc_lba[off]);
+			crc = megaz->bmkeys->stm_crc_pg[off];
+		}
+		if (pblock && lba) {
+			const int wqueue = 0;
+			is_flung = 1;
+
+			REF(pblock->refcount);
+			load_crc_meta_pg(megaz, pblock, lba, crc, wqueue);
+			mutex_lock(&pblock->lock_pg);
+			set_bit(IS_DIRTY, &pblock->flags);
+			pblock->age = jiffies_64;
+			mutex_unlock(&pblock->lock_pg);
+			if (pblock->lba != dm_s) {
+				Z_ERR(znd, "Excess churn? lba %"PRIx64
+				      " [last: %"PRIx64"]", dm_s, pblock->lba );
+			}
+			DEREF(pblock->refcount);
+		}
+	}
+	return is_flung;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void z_do_copy_more(z_gc_queue_entry_t * gc_entry)
+{
+	unsigned long flags;
+	zoned_t *znd = gc_entry->megaz->znd;
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	set_bit(DO_GC_PREPARE, &gc_entry->gc_flags);
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int verify_zone_is_empty(megazone_t *megaz, u32 z_id)
+{
+	zoned_t *znd = megaz->znd;
+	z_mapped_t *ORmap;
+	z_map_addr_t *ORaddr;
+	z_map_addr_t *Saddr;
+	mz_lam_t * lam = &megaz->logical_map;
+	u64 mz_data;
+	u64 from_lba;
+	int rcode = 0;
+	int count;
+
+	from_lba = (lam->mz_base + z_id) * Z_BLKSZ;
+	mz_data  = (lam->mz_base + ZDM_DATA_START_ZNR) * Z_BLKSZ;
+
+	mutex_lock(&megaz->mz_io_mutex);
+
+	ORaddr = ZDM_ALLOC(znd, sizeof(*ORaddr), KM_12);
+	Saddr = ZDM_ALLOC(znd, sizeof(*Saddr), KM_12);
+
+	if (!ORaddr || !Saddr) {
+		rcode = -ENOMEM;
+		goto out;
+	}
+
+	for (count = 0; count < Z_BLKSZ; count++) {
+		u32 ORencoded;
+		u64 ORlba = from_lba + count;
+
+		map_addr_calc(ORlba, ORaddr);
+		ORmap = reverse_map_entry(megaz, ORaddr);
+		if (ORmap && ORmap->mdata) {
+			int off = ORaddr->offentry;
+			REF(ORmap->refcount);
+			mapped_lock(ORmap);
+			BUG_ON(off >= 1024);
+
+			ORencoded = ORmap->mdata[off];
+			if (ORencoded != MZTEV_UNUSED) {
+				u64 dm_s = map_value(megaz, ORencoded);
+				u64 new_lba = 0ul;
+				int set_unused = 0;
+
+				if (dm_s < znd->nr_blocks) {
+					map_addr_calc(dm_s, Saddr);
+					new_lba = z_lookup(megaz, Saddr);
+				}
+
+				if ( 0ul == new_lba ) {
+					set_unused = 1;
+				} else if (ORlba != new_lba) {
+					set_unused = 1;
+				} else if (dm_s < mz_data) {
+					mapped_unlock(ORmap);
+					set_unused = metadata_dirty_fling(megaz, dm_s);
+					mapped_lock(ORmap);
+					Z_DBG(znd, "Metadata [%" PRIx64
+						"] %s moved from %"
+						PRIx64, dm_s,
+						set_unused ? "WAS" : "NOT",
+						ORlba);
+				}
+				if ( !set_unused ) {
+					Z_ERR(znd, "Not empty: lba %" PRIx64
+						" -> %" PRIx64 " -> %" PRIx64
+						" ... GC failed to move.",
+						ORlba, dm_s, new_lba );
+				}
+				if (set_unused) {
+					ORmap->mdata[off] = MZTEV_UNUSED;
+					set_bit(IS_DIRTY, &ORmap->flags);
+				}
+			}
+			mapped_unlock(ORmap);
+			DEREF(ORmap->refcount);
+		}
+	}
+
+out:
+	if (Saddr)
+		ZDM_FREE(znd, Saddr, sizeof(*Saddr), KM_12 );
+	if (ORaddr)
+		ZDM_FREE(znd, ORaddr, sizeof(*ORaddr), KM_12 );
+
+	mutex_unlock(&megaz->mz_io_mutex);
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int gc_post(megazone_t * megaz, u64 dm_s, u64 lba)
+{
+	zoned_t *znd = megaz->znd;
+	z_mapcache_t *post = &znd->gc_postmap;
+	int handled = 0;
+
+	if (post->jcount < post->jsize) {
+		u16 idx = ++post->jcount;
+
+		post->jdata[idx].logical = lba48_to_le64(0, dm_s);
+		post->jdata[idx].physical = lba48_to_le64(0, lba);
+		handled = 1;
+	}
+	return handled;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_zone_gc_metadata_to_ram(z_gc_queue_entry_t * gc_entry)
+{
+	megazone_t *megaz = gc_entry->megaz;
+	zoned_t *znd = megaz->znd;
+	z_mapped_t *ORmap;
+	z_map_addr_t *ORaddr;
+	z_mapped_t *Smap;
+	z_map_addr_t *Saddr;
+	mz_lam_t * lam = &megaz->logical_map;
+	u64 from_lba;
+	int rcode = 0;
+	int count;
+
+	from_lba = (lam->mz_base + gc_entry->z_gc) * Z_BLKSZ;
+
+	mutex_lock(&megaz->mz_io_mutex);
+
+	ORaddr = ZDM_ALLOC(znd, sizeof(*ORaddr), KM_12);
+	Saddr = ZDM_ALLOC(znd, sizeof(*Saddr), KM_12);
+
+	if (!ORaddr || !Saddr ) {
+		rcode = -ENOMEM;
+		goto out;
+	}
+
+	for (count = 0; count < MZKY_NCRC; count++) {
+		mz_crc_block_t *pblock;
+		u16 crc;
+		u64 lba = le64_to_cpu(megaz->bmkeys->rtm_crc_lba[count]);
+		int wqueue = 0;
+
+		if ( lba && (_calc_zone(lba) == gc_entry->z_gc) ) {
+			pblock = &megaz->rtm_crc[count];
+			crc = megaz->bmkeys->rtm_crc_pg[count];
+			REF(pblock->refcount);
+			load_crc_meta_pg(megaz, pblock, lba, crc, wqueue);
+			mutex_lock(&pblock->lock_pg);
+			set_bit(IS_DIRTY, &pblock->flags);
+			pblock->age = jiffies_64;
+			mutex_unlock(&pblock->lock_pg);
+			DEREF(pblock->refcount);
+		}
+		lba = le64_to_cpu(megaz->bmkeys->stm_crc_lba[count]);
+		if ( lba && (_calc_zone(lba) == gc_entry->z_gc) ) {
+			pblock = &megaz->stm_crc[count];
+			crc = megaz->bmkeys->stm_crc_pg[count];
+			REF(pblock->refcount);
+			load_crc_meta_pg(megaz, pblock, lba, crc, wqueue);
+			mutex_lock(&pblock->lock_pg);
+			set_bit(IS_DIRTY, &pblock->flags);
+			pblock->age = jiffies_64;
+			mutex_unlock(&pblock->lock_pg);
+			DEREF(pblock->refcount);
+		}
+	}
+
+	// pull all of the affect z_mapped_t and crc pages into memory:
+	for (count = 0; count < Z_BLKSZ; count++) {
+		u32 ORencoded;
+		u64 ORlba = from_lba + count;
+
+		map_addr_calc(ORlba, ORaddr);
+		ORmap = reverse_map_entry(megaz, ORaddr);
+
+		if (ORmap && ORmap->mdata) {
+			REF(ORmap->refcount);
+
+			BUG_ON(ORaddr->offentry >= 1024);
+
+			mapped_lock(ORmap);
+			ORencoded = ORmap->mdata[ORaddr->offentry];
+			mapped_unlock(ORmap);
+
+			if (ORencoded != MZTEV_UNUSED) {
+				u64 dm_s = map_value(megaz, ORencoded);
+				if (dm_s < znd->nr_blocks) {
+					map_addr_calc(dm_s, Saddr);
+					Smap = sector_map_entry(megaz, Saddr);
+					if (!Smap) {
+						rcode = -ENOMEM;
+					}
+				} else {
+					Z_ERR(znd, "Invalid rmap entry: %x.",
+					      ORencoded);
+				}
+				BUG_ON(dm_s >= znd->nr_blocks);
+
+				if (!metadata_dirty_fling(megaz, dm_s)) {
+					gc_post(megaz, dm_s, ORlba);
+				}
+			}
+			DEREF(ORmap->refcount);
+		}
+	}
+out:
+	if (Saddr)
+		ZDM_FREE(znd, Saddr, sizeof(*Saddr), KM_12 );
+	if (ORaddr)
+		ZDM_FREE(znd, ORaddr, sizeof(*ORaddr), KM_12 );
+	mutex_unlock(&megaz->mz_io_mutex);
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int append_blks(megazone_t *megaz, u64 lba, io_4k_t * io_buf, int count)
+{
+	int rcode = 0;
+	int rc;
+	u32 chunk;
+	zoned_t *znd = megaz->znd;
+	io_4k_t *sync_cache = megaz->sync_cache;
+
+	for (chunk = 0; chunk < count; chunk += SYNC_CACHE_PAGES) {
+		u32 nblks = count - chunk;
+
+		if (nblks > SYNC_CACHE_PAGES) {
+			nblks = SYNC_CACHE_PAGES;
+		}
+
+		rc = read_block(znd->ti, DM_IO_KMEM, sync_cache, lba, nblks, 0);
+		if (rc) {
+			Z_ERR(znd, "Reading error ... disable zone: %u",
+				(u32)(lba >> 16) );
+			rcode = -EIO;
+			goto out;
+		}
+		memcpy(&io_buf[chunk], sync_cache, nblks * Z_C4K);
+		lba += nblks;
+	}
+out:
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_zone_gc_read(z_gc_queue_entry_t * gc_entry)
+{
+	megazone_t *megaz = gc_entry->megaz;
+	zoned_t *znd = megaz->znd;
+	io_4k_t * io_buf = megaz->znd->gc_io_buf;
+	z_mapcache_t *post = &znd->gc_postmap;
+	unsigned long flags;
+	u64 start_lba;
+	int nblks;
+	int rcode = 0;
+	int fill = 0;
+	int jstart;
+	int jentry;
+
+	spin_lock_irqsave(&megaz->znd->gc_lock, flags);
+	jstart = gc_entry->r_ptr;
+	spin_unlock_irqrestore(&megaz->znd->gc_lock, flags);
+
+	if (0 == jstart) {
+		jstart++;
+	}
+
+	mutex_lock(&post->cached_lock);
+
+	/* A discard may have puched holes in the postmap. re-sync lba */
+	jentry = jstart;
+	while ( jentry <= post->jcount && (Z_LOWER48 ==
+			le64_to_lba48(post->jdata[jentry].physical, NULL)) ) {
+		jentry++;
+	}
+	/* nothing left to move */
+	if (jentry > post->jcount) {
+		goto out_finished;
+	}
+
+	/* skip over any discarded blocks */
+	if (jstart != jentry) {
+		jstart = jentry;
+	}
+
+	start_lba = le64_to_lba48(post->jdata[jentry].physical, NULL);
+	post->jdata[jentry].physical = lba48_to_le64(GC_READ, start_lba);
+	nblks = 1;
+	jentry++;
+
+	while (jentry <= post->jcount && (nblks+fill) < GC_MAX_STRIPE) {
+		u64 dm_s = le64_to_lba48(post->jdata[jentry].logical, NULL);
+		u64 lba = le64_to_lba48(post->jdata[jentry].physical, NULL);
+
+		if ( Z_LOWER48 == dm_s || Z_LOWER48 == lba ) {
+			jentry++;
+			continue;
+		}
+
+		post->jdata[jentry].physical = lba48_to_le64(GC_READ, lba);
+
+		/* if the block is contiguous add it to the read */
+		if (lba == (start_lba + nblks) ) {
+			nblks++;
+		} else {
+			if (nblks) {
+				int err;
+				err = append_blks(megaz, start_lba, &io_buf[fill], nblks);
+				if (err) {
+					rcode = err;
+					goto out;
+				}
+				fill += nblks;
+			}
+			start_lba = lba;
+			nblks = 1;
+		}
+		jentry++;
+	}
+
+	/* Issue a copy of 'nblks' blocks */
+	if (nblks > 0) {
+		int err;
+		err = append_blks(megaz, start_lba, &io_buf[fill], nblks);
+		if (err) {
+			rcode = err;
+			goto out;
+		}
+		fill += nblks;
+	}
+
+out_finished:
+	Z_DBG(znd, "Read %d blocks from %d", fill, gc_entry->r_ptr );
+
+	spin_lock_irqsave(&megaz->znd->gc_lock, flags);
+	gc_entry->nblks = fill;
+	gc_entry->r_ptr = jentry;
+	if (fill > 0) {
+		set_bit(DO_GC_WRITE, &gc_entry->gc_flags);
+	} else {
+		set_bit(DO_GC_COMPLETE, &gc_entry->gc_flags);
+	}
+	spin_unlock_irqrestore(&megaz->znd->gc_lock, flags);
+
+out:
+	mutex_unlock(&post->cached_lock);
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline u64 current_mapping(megazone_t *megaz, u64 dm_s)
+{
+	z_map_addr_t maddr;
+
+	map_addr_calc(dm_s, &maddr);
+	return z_lookup(megaz, &maddr);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_zone_gc_write(z_gc_queue_entry_t * gc_entry)
+{
+	megazone_t *megaz = gc_entry->megaz;
+	zoned_t *znd = megaz->znd;
+	io_4k_t * io_buf = megaz->znd->gc_io_buf;
+	z_mapcache_t *post = &znd->gc_postmap;
+	unsigned long flags;
+	u64 lba;
+	u32 nblks;
+	u32 out = 0;
+	int err = 0;
+	int jentry;
+
+	spin_lock_irqsave(&megaz->znd->gc_lock, flags);
+	jentry = gc_entry->w_ptr;
+	nblks = gc_entry->nblks;
+	spin_unlock_irqrestore(&megaz->znd->gc_lock, flags);
+
+	if (0 == jentry) {
+		jentry++;
+	}
+
+	mutex_lock(&post->cached_lock);
+
+	while (nblks > 0) {
+		u32 nfound = 0;
+		u32 added = 0;
+
+		lba = z_acquire(megaz, Z_AQ_GC, nblks, &nfound);
+		if (!lba) {
+			if (nfound) {
+				u32 avail = nfound;
+				nfound = 0;
+				lba = z_acquire(megaz, Z_AQ_GC, avail, &nfound);
+			}
+		}
+
+		if (!lba) {
+			err = -ENOSPC;
+			goto out;
+		}
+
+		err = write_block(znd->ti, DM_IO_VMA, &io_buf[out], lba, nfound, 0);
+		if (err) {
+			Z_ERR(znd, "Write %d blocks to %"PRIx64". ERROR: %d",
+			      nfound, lba, err );
+			goto out;
+		}
+		out += nfound;
+
+		while ( (jentry <= post->jcount) && (added < nfound) ) {
+			u16 rflg;
+			u64 orig = le64_to_lba48(post->jdata[jentry].physical, &rflg);
+			u64 dm_s = le64_to_lba48(post->jdata[jentry].logical, NULL);
+
+			if ( (Z_LOWER48 == dm_s || Z_LOWER48 == orig) ) {
+				jentry++;
+
+				if (rflg & GC_READ) {
+					Z_ERR(znd, "ERROR: %"PRIx64" read and not"
+						   " written %"PRIx64"", orig, dm_s);
+					lba++;
+					added++;
+				}
+				continue;
+			}
+			rflg &= ~GC_READ;
+			post->jdata[jentry].physical = lba48_to_le64(rflg, lba);
+			lba++;
+			added++;
+			jentry++;
+		}
+		nblks -= nfound;
+	}
+	Z_DBG(znd, "Write %d blocks from %d", gc_entry->nblks, gc_entry->w_ptr );
+	set_bit(DO_GC_META, &gc_entry->gc_flags);
+
+out:
+
+	spin_lock_irqsave(&megaz->znd->gc_lock, flags);
+	gc_entry->nblks = 0;
+	gc_entry->w_ptr = jentry;
+	spin_unlock_irqrestore(&megaz->znd->gc_lock, flags);
+
+	mutex_unlock(&post->cached_lock);
+
+	return err;
+}
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int gc_finalize(z_gc_queue_entry_t * gc_entry)
+{
+	int err = 0;
+	megazone_t *megaz = gc_entry->megaz;
+	z_mapcache_t *post = &megaz->znd->gc_postmap;
+
+	int jentry;
+
+	mutex_lock(&post->cached_lock);
+	for (jentry = post->jcount; jentry > 0; jentry--) {
+		u64 dm_s = le64_to_lba48(post->jdata[jentry].logical, NULL);
+		u64 lba = le64_to_lba48(post->jdata[jentry].physical, NULL);
+
+		if (dm_s != Z_LOWER48 || lba != Z_LOWER48) {
+			Z_ERR(megaz->znd, "GC: Failed to move MZ# %u "
+			                  "%"PRIx64" from %"PRIx64" [%d]",
+				megaz->mega_nr, dm_s, lba, jentry);
+			err = -EIO;
+		}
+	}
+	mutex_unlock(&post->cached_lock);
+
+	post->jcount = jentry;
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_zone_gc_metadata_update(z_gc_queue_entry_t * gc_entry)
+{
+	megazone_t *megaz = gc_entry->megaz;
+	zoned_t *znd = megaz->znd;
+	z_mapcache_t *post = &znd->gc_postmap;
+	u32 used = post->jcount;
+	int err = 0;
+	int jentry;
+	mz_lam_t * lam = &megaz->logical_map;
+
+	mutex_lock(&megaz->mz_io_mutex);
+
+	for (jentry = post->jcount; jentry > 0; jentry--) {
+		unsigned long flags;
+		int discard = 0;
+		int mapping = 0;
+		z_mapped_t *mapped = NULL;
+		u64 dm_s = le64_to_lba48(post->jdata[jentry].logical, NULL);
+		u64 lba = le64_to_lba48(post->jdata[jentry].physical, NULL);
+
+		if ((lam->r_base <= dm_s) && dm_s < (lam->r_base + Z_BLKSZ)) {
+			u64 off = dm_s - lam->r_base;
+			mapped = megaz->reversetm[off];
+			mapping = 1;
+		} else if ((lam->s_base <= dm_s) &&
+			   (dm_s < (lam->s_base + Z_BLKSZ))) {
+			u64 off = dm_s - lam->s_base;
+			mapped = megaz->sectortm[off];
+			mapping = 1;
+		}
+
+		if (mapping && !mapped) {
+			Z_ERR(megaz->znd, "MD: dm_s: %" PRIx64 " -> lba: %" PRIx64
+				 " no mapping in ram.", dm_s, lba);
+		}
+
+		if (mapped) {
+			REF(mapped->refcount);
+			mapped_lock(mapped);
+			if (_calc_zone(mapped->last_write) != gc_entry->z_gc) {
+				Z_ERR(znd, "MD: %" PRIx64
+				      " Discarded - %" PRIx64
+				      " already flown to: %x",
+				      dm_s, mapped->last_write,
+				      _calc_zone(mapped->last_write));
+				discard = 1;
+			} else if (mapped->mdata &&
+				   test_bit(IS_DIRTY, &mapped->flags)) {
+				Z_ERR(znd, "MD: %" PRIx64 " Discarded - %"PRIx64
+				       " is in-flight",
+				       dm_s, mapped->last_write);
+				discard = 2;
+			}
+			if (!discard) {
+				mapped->last_write = lba;
+			}
+			mapped_unlock(mapped);
+			DEREF(mapped->refcount);
+		}
+
+		/*
+		 * location of the SLT key sectors need to be
+		 * stashed into the sector lookup table block map
+		 * Does dm_s point in the sector lookup table block map ?
+		 */
+		if ((lam->sk_low <= dm_s) && (dm_s < lam->sk_high)) {
+			if (!discard) {
+				int off = dm_s - lam->sk_low;
+
+				spin_lock_irqsave(&megaz->jlock, flags);
+				megaz->bmkeys->stm_keys[off] = cpu_to_le64(lba);
+				spin_unlock_irqrestore(&megaz->jlock, flags);
+			}
+		} else if (lam->crc_low <= dm_s && dm_s < lam->crc_hi) {
+			const u16 z_gc = gc_entry->z_gc;
+			int off = dm_s - lam->crc_low;
+			mz_crc_block_t *pblock = NULL;
+
+			spin_lock_irqsave(&megaz->jlock, flags);
+
+			if (off < 32) {
+				pblock = &megaz->rtm_crc[off];
+			} else {
+				pblock = &megaz->stm_crc[off - 32];
+			}
+
+			if (_calc_zone(pblock->last_write) != z_gc) {
+
+				Z_ERR(znd, "MD: %" PRIx64 " Discarded - %"PRIx64
+				        " already flown to: %x [CRC]",
+				       dm_s, pblock->last_write,
+				       _calc_zone(pblock->last_write));
+				discard = 1;
+			} else if (pblock->crc_pg &&
+				   test_bit(IS_DIRTY, &pblock->flags)) {
+
+				Z_ERR(znd, "MD: %" PRIx64 " Discarded - %"PRIx64
+				      " is in-flight [CRC]",
+				      dm_s, pblock->last_write);
+				discard = 2;
+			}
+
+			/* update current lba for 'moved' and 'in-flight' */
+			if (1 != discard) {
+				mutex_lock(&pblock->lock_pg);
+
+				Z_ERR(znd, "MD: %" PRIx64 " mv from  %"PRIx64
+				           " to: %" PRIu64 " [CRC]",
+				      dm_s, pblock->last_write, lba);
+
+				pblock->last_write = lba;
+				if (off < 32) {
+					megaz->bmkeys->rtm_crc_lba[off] =
+					    cpu_to_le64(lba);
+				} else {
+					megaz->bmkeys->stm_crc_lba[off - 32] =
+					    cpu_to_le64(lba);
+				}
+				mutex_unlock(&pblock->lock_pg);
+			}
+			spin_unlock_irqrestore(&megaz->jlock, flags);
+		}
+		spin_lock_irqsave(&megaz->jlock, flags);
+		if (1 == discard) {
+			post->jdata[jentry].logical = ~0ul;
+			post->jdata[jentry].physical = ~0ul;
+		}
+		if (~0ul == post->jdata[jentry].logical &&
+		    ~0ul == post->jdata[jentry].physical) {
+			used--;
+		} else {
+			u16 zone = _calc_zone(lba);
+			megaz->z_commit[zone]++;
+			if (megaz->z_commit[zone] == Z_BLKSZ) {
+				mutex_lock(&megaz->zp_lock);
+				megaz->z_ptrs[zone] |= Z_WP_GC_READY;
+				mutex_unlock(&megaz->zp_lock);
+			}
+		}
+		spin_unlock_irqrestore(&megaz->jlock, flags);
+	}
+
+	err = move_to_map_tables(megaz, post);
+	if (err) {
+		Z_ERR(znd, "Move to tables post GC failure");
+	}
+
+	mutex_unlock(&megaz->mz_io_mutex);
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static sector_t _blkalloc(megazone_t * megaz, u32 z_at, u32 nblks, u32 * nfound)
+{
+	z_wptr_t wptr = megaz->z_ptrs[z_at];
+	sector_t found = 0;
+	u32 avail = 0;
+	int do_open_zone = 0;
+
+	if (wptr < Z_BLKSZ) {
+		avail = Z_BLKSZ - wptr;
+	}
+
+	*nfound = avail;
+
+	if (nblks <= avail) {
+		u64 disk_zone = ((megaz->mega_nr * 1024) + z_at);
+
+		mutex_lock(&megaz->zp_lock);
+		found = (disk_zone * Z_BLKSZ) + wptr;
+		*nfound = nblks;
+		if (0 == megaz->z_ptrs[z_at]) {
+			do_open_zone = 1;
+		}
+		megaz->z_ptrs[z_at] += nblks;
+		megaz->zfree_count[z_at] -= nblks;
+		mutex_unlock(&megaz->zp_lock);
+
+		if (do_open_zone) {
+			dmz_open_zone(megaz, z_at);
+		}
+	}
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u16 _gc_tag = 1;
+
+/**
+ * Add a zone onto the GC queue -- and kick start the compact operation if
+ *  not already in progress.
+ */
+static int z_zone_compact_queue(megazone_t * megaz, u64 z_gc, int delay)
+{
+	unsigned long flags;
+	zoned_t *znd = megaz->znd;
+	int do_queue = 0;
+	int err = 0;
+
+	z_gc_queue_entry_t *gc_entry = ZDM_ALLOC(znd, sizeof(*gc_entry), KM_16);
+	if (!gc_entry) {
+		Z_ERR(znd, "No Memory for compact!!");
+		return -ENOMEM;
+	}
+	gc_entry->megaz = megaz;
+	gc_entry->z_gc = z_gc;
+	gc_entry->tag = _gc_tag++;
+
+	set_bit(DO_GC_NEW, &gc_entry->gc_flags);
+	znd->gc_backlog++;
+
+	if (znd->gc_backlog > 1 || !delay) {
+		set_bit(ZF_NO_GC_DELAY, &znd->flags);
+		Z_ERR(znd, "do something about the backlog here?");
+	}
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	if (!znd->gc_active) {
+		znd->gc_active = gc_entry;
+		do_queue = 1;
+	}
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+
+	if (do_queue) {
+		Z_ERR(znd, "%s: Queue GC: MZ# %d Z# %" PRIx64 ", wp: "
+		      "%x, free %x - tag %u", __func__, megaz->mega_nr,
+		      z_gc, megaz->z_ptrs[z_gc], megaz->zfree_count[z_gc],
+		      gc_entry->tag);
+
+		queue_work(znd->gc_wq, &znd->gc_work);
+		Z_DBG(znd, "%s: Queue GC: %u", __func__, gc_entry->tag);
+	}
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * Called periodically to see if GC needs to be done. ...
+ */
+
+static int gc_compact_check(megazone_t * megaz, int delay)
+{
+	u32 min_blocks = megaz->aggressive_gc
+	    ? GC_COMPACT_AGGRESSIVE : GC_COMPACT_NORMAL;
+	int queued = 0;
+	int n_filled = 0;
+	int n_empty = 0;
+	int z_gc = megaz->z_data;
+	u16 top_roi[6] = { 0, };
+
+	Z_DBG(megaz->znd, "MZ# %u Checking zone range [%u,%u)",
+		megaz->mega_nr, z_gc, megaz->z_count);
+
+	for (; z_gc < megaz->z_count; z_gc++) {
+		u32 wp = megaz->z_ptrs[z_gc] & Z_WP_VALUE_MASK;
+		int is_ready = (megaz->z_ptrs[z_gc] & Z_WP_GC_READY) ? 1 : 0;
+		if (0 != (megaz->z_ptrs[z_gc] & (Z_WP_GC_PENDING))) {
+			n_filled++;
+			continue;	/* already queued or targeted for GC */
+		}
+		if (is_ready && (wp >= Z_BLKSZ) ) {
+			n_filled++;
+			if (megaz->zfree_count[z_gc] > min_blocks) {
+				int at;
+				u16 top = top_roi[0];
+				if (0 == top || (megaz->zfree_count[z_gc] >
+						 megaz->zfree_count[top])) {
+					for (at = 5; at > 0; at--) {
+						top_roi[at] = top_roi[at - 1];
+					}
+					top_roi[0] = z_gc;
+				}
+			}
+		} else if ((Z_BLKSZ - wp) < 0xff) {
+			n_filled++;
+		}
+		if (0 == megaz->z_ptrs[z_gc]) {
+			n_empty++;
+		}
+	}
+	megaz->aggressive_gc = (megaz->z_count - n_filled < 10) ? 1 : 0;
+	megaz->z_gc_free = n_empty;
+
+	if (top_roi[0]) {
+		u16 at = 0;
+		int err = 0;
+		for (at = 0; at < 5; at++) {
+			int do_queue_zone = 0;
+			z_gc = top_roi[at];
+			if (0 == z_gc) {
+				goto out;
+			}
+			if (n_empty > (megaz->z_count / 3)) {
+				/* lots of empty zones yet ... unless it's
+				   just too good to pass up: */
+				if (megaz->zfree_count[z_gc] > 0xFF00) {
+					do_queue_zone = 1;
+				}
+			} else {
+				if (megaz->zfree_count[z_gc] > 1024) {
+					do_queue_zone = 1;
+				}
+			}
+
+			if (megaz->aggressive_gc
+			    && megaz->zfree_count[z_gc] > 64)
+				do_queue_zone = 1;
+
+			if (do_queue_zone) {
+				err = z_zone_compact_queue(megaz, z_gc, delay);
+				if (err) {
+					goto out;
+				}
+				queued = 1;
+			}
+			goto out;
+		}
+	}
+out:
+	return queued;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void gc_yield(z_gc_queue_entry_t * gc_entry)
+{
+	int mdelay = 3;
+
+	might_sleep();
+
+	while (--mdelay > 0) {
+		if (test_bit(ZF_NO_GC_DELAY, &gc_entry->megaz->znd->flags)) {
+			return;
+		} else {
+			msleep_interruptible(10);
+		}
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void z_zone_gc_compact(z_gc_queue_entry_t * gc_entry)
+{
+	unsigned long flags;
+	int err = 0;
+	int do_meta_flush = 0;
+	megazone_t *megaz = gc_entry->megaz;
+
+	might_sleep();
+
+	megaz->age = jiffies_64;
+	if (test_and_clear_bit(DO_GC_NEW, &gc_entry->gc_flags)) {
+
+		mutex_lock(&megaz->zp_lock);
+		megaz->z_ptrs[gc_entry->z_gc] |= Z_WP_GC_FULL;
+		mutex_unlock(&megaz->zp_lock);
+
+		mutex_lock(&megaz->mz_io_mutex);
+		err = _journal_blocks_to_table(megaz, gc_entry->z_gc);
+		set_bit(DO_GC_NO_PURGE, &megaz->flags);
+		mutex_unlock(&megaz->mz_io_mutex);
+		if (err) {
+			Z_ERR(megaz->znd, "Failed to purge journal: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+
+		if (megaz->znd->gc_postmap.jcount > 0) {
+			Z_ERR(megaz->znd, "*** Unexpected data in postmap!!");
+			megaz->znd->gc_postmap.jcount = 0;
+		}
+
+		err = z_zone_gc_metadata_to_ram(gc_entry);
+		if (err) {
+			Z_ERR(megaz->znd, "Pre-load metadata to memory failed!! %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+		set_bit(DO_GC_PREPARE, &gc_entry->gc_flags);
+	}
+
+next_in_queue:
+	gc_yield(gc_entry);
+	megaz->age = jiffies_64;
+	if (test_and_clear_bit(DO_GC_PREPARE, &gc_entry->gc_flags)) {
+		mutex_lock(&megaz->mz_io_mutex);
+		err = z_zone_gc_read(gc_entry);
+		mutex_unlock(&megaz->mz_io_mutex);
+		if (err < 0) {
+			Z_ERR(megaz->znd, "z_zone_gc_chunk issue failure: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+	}
+
+	if (test_and_clear_bit(DO_GC_WRITE, &gc_entry->gc_flags)) {
+		mutex_lock(&megaz->mz_io_mutex);
+		err = z_zone_gc_write(gc_entry);
+		mutex_unlock(&megaz->mz_io_mutex);
+		if (err) {
+			Z_ERR(megaz->znd, "z_zone_gc_chunk issue failure: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+	}
+
+	if (test_and_clear_bit(DO_GC_META, &gc_entry->gc_flags)) {
+		z_do_copy_more(gc_entry);
+		goto next_in_queue;
+	}
+
+	megaz->age = jiffies_64;
+	if (test_and_clear_bit(DO_GC_COMPLETE, &gc_entry->gc_flags)) {
+
+		err = z_zone_gc_metadata_update(gc_entry);
+		gc_entry->result = err;
+
+		if (err) {
+			Z_ERR(megaz->znd, "Metadata error ... disable zone: %u",
+			      gc_entry->z_gc);
+		}
+
+		err = gc_finalize(gc_entry);
+		if (err) {
+			Z_ERR(megaz->znd, "GC: Failed to finalize: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+
+		gc_verify_cache(megaz, gc_entry->z_gc);
+
+		mutex_lock(&megaz->mz_io_mutex);
+		err = _journal_blocks_to_table(megaz, gc_entry->z_gc);
+		set_bit(DO_GC_NO_PURGE, &megaz->flags);
+		mutex_unlock(&megaz->mz_io_mutex);
+		if (err) {
+			Z_ERR(megaz->znd, "Failed to purge journal: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+		verify_zone_is_empty(megaz, gc_entry->z_gc);
+
+		// Release the zones for writing
+		dmz_reset_wp(megaz, gc_entry->z_gc);
+
+		mutex_lock(&megaz->zp_lock);
+		megaz->z_ptrs[gc_entry->z_gc] = 0;
+		megaz->z_commit[gc_entry->z_gc] = 0;
+		megaz->zfree_count[gc_entry->z_gc] = Z_BLKSZ;
+		megaz->z_gc_free++;
+		if (megaz->z_gc_resv & Z_WP_GC_ACTIVE) {
+			megaz->z_gc_resv = gc_entry->z_gc;
+		} else if (megaz->z_meta_resv & Z_WP_GC_ACTIVE) {
+			megaz->z_meta_resv = gc_entry->z_gc;
+		}
+		mutex_unlock(&megaz->zp_lock);
+
+		Z_ERR(megaz->znd, "%s: MZ# %d, z: 0x%x, wp:%08x,free:%x -> "
+		       "tag: %d GC DONE.",
+		       __func__, megaz->mega_nr,
+		       gc_entry->z_gc, megaz->z_ptrs[gc_entry->z_gc],
+		       megaz->zfree_count[gc_entry->z_gc],
+		       gc_entry->tag);
+
+		spin_lock_irqsave(&megaz->znd->gc_lock, flags);
+		megaz->znd->gc_backlog--;
+		megaz->znd->gc_active = NULL;
+		spin_unlock_irqrestore(&megaz->znd->gc_lock, flags);
+
+		ZDM_FREE(megaz->znd, gc_entry, sizeof(*gc_entry), KM_16);
+
+		set_bit(DO_JOURNAL_MOVE, &megaz->flags);
+		set_bit(DO_MEMPOOL, &megaz->flags);
+		set_bit(DO_SYNC, &megaz->flags);
+		set_bit(DO_SYNC_CRC, &megaz->flags);
+#if PARANOIA
+		set_bit(DO_META_CHECK, &megaz->flags);
+#endif // PARANOIA
+		do_meta_flush = 1;
+	}
+out:
+	clear_bit(DO_GC_NO_PURGE, &megaz->flags);
+	if (do_meta_flush) {
+		zoned_t * znd = megaz->znd;
+		if ( !work_pending(&megaz->meta_work) ) {
+			queue_work(znd->meta_wq, &megaz->meta_work);
+//			flush_workqueue(znd->meta_wq);
+		}
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void gc_work_task(struct work_struct *work)
+{
+	z_gc_queue_entry_t *gc_entry = NULL;
+	unsigned long flags;
+	zoned_t *znd;
+
+	if (!work) {
+		return;
+	}
+
+	znd = container_of(work, zoned_t, gc_work);
+
+	if (!znd) {
+		return;
+	}
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	if (znd->gc_active) {
+		gc_entry = znd->gc_active;
+	}
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+
+	if (!gc_entry) {
+		Z_ERR(znd, "ERROR: gc_active not set!");
+		return;
+	}
+
+	z_zone_gc_compact(gc_entry);
+
+	on_timeout_activity(znd);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int inline is_reserved(megazone_t * megaz, const u32 z_pref)
+{
+	const u32 gc   = megaz->z_gc_resv & Z_WP_VALUE_MASK;
+	const u32 meta = megaz->z_meta_resv & Z_WP_VALUE_MASK;
+
+	return (gc == z_pref || meta == z_pref) ? 1 : 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 z_acquire(megazone_t * megaz, u32 flags, u32 nblks, u32 * nfound)
+{
+	sector_t found = 0;
+	u32 z_pref = megaz->z_current;
+
+	found = _blkalloc(megaz, z_pref, nblks, nfound);
+	if (found || *nfound) {
+		goto out;
+	}
+
+	/* no space left in zone .. explicitly close it */
+	dmz_close_zone(megaz, z_pref);
+
+retry:
+	for (z_pref = megaz->z_data; z_pref < megaz->z_count; z_pref++) {
+		if (is_reserved(megaz, z_pref)) {
+			continue;
+		}
+		found = _blkalloc(megaz, z_pref, nblks, nfound);
+		if (found || *nfound) {
+			megaz->z_current = z_pref;
+			megaz->z_gc_free--;
+			goto out;
+		}
+	}
+
+	if (flags & Z_AQ_GC) {
+		u32 gresv = megaz->z_gc_resv & Z_WP_VALUE_MASK;
+
+		Z_ERR(megaz->znd, "MZ# %u: Using GC Reserve (%u)",
+			megaz->mega_nr, gresv);
+		found = _blkalloc(megaz, gresv, nblks, nfound);
+		megaz->z_gc_resv |= Z_WP_GC_ACTIVE;
+	}
+
+	if (flags & Z_AQ_META) {
+		int can_retry = 0;
+		zoned_t * znd = megaz->znd;
+		int queued;
+
+		queued = gc_compact_check(megaz, 0);
+
+		Z_ERR(znd, "%s: MZ# %u: Metadata ... no space. Try GC q'd=%d.",
+		       __func__, megaz->mega_nr, queued);
+
+		if ( test_and_set_bit(ZF_NO_GC_DELAY, &znd->flags) ) {
+			if (work_pending(&znd->gc_work)) {
+				can_retry = 1;
+
+				Z_ERR(znd, "Metadata acquire ..."
+				   " Flushing GC queue.");
+
+				flush_workqueue(znd->gc_wq);
+			}
+		}
+
+		if (can_retry) {
+			goto retry;
+		} else {
+			u32 mresv = megaz->z_meta_resv & Z_WP_VALUE_MASK;
+
+			Z_ERR(megaz->znd, "MZ# %u: Using META Reserve (%u)",
+				megaz->mega_nr, megaz->z_meta_resv);
+			found = _blkalloc(megaz, mresv, nblks, nfound);
+		}
+	}
+
+out:
+	if ( !found && (0 == *nfound) ) {
+		Z_ERR(megaz->znd, "%s: -> MZ# %u: Out of space.",
+		       __func__, megaz->mega_nr);
+	}
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void do_free_mapped(megazone_t * megaz, z_mapped_t * mapped)
+{
+	if (!test_bit(DO_GC_NO_PURGE, &megaz->flags)) {
+		int entry;
+
+		mapped_lock(mapped);
+		if ( test_bit(IS_DIRTY, &mapped->flags) ||
+		    (mapped->refcount.counter > 0) ) {
+			Z_ERR(megaz->znd, "%s: Cannot free: %"PRIx64,
+				__func__, mapped->lba );
+			mapped_unlock(mapped);
+			return;
+		}
+
+		spin_lock(&megaz->map_pool_lock);
+		list_del(&(mapped->inpool));
+		mapped->inpool.prev = mapped->inpool.next = NULL;
+		spin_unlock(&megaz->map_pool_lock);
+
+		if (mapped->mdata) {
+			ZDM_FREE(megaz->znd, mapped->mdata, Z_C4K, PG_27);
+			megaz->incore_count--;
+		}
+		mapped_unlock(mapped);
+
+		entry = mapped->lba & 0xFFFF;
+		if (2 == (mapped->lba / Z_BLKSZ) % 1024) {
+			z_mapped_t * mentry = megaz->reversetm[entry];
+			ZDM_FREE(megaz->znd, mentry, sizeof(*mentry), KM_20);
+			megaz->reversetm[entry] = NULL;
+		} else {
+			z_mapped_t * mentry = megaz->sectortm[entry];
+			ZDM_FREE(megaz->znd, mentry, sizeof(*mentry), KM_20);
+			megaz->sectortm[entry] = NULL;
+		}
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int expunge_oldest_pages(megazone_t * megaz, int count)
+{
+	struct list_head *_pages = &(megaz->smtpool);
+	z_mapped_t *_oldpg;
+	z_mapped_t *t_pg;
+	z_mapped_t *oldest = NULL;
+	u64 tpurge = msecs_to_jiffies(MEM_PURGE_MSECS);
+	u64 tnow = jiffies_64;
+	tpurge = (tpurge < tnow) ? tnow - tpurge : 0;
+
+	if (list_empty(_pages)) {
+		return 0;
+	}
+
+	list_for_each_entry_safe_reverse(_oldpg, t_pg, _pages, inpool) {
+		if (count-- > 0) {
+			oldest = _oldpg;
+
+			if (test_bit(IS_DIRTY, &oldest->flags)) {
+				write_if_dirty(megaz, oldest, 0);
+			} else {
+				if (tpurge &&
+				    time_before64(oldest->age, tpurge) &&
+				    (0 == oldest->refcount.counter)) {
+					do_free_mapped(megaz, oldest);
+				}
+			}
+		}
+	}
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int load_crc_meta_pg(megazone_t * megaz, mz_crc_block_t *pblock,
+                            u64 pg_lba, u16 crc16, int use_wq )
+{
+	int err = 0;
+
+	if (pblock && !pblock->crc_pg) {
+		u16 *data = ZDM_ALLOC(megaz->znd, Z_C4K, PG_17);
+		if (!data) {
+			Z_ERR(megaz->znd, "Out of memory");
+			err = -ENOMEM;
+			goto out;
+		}
+
+		if (pg_lba) {
+			u16 check;
+			int count = 1;
+
+			Z_DBG(megaz->znd, "get_meta_pg_crc:  %" PRIx64
+			                  " from lba: %" PRIx64,
+			                  pblock->lba, pg_lba);
+
+			if (warn_bad_lba(megaz, pg_lba)) {
+				Z_ERR(megaz->znd, "Bad CRC pg %" PRIx64, pg_lba);
+			}
+
+			err = read_block(megaz->znd->ti, DM_IO_KMEM, data,
+					pg_lba, count, use_wq);
+			if (err) {
+				ZDM_FREE(megaz->znd, data, Z_C4K, PG_17);
+
+				Z_ERR(megaz->znd, "Read of CRC page @%" PRIx64
+			              " failed: %d", pg_lba, err);
+				goto out;
+			}
+			check = crc_md_le16(data, Z_CRC_4K);
+
+			if (check == crc16) {
+				pblock->last_write = pg_lba;
+			} else {
+				Z_ERR(megaz->znd, "CRC PG: %" PRIx64 " from %"
+				      PRIx64 " [%04x != %04x]"
+				      " last written to: %" PRIx64 "",
+				      pblock->lba, pg_lba,
+				      le16_to_cpu(check), le16_to_cpu(crc16),
+				      pblock->last_write);
+			}
+		} else {
+			memset(data, 0, Z_C4K);
+		}
+		pblock->crc_pg = data;
+	}
+	if (pblock) {
+		pblock->age = jiffies_64;
+	}
+
+out:
+        return err;
+}
+
+static mz_crc_block_t *get_meta_pg_crc(megazone_t * megaz,
+				       z_map_addr_t * madr,
+				       int is_map_to, int use_wq)
+{
+	mz_crc_block_t *pblock = NULL;
+	int pg_no = (madr->mz_off & 0xFFFF) / 2048;
+	u16 crc16 = 0;
+	u64 pg_lba = 0ul;
+	int is_rtz = is_reverse_table_zone(megaz, madr);
+	int err;
+
+	if (is_rtz == is_map_to) {
+		Z_ERR(megaz->znd, "CRC: is_rtz == is_map_to [%d != %d] lba: %"
+		      PRIx64, is_rtz, is_map_to, madr->dm_s);
+	}
+
+	if (0 == is_map_to) {
+		pblock = &megaz->rtm_crc[pg_no];
+		crc16 = megaz->bmkeys->rtm_crc_pg[pg_no];
+		pg_lba = le64_to_cpu(megaz->bmkeys->rtm_crc_lba[pg_no]);
+	} else {
+		pblock = &megaz->stm_crc[pg_no];
+		crc16 = megaz->bmkeys->stm_crc_pg[pg_no];
+		pg_lba = le64_to_cpu(megaz->bmkeys->stm_crc_lba[pg_no]);
+	}
+
+	if (pblock->last_write && pg_lba != pblock->last_write) {
+		Z_ERR(megaz->znd, "load1 %"PRIx64", last %"PRIx64,
+		      pg_lba, pblock->last_write );
+	}
+
+	err = load_crc_meta_pg(megaz, pblock, pg_lba, crc16, use_wq);
+	if (err) {
+		pblock = NULL;
+	}
+	return pblock;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int write_if_dirty(megazone_t * megaz, z_mapped_t * oldest, int use_wq)
+{
+	int rcode = 0;
+	z_map_addr_t *maddr = NULL;
+
+	mapped_lock(oldest);
+
+	if (test_bit(IS_DIRTY, &oldest->flags)
+	    && (0 == oldest->refcount.counter)) {
+
+		u32 nblks = 1;
+		u32 nfound = 0;
+		u64 dm_s = oldest->lba;
+		u64 lba;
+
+		maddr = ZDM_ALLOC(megaz->znd, sizeof(*maddr), KM_18);
+		if (!maddr) {
+			rcode = -ENOMEM;
+			goto out;
+		}
+
+		/* we need the maddr to acquire a free block in
+		 * the correct megazone */
+		map_addr_calc(dm_s, maddr);
+		lba = z_acquire(megaz, Z_AQ_META, nblks, &nfound);
+		if (lba && nfound) {
+			int rcwrt;
+			int count = 1;
+			void *data = oldest->mdata;
+			int crce = (maddr->mz_off & 0xFFFF) % 2048;
+			mz_crc_block_t *pblock;
+			int is_to = !is_reverse_table_zone(megaz, maddr);
+			u32 crcb4;
+
+			Z_DBG(megaz->znd, "%" PRIx64 " -> is map to %d", dm_s, is_to);
+
+			pblock = get_meta_pg_crc(megaz, maddr, is_to, use_wq);
+			if (!pblock) {
+				Z_ERR(megaz->znd, "%s: Out of space for metadata?",
+				      __func__);
+				rcode = -ENOSPC;
+				goto out;
+			}
+
+			REF(pblock->refcount);
+			mutex_lock(&pblock->lock_pg);
+			pblock->crc_pg[crce] = crc_md_le16(data, Z_CRC_4K);
+			crcb4 = crcpg(data);
+			set_bit(IS_DIRTY, &pblock->flags);
+			pblock->age = jiffies_64;
+			mutex_unlock(&pblock->lock_pg);
+			DEREF(pblock->refcount);
+
+			Z_DBG(megaz->znd, "write_page: %" PRIx64 " -> %" PRIx64,
+				 dm_s, lba);
+
+			rcwrt = write_block(megaz->znd->ti, DM_IO_KMEM, data,
+					    lba, count, use_wq);
+
+			oldest->age = jiffies_64;
+			oldest->last_write = lba;
+
+			if (rcwrt) {
+				Z_ERR(megaz->znd, "write_page: %" PRIx64 " -> %"
+				      PRIx64 " ERR: %d",
+				      oldest->lba, lba, rcwrt);
+				rcode = rcwrt;
+				goto out;
+			}
+
+			Z_DBG(megaz->znd, "meta: %" PRIx64 " -> %" PRIx64
+			      " (table entry)", dm_s, lba);
+
+			mapped_unlock(oldest);
+			rcwrt = z_mapped_addmany(megaz, dm_s, lba, nfound);
+			if (!rcwrt && crcpg(data) == crcb4) {
+				clear_bit(IS_DIRTY, &oldest->flags);
+			}
+			mapped_lock(oldest);
+			if (rcwrt) {
+				Z_ERR(megaz->znd, "%s: Journal MANY failed.", __func__);
+				rcode = rcwrt;
+				goto out;
+			}
+
+		} else {
+			Z_ERR(megaz->znd, "%s: Out of space for metadata?", __func__);
+			rcode = -ENOSPC;
+			goto out;
+		}
+	}
+out:
+
+	mapped_unlock(oldest);
+
+	if (maddr) {
+		ZDM_FREE(megaz->znd, maddr, sizeof(*maddr), KM_18);
+	}
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int fpages(megazone_t * megaz, int allowed_pages)
+{
+	int rc = 0;
+
+	if (megaz->incore_count > allowed_pages) {
+		int count = megaz->incore_count - allowed_pages;
+		rc = expunge_oldest_pages(megaz, count);
+		if (rc) {
+			Z_ERR(megaz->znd, "%s: Failed to remove oldest pages!", __func__);
+		}
+	}
+
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int write_if_pg_blk_dirty(megazone_t * megaz,
+				 mz_crc_block_t * pblock,
+				 u64 * paddrs, u16 * crc16, int pg_no)
+{
+	int rcode = 0;
+
+	if (!pblock->crc_pg) {
+		goto out;	// nothing to write.
+	}
+
+	if (test_bit(IS_DIRTY, &pblock->flags)) {
+		u32 nblks = 1;
+		u32 nfound = 0;
+		u64 dm_s = pblock->lba;
+		u64 lba;
+
+		lba = z_acquire(megaz, Z_AQ_META, nblks, &nfound);
+		if (lba && nfound) {
+			int count = 1;
+			int use_wq = 0;
+			void *data = pblock->crc_pg;
+			u32 crcb4 = crcpg(data);
+
+			Z_DBG(megaz->znd, "write_crc_page: %d: %" PRIx64 " -> %"
+			      PRIx64, pg_no, pblock->lba, lba);
+
+			rcode = write_block(megaz->znd->ti, DM_IO_KMEM,
+					    data, lba, count, use_wq);
+			if (rcode) {
+				Z_ERR(megaz->znd, "%s: %d: %" PRIx64 " -> %" PRIx64
+				      " ERR: %d", __func__, pg_no,
+				      pblock->lba, lba, rcode);
+				goto out;
+			}
+
+			mutex_lock(&pblock->lock_pg);
+			paddrs[pg_no] = cpu_to_le64(lba);
+			crc16[pg_no] = crc_md_le16(data, Z_CRC_4K);
+			pblock->last_write = lba;
+			pblock->age = jiffies_64;
+			if ( crcpg(data) == crcb4 ) {
+				clear_bit(IS_DIRTY, &pblock->flags);
+			}
+			mutex_unlock(&pblock->lock_pg);
+
+			Z_DBG(megaz->znd, "meta: %" PRIx64 " -> %" PRIx64
+			      " (crc entry)", dm_s, lba);
+
+			rcode = z_mapped_addmany(megaz, dm_s, lba, nfound);
+			if (rcode) {
+				Z_ERR(megaz->znd, "%s: Journal MANY failed.", __func__);
+				goto out;
+			}
+
+		} else {
+			Z_ERR(megaz->znd, "%s: Out of space for metadata?", __func__);
+			rcode = -ENOSPC;
+			goto out;
+		}
+	}
+out:
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int write_purge_crc_pages(megazone_t * megaz, mz_crc_block_t * pgs,
+				 u64 * lbas, u16 * crcs)
+{
+	int err = 0;
+	int pg_no = 0;
+
+	u64 tpurge = msecs_to_jiffies(MEM_PURGE_MSECS);
+	u64 tnow = jiffies_64;
+	tpurge = (tpurge < tnow) ? tnow - tpurge : 0;
+
+	for (pg_no = 0; pg_no < 32; pg_no++) {
+		int sync_err;
+		mz_crc_block_t *pblock = &pgs[pg_no];
+
+		REF(pblock->refcount);
+		sync_err = write_if_pg_blk_dirty(megaz, pblock, lbas,
+						 crcs, pg_no);
+		DEREF(pblock->refcount);
+		if (sync_err) {
+			err = sync_err;
+		}
+
+		if ((pblock->crc_pg)
+		    && (0 != tpurge)
+		    && (0 == pblock->refcount.counter)
+		    && time_before64(pblock->age, tpurge)) {
+			if (!test_bit(DO_GC_NO_PURGE, &megaz->flags)) {
+				ZDM_FREE(megaz->znd, pblock->crc_pg, Z_C4K, PG_17);
+			}
+		}
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int sync_crc_pages(megazone_t * megaz)
+{
+	int err = 0;
+
+	int rcode;
+	rcode = write_purge_crc_pages(megaz, megaz->stm_crc,
+				      megaz->bmkeys->stm_crc_lba,
+				      megaz->bmkeys->stm_crc_pg);
+	if (rcode) {
+		err = rcode;
+	}
+	rcode = write_purge_crc_pages(megaz, megaz->rtm_crc,
+				      megaz->bmkeys->rtm_crc_lba,
+				      megaz->bmkeys->rtm_crc_pg);
+	if (rcode) {
+		err = rcode;
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static z_mapped_t *get_map_table_entry(megazone_t * megaz, u64 lba, int is_map_to)
+{
+	z_mapped_t *found = NULL;
+	z_mapped_t **table;
+	u64 entry;
+
+	if (is_map_to) {
+		table = megaz->sectortm;
+	} else {
+		table = megaz->reversetm;
+	}
+	entry = lba & 0xFFFF;
+	found = table[entry];
+
+	if (!found) {
+		/* if we didn't find one .. create it */
+		found = ZDM_ALLOC(megaz->znd, sizeof(*found), KM_20);
+		if (found) {
+			found->lba = lba;
+			found->mdata = NULL;
+
+			mutex_init(&found->md_lock);
+			table[entry] = found;
+		} else {
+			Z_ERR(megaz->znd, "NO MEM for mapped_t !!!");
+		}
+	}
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void megazone_free_one(megazone_t * megaz)
+{
+	if (megaz) {
+		zoned_t * znd = megaz->znd;
+
+		size_t mapsz = Z_BLKSZ * sizeof(z_mapped_t *);
+
+		if (megaz->sectortm) {
+			ZDM_FREE(megaz->znd, megaz->sectortm, mapsz, VM_21);
+		}
+		if (megaz->reversetm) {
+			ZDM_FREE(megaz->znd, megaz->reversetm, mapsz, VM_22);
+		}
+		if (megaz->sync_io) {
+			size_t ssz = sizeof(*megaz->sync_io);
+			ZDM_FREE(megaz->znd, megaz->sync_io, ssz, MP_SIO);
+			megaz->z_ptrs = NULL;
+			megaz->zfree_count = NULL;
+			megaz->bmkeys = NULL;
+		}
+		if (megaz->sync_cache) {
+			size_t csz = SYNC_CACHE_PAGES * sizeof(io_4k_t);
+			ZDM_FREE(megaz->znd, megaz->sync_cache, csz, MP_CACHE);
+		}
+		if (megaz->cow_block) {
+			ZDM_FREE(znd, megaz->cow_block, Z_C4K, PG_02);
+		}
+	}
+}
+
+
+static int megazone_init_one_b(megazone_t * megaz)
+{
+	int rcode = 0;
+	zoned_t * znd = megaz->znd;
+
+	megaz->sectortm = ZDM_CALLOC(znd, Z_BLKSZ, sizeof(z_mapped_t *), VM_21);
+	megaz->reversetm = ZDM_CALLOC(znd, Z_BLKSZ, sizeof(z_mapped_t *), VM_22);
+	megaz->sync_io = ZDM_ALLOC(znd, sizeof(*megaz->sync_io), MP_SIO);
+	megaz->sync_cache = ZDM_CALLOC(znd, sizeof(io_4k_t), SYNC_CACHE_PAGES, MP_CACHE);
+
+	if (!megaz->sectortm || !megaz->reversetm ||
+	    !megaz->sync_io  || !megaz->sync_cache) {
+		rcode = -ENOMEM;
+		goto out;
+	}
+
+	megaz->bmkeys = &megaz->sync_io->bmkeys;
+	megaz->z_ptrs = megaz->sync_io->z_ptrs;
+	megaz->zfree_count = megaz->sync_io->zfree;
+
+	megaz->incore_count = 0;
+	megaz->last_w = ~0ul;
+	megaz->bmkeys->sig[0] = Z_KEY_SIG;
+	megaz->bmkeys->sig[1] = cpu_to_le64(Z_KEY_SIG);
+	megaz->bmkeys->magic  = cpu_to_le64(Z_TABLE_MAGIC);
+
+	megazone_fill_lam(megaz, &megaz->logical_map);
+
+
+	if (megaz->z_count > znd->mz_provision) {
+		u64 lba = megaz->logical_map.crc_low;
+		int znr;
+
+		for (znr = 0; znr < megaz->z_count; znr++) {
+			megaz->zfree_count[znr] = Z_BLKSZ;
+		}
+
+		for (; znr < 1024; znr++) {
+			megaz->z_ptrs[znr] = 0xffffffff;
+			megaz->zfree_count[znr] = 0;
+		}
+
+		megaz->z_gc_free = megaz->z_count - 2;
+		megaz->z_current = 2;
+
+		if (0 == megaz->mega_nr && megaz->znd->preserve_z0) {
+			megaz->z_gc_free--;
+			megaz->z_current++;
+		}
+		megaz->z_data = megaz->z_current;
+		megaz->z_meta_resv = megaz->z_count - 2;
+		megaz->z_gc_resv = megaz->z_count - 1;
+		megaz->z_gc_free -= 2;
+
+		for (znr = 0; znr < 32; znr++) {
+			megaz->rtm_crc[znr].lba = lba + znr;
+			megaz->stm_crc[znr].lba = lba + znr + MZKY_NCRC;
+
+			mutex_init(&megaz->rtm_crc[znr].lock_pg);
+			mutex_init(&megaz->stm_crc[znr].lock_pg);
+		}
+	}
+
+out:
+	if (rcode) {
+		megazone_free_one(megaz);
+	}
+	return rcode;
+}
+
+
+static int megazone_init_one_a(megazone_t * megaz)
+{
+	int rcode = 0;
+
+	megaz->flags = 0;
+
+	INIT_LIST_HEAD(&megaz->jlist);
+	INIT_LIST_HEAD(&megaz->smtpool);
+
+	spin_lock_init(&megaz->map_pool_lock);
+	spin_lock_init(&megaz->jlock);
+
+	mutex_init(&megaz->zp_lock);
+	mutex_init(&megaz->discard_lock);
+	mutex_init(&megaz->mz_io_mutex);
+	INIT_WORK(&megaz->meta_work, meta_work_task);
+
+	megaz->incore_count = 0;
+	megaz->last_w = ~0ul;
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int megazone_init(zoned_t * znd)
+{
+	int rcode = 0;
+	megazone_t *megazones;
+	u32 iter;
+
+	megazones = ZDM_CALLOC(znd, znd->mega_zones_count, sizeof(*megazones), KM_25);
+	if (!megazones) {
+		Z_ERR(znd, "No memory for megazone array.");
+		rcode = -ENOMEM;
+		goto out;
+	}
+	znd->z_mega = megazones;
+
+	for (iter = 0; iter < znd->mega_zones_count; iter++) {
+		u64 remaining = znd->device_zone_count - (1024 * iter);
+		megazone_t *megaz = &megazones[iter];
+
+		megaz->mega_nr = iter;
+		megaz->znd = znd;
+		megaz->z_count = remaining < 1024 ? remaining : 1024;
+
+		rcode = megazone_init_one_a(megaz);
+		if (rcode) {
+			Z_ERR(megaz->znd, "Catastrophic FIXME cleanup"
+				" megazone init failed!!");
+			goto out;
+		}
+	}
+
+	for (iter = 0; iter < znd->mega_zones_count; iter++) {
+		megazone_t *megaz = &megazones[iter];
+
+		rcode = megazone_init_one_b(megaz);
+		if (rcode) {
+			Z_ERR(megaz->znd, "Catastrophic FIXME cleanup"
+				" megazone init failed!!");
+			goto out;
+		}
+	}
+
+out:
+	if (rcode) {
+		megazone_free_all(znd);
+	}
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int megazone_wp_sync(zoned_t * znd)
+{
+	int rcode = 0;
+/* ZAC_IN/ZAC_OUT from Hannes before turning this back on ... */
+#if 0
+	struct bdev_zone_report_result_t * report;
+	u32 rcount = 0;
+	u32 iter;
+
+	for (iter = 0; iter < znd->mega_zones_count; iter++) {
+		megazone_t *megaz = &znd->z_mega[iter];
+		int entry = (iter % 4) * 1024;
+		int z_nr;
+
+		if (0 == entry) {
+			u64 from = megaz->mega_nr * 1024;
+			int err = dmz_report_zones(znd, from, report);
+			if (err) {
+				if (err != -ENOTSUPP) {
+					rcode = err;
+				}
+				goto out;
+			}
+			rcount = be32_to_cpu(report->descriptor_count);
+		}
+
+
+		for (z_nr = 0;
+		     z_nr < megaz->z_count && entry < rcount;
+		     z_nr++, entry++) {
+			struct bdev_zone_descriptor_entry_t * dentry
+				= &report->descriptors[entry];
+			unsigned int type  = dentry->type & 0xF;
+			u32 wp = be64_to_cpu(dentry->lba_wptr) / 8;
+			u32 wp_at = megaz->z_ptrs[z_nr] & Z_WP_VALUE_MASK;
+
+//			unsigned int flags = dentry->flags;
+//			u64 start = be64_to_cpu(dentry->lba_start) / 8;
+//			u8 cond = (flags & 0xF0) >> 4;
+
+			if (type == ZTYP_CONVENTIONAL) {
+				Z_ERR(znd, "MZ# %u zone %u is conventional.",
+					megaz->mega_nr, z_nr );
+
+				megaz->z_ptrs[z_nr] |= Z_WP_NON_SEQ;
+			} else {
+				megaz->z_ptrs[z_nr] &= ~Z_WP_NON_SEQ;
+			}
+
+
+// TODO: Report and fix
+
+			if (wp >= wp_at) {
+				// indicates data loss.
+				// mark the interrum lbas as unused
+				// adjust the z_ptrs forward and free count is unchanged.
+			}
+
+		}
+	}
+
+out:
+#endif
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int _sync_page(megazone_t * megaz, z_mapped_t * mapped, int *is_dirty)
+{
+	int err = 0;
+
+	if (mapped) {
+		if (test_bit(IS_DIRTY, &mapped->flags)) {
+			BUG_ON(!mapped->mdata);
+			*is_dirty |= 1;
+		}
+		err = write_if_dirty(megaz, mapped, 0);
+		if (err) {
+			Z_ERR(megaz->znd, "%s: lut write failed", __func__);
+		}
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int _sync_mapped_pages(megazone_t * megaz)
+{
+	struct list_head *_pages = &(megaz->smtpool);
+	z_mapped_t *_oldpg;
+	z_mapped_t *t_pg;
+	z_mapped_t *oldest = NULL;
+
+	if (list_empty(_pages)) {
+		return 0;
+	}
+
+	list_for_each_entry_safe_reverse(_oldpg, t_pg, _pages, inpool) {
+		oldest = _oldpg;
+		write_if_dirty(megaz, oldest, 0);
+	}
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_sync_tables(megazone_t * megaz, int need_table_push)
+{
+	int err = 0;
+	int is_dirty;
+
+	if (test_and_clear_bit(DO_JOURNAL_MOVE, &megaz->flags) ||
+	    need_table_push) {
+		err = _journal_blocks_to_table(megaz, MAX_ZONES_PER_MZ);
+		if (err) {
+			goto out;
+		}
+	}
+
+	_sync_mapped_pages(megaz);
+	sync_crc_pages(megaz);
+
+	do {
+		int entry;
+		is_dirty = 0;
+
+		for (entry = 0; entry < Z_BLKSZ; entry++) {
+			err = _sync_page(megaz, megaz->sectortm[entry], &is_dirty);
+			if (err) {
+				goto out;
+			}
+			err = _sync_page(megaz, megaz->reversetm[entry], &is_dirty);
+			if (err) {
+				goto out;
+			}
+		}
+	} while (is_dirty);
+out:
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void mapped_free(megazone_t * megaz, z_mapped_t * mapped)
+{
+	if (mapped) {
+		mapped_lock(mapped);
+		BUG_ON(test_bit(IS_DIRTY, &mapped->flags));
+		if (mapped->mdata) {
+			ZDM_FREE(megaz->znd, mapped->mdata, Z_C4K, PG_27);
+			megaz->incore_count--;
+		}
+		mapped_unlock(mapped);
+		ZDM_FREE(megaz->znd, mapped, sizeof(*mapped), KM_20);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int release_table_pages(megazone_t * megaz)
+{
+	u64 entry;
+
+	INIT_LIST_HEAD(&megaz->smtpool);
+
+	for (entry = 0; entry < Z_BLKSZ; entry++) {
+		if (megaz->sectortm[entry]) {
+			write_if_dirty(megaz, megaz->sectortm[entry], 1);
+		}
+		if (megaz->reversetm[entry]) {
+			write_if_dirty(megaz, megaz->reversetm[entry], 1);
+		}
+	}
+
+	for (entry = 0; entry < Z_BLKSZ; entry++) {
+		mapped_free(megaz, megaz->sectortm[entry]);
+		mapped_free(megaz, megaz->reversetm[entry]);
+		megaz->sectortm[entry] = NULL;
+		megaz->reversetm[entry] = NULL;
+	}
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int release_journal_pages(megazone_t * megaz)
+{
+	struct list_head *_jhead = &(megaz->jlist);
+	z_mapcache_t *jrnl;
+	z_mapcache_t *jtmp;
+
+	if (list_empty(_jhead)) {
+		return 0;
+	}
+
+	list_for_each_entry_safe(jrnl, jtmp, _jhead, jlist) {
+		/** move all the journal entries into the SLT */
+		spin_lock(&megaz->jlock);
+		list_del(&jrnl->jlist);
+		ZDM_FREE(megaz->znd, jrnl->jdata, Z_C4K, PG_08);
+		ZDM_FREE(megaz->znd, jrnl, sizeof(*jrnl), KM_07 );
+		spin_unlock(&megaz->jlock);
+	}
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void megazone_free_all(zoned_t * znd)
+{
+	if (znd->z_mega) {
+		u32 iter;
+		for (iter = 0; iter < znd->mega_zones_count; iter++) {
+			megazone_t *megaz = &znd->z_mega[iter];
+			if (megaz) {
+				release_table_pages(megaz);
+				release_journal_pages(megaz);
+
+				megazone_free_one(megaz);
+			}
+		}
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void megazone_flush_all(zoned_t * znd)
+{
+	set_bit(ZF_FREEZE, &znd->flags);
+	if (znd->z_mega) {
+		u32 iter;
+
+		set_bit(ZF_NO_GC_DELAY, &znd->flags);
+		flush_workqueue(znd->gc_wq);
+
+		for (iter = 0; iter < znd->mega_zones_count; iter++) {
+			megazone_t *megaz = &znd->z_mega[iter];
+			if (megaz) {
+				clear_bit(DO_GC_NO_PURGE, &megaz->flags);
+				set_bit(DO_JOURNAL_MOVE, &megaz->flags);
+				set_bit(DO_MEMPOOL, &megaz->flags);
+				set_bit(DO_SYNC, &megaz->flags);
+				set_bit(DO_SYNC_CRC, &megaz->flags);
+				queue_work(znd->meta_wq, &megaz->meta_work);
+			}
+		}
+		flush_workqueue(znd->meta_wq);
+		flush_workqueue(znd->gc_wq);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void megazone_destroy(zoned_t * znd)
+{
+	set_bit(ZF_FREEZE, &znd->flags);
+	if (znd->z_mega) {
+		size_t zmsz = znd->mega_zones_count * sizeof(*znd->z_mega);
+
+		megazone_flush_all(znd);
+		megazone_free_all(znd);
+		ZDM_FREE(znd, znd->z_mega, zmsz, KM_25);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * dm_s is a logical sector that maps 1:1 to the whole disk in 4k blocks
+ * Here the logical LBA and field are calculated for the lookup table
+ * where the physical LBA can be read from disk.
+ */
+static int map_addr_calc(u64 dm_s, z_map_addr_t * out)
+{
+	u64 zone_nr = dm_s / Z_BLKSZ;
+	u64 mz_nr = zone_nr / 1024;
+
+	/* offset: blocks from base of mega zone */
+	u64 offset = dm_s - (mz_nr * Z_BLKSZ * 1024);
+
+	/* In SLT which block will hold the u32 mapping?
+	 *  -> dm_tbl_s set [0-65536) */
+	u64 dm_tbl_s = offset / 1024;
+
+	/* In that block, which index (as an array of 1024 entries?
+	 *  -> entry in the set [0-1024) */
+	u64 entry = offset - (dm_tbl_s * 1024);
+
+	/* Logically what LBA is the table block? -> lut_s */
+	u64 b_addr = dm_tbl_s + (mz_nr * Z_BLKSZ * 1024);
+
+	/* NOTE: CRC pages start at b_addr+0 */
+	/*       REVERSE map table starts at b_addr + (1 * zone size) */
+	/*       FORWARD map table starts at b_addr + (2 * zone size) */
+	b_addr += Z_BLKSZ;	/* zone 0 is not mapped */
+
+	out->dm_s = dm_s;
+	out->z_id = zone_nr;
+	out->mz_id = mz_nr;
+
+	// lookup table mapping:
+	out->mz_off = offset;	   /* (0->65536*1024] */
+	out->offentry = entry;          /* (0->1024] */
+	out->lut_r = b_addr + Z_BLKSZ;
+	out->lut_s = b_addr + (2 * Z_BLKSZ);
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/* data sector (fs layer) -> mapper sector */
+/**
+ * s_nr is a logical sector that maps 1:1 to
+ * to the subset of the disk presented as a block device to
+ * an upper layer (typically a file-system or raid).
+ *
+ * read/write to this level are mapped onto dm_s
+ */
+static int map_addr_onto_dm_zoned(zoned_t * znd, u64 s_nr, z_map_addr_t * out)
+{
+#define FIRST_MZSZ      ((0x400ul - (znd->mz_provision + 1)) << 16)
+#define FIRST_MZ_OFF    (           (znd->mz_provision + 1)  << 16)
+#define LOW_CHUNK       ((0x400ul -  znd->mz_provision )     << 16)
+#define MAP_BUMP        (            znd->mz_provision       << 16)
+#define EXPAND          ( 0x400ul                            << 16)
+
+	u64 dm_s = 0;
+	u64 s = s_nr;
+	u64 jno;
+	u64 joff;
+	u64 mz0_Size = LOW_CHUNK;
+	u64 mz0_Offset = MAP_BUMP;
+
+	if (znd->preserve_z0) {
+		mz0_Size = FIRST_MZSZ;
+		mz0_Offset = FIRST_MZ_OFF;
+	}
+	if (s < mz0_Size) {
+		dm_s = s + mz0_Offset;
+	} else {
+		s -= mz0_Size;
+		jno = s / LOW_CHUNK;
+		joff = s - (jno * LOW_CHUNK);
+		jno++;
+		dm_s = (jno * EXPAND) + joff + MAP_BUMP;
+	}
+
+	out->dm_s = dm_s;
+
+	map_addr_calc(dm_s, out);
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * ---------------------------------------------------------------------
+ *
+ *   Load a page of the sector lookup table that maps to sm->lut_s.
+ *
+ *   This sector is normally stored within the zone data sectors
+ *    co-located with the upper level data.
+ *
+ *   When no sector has been written fill a new block of the memory
+ *   pool with 0xff
+ */
+static int load_page(megazone_t * megaz, z_mapped_t * mapped, u64 lba, int is_map_to)
+{
+	u64 lba48;
+	int rcode = 0;
+
+	/**
+	 * This table entry may be on-disk, if so it needs to
+	 * be loaded.
+	 * If not it needs to be initialized to 0xFF
+	 */
+
+	mapped->maddr = ZDM_ALLOC(megaz->znd, sizeof(*mapped->maddr), KM_26);
+	if (!mapped->maddr) {
+		rcode = -ENOMEM;
+		goto out;
+	}
+
+	map_addr_calc(lba, mapped->maddr);
+	lba48 = z_lookup(megaz, mapped->maddr);  /* may be recursive to load_page */
+	if (lba48) {
+		const int count = 1;
+		const int use_wq = 1;
+		const int crce = (mapped->maddr->mz_off & 0xFFFF) % 2048;
+		int rd;
+		mz_crc_block_t *pblock;
+		u16 check;
+
+		if (mapped->last_write == lba48) {
+			Z_DBG(megaz->znd, "Page RE-LOAD %" PRIx64 " from %" PRIx64,
+				 mapped->lba, lba48);
+		}
+
+		REF(mapped->refcount);
+		mapped_lock(mapped);
+		if (warn_bad_lba(megaz, lba48)) {
+			Z_ERR(megaz->znd, "Bad PAGE %" PRIx64, lba48);
+		}
+
+		rd = read_block(megaz->znd->ti, DM_IO_KMEM, mapped->mdata,
+				lba48, count, use_wq);
+
+		check = crc_md_le16(mapped->mdata, Z_CRC_4K);
+		mapped_unlock(mapped);
+		DEREF(mapped->refcount);
+
+		if (rd) {
+			Z_ERR(megaz->znd, "%s: read_block: ERROR: %d",
+				__func__, rd);
+			rcode = -EIO;
+			goto out;
+		}
+
+		pblock = get_meta_pg_crc(megaz, mapped->maddr, is_map_to, use_wq);
+		if (!pblock) {
+			Z_ERR(megaz->znd, "%s: Out of space for metadata?",
+				__func__);
+			rcode = -ENOSPC;
+			goto out;
+		}
+
+		REF(pblock->refcount);
+		mutex_lock(&pblock->lock_pg);
+		if (pblock->crc_pg[crce] == check) {
+			mapped->last_write = lba48;
+		}
+		mutex_unlock(&pblock->lock_pg);
+		DEREF(pblock->refcount);
+
+		if (pblock->crc_pg[crce] != check) {
+			// FIXME:!!
+
+			int count;
+			u64 lba_recheck;
+			z_map_addr_t maddr;
+			map_addr_calc(lba, &maddr);
+			lba_recheck = z_lookup(megaz, &maddr);
+
+			Z_ERR(megaz->znd, "Sanity: %" PRIx64 " mapped to %" PRIx64 " vs %"
+			      PRIx64 "", lba, lba_recheck, lba48);
+
+			Z_ERR(megaz->znd, "Corrupt metadata: %" PRIx64 " from %" PRIx64
+			      " [%04x != %04x] crc lba: %" PRIx64 " flags:%lx",
+			      lba, lba48, le16_to_cpu(check),
+			      le16_to_cpu(pblock->crc_pg[crce]),
+			      pblock->lba, mapped->flags);
+
+			Z_ERR(megaz->znd, "load_page: %" PRIx64 " from lba:%" PRIx64
+			      " last written to: %" PRIx64 " - map_to? %d",
+			      mapped->lba, lba48, mapped->last_write,
+			      is_map_to);
+
+			mapped_lock(mapped);
+			for (count = 0; count < 1024; count++) {
+				Z_ERR(megaz->znd, "mapped->mdata[%d] -> %08x", count,
+				      mapped->mdata[count]);
+			}
+			mapped_unlock(mapped);
+
+			rcode = -EIO;
+			goto out;
+		}
+		rcode = 1;
+	}
+
+out:
+	if (mapped->maddr) {
+		ZDM_FREE(megaz->znd, mapped->maddr, sizeof(*mapped->maddr), KM_26 );
+	}
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int map_entry_page(megazone_t * megaz, z_mapped_t *mapped, u64 lba, int mt)
+{
+	struct list_head *mapped_pool = &(megaz->smtpool);
+	int rc = -ENOMEM;
+
+	REF(mapped->refcount);
+	mapped_lock(mapped);
+	mapped->mdata = ZDM_ALLOC(megaz->znd, Z_C4K, PG_27);
+	if (mapped->mdata) {
+		memset(mapped->mdata, 0xFF, Z_C4K);
+	}
+	mapped_unlock(mapped);
+
+	if (!mapped->mdata) {
+		Z_ERR(megaz->znd, "%s: Out of memory.", __func__);
+		DEREF(mapped->refcount);
+		mapped = NULL;
+		goto out;
+	}
+
+	rc = load_page(megaz, mapped, lba, mt);
+	if (rc < 0) {
+		Z_ERR(megaz->znd, "%s: load_page from %" PRIx64
+		      " [to? %d] error: %d", __func__, lba,
+		      mt, rc);
+		DEREF(mapped->refcount);
+		mapped = NULL;
+		goto out;
+	}
+	mapped->age = jiffies_64;
+
+	spin_lock(&megaz->map_pool_lock);
+	list_add(&mapped->inpool, mapped_pool);
+	megaz->incore_count++;
+	spin_unlock(&megaz->map_pool_lock);
+	DEREF(mapped->refcount);
+
+	Z_DBG(megaz->znd, "Page loaded: lba: %" PRIx64, lba );
+out:
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static z_mapped_t *get_map_entry(megazone_t * megaz, z_map_addr_t * maddr, int is_map_to)
+{
+	u64 lba = is_map_to ? maddr->lut_s : maddr->lut_r;
+	z_mapped_t *mapped = get_map_table_entry(megaz, lba, is_map_to);
+	if (mapped) {
+		if (!mapped->mdata) {
+			int rc = map_entry_page(megaz, mapped, lba, is_map_to);
+			if (rc < 0) {
+				megaz->meta_result = rc;
+			}
+		}
+	} else {
+		Z_ERR(megaz->znd, "%s: No table for %" PRIx64 " page# %" PRIx64 ".",
+		      __func__, maddr->dm_s, lba);
+	}
+	return mapped;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/* load a page of the (sector -> lba) sector map table into core memory */
+static z_mapped_t *sector_map_entry(megazone_t * megaz, z_map_addr_t * maddr)
+{
+	int is_map_to = 1;
+	return get_map_entry(megaz, maddr, is_map_to);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/* load a page of the (lba -> sector) reverse map table into core memory */
+static z_mapped_t *reverse_map_entry(megazone_t * megaz, z_map_addr_t * maddr)
+{
+	int is_map_to = 0;
+	return get_map_entry(megaz, maddr, is_map_to);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/* resolve a sector mapping to the on-block-device lba via the lookup table */
+static u64 locate_sector(megazone_t * megaz, z_map_addr_t * maddr)
+{
+	z_mapped_t *mapped;
+	u64 old_phy = 0;
+
+	mapped = sector_map_entry(megaz, maddr);
+	if (mapped) {
+		if (mapped->mdata) {
+			struct list_head *pool_pages = &(megaz->smtpool);
+			struct list_head *m_item = &(mapped->inpool);
+			u32 delta;
+
+			REF(mapped->refcount);
+			mapped_lock(mapped);
+			delta = mapped->mdata[maddr->offentry];
+			mapped_unlock(mapped);
+			DEREF(mapped->refcount);
+
+			old_phy = map_value(megaz, delta);
+
+			BUG_ON(old_phy >= megaz->znd->nr_blocks);
+
+			mapped->age = jiffies_64;
+
+			if (m_item->next) {
+				incore_hint(megaz, pool_pages, m_item);
+			}
+		}
+	}
+	return old_phy;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * when is_fwd is 0:
+ *  - maddr->dm_s is a sector -> lba.
+ *         in this case the old lba is discarded and scheduled for cleanup
+ *         by updating the reverse map lba tables noting that this location
+ *         is now unused.
+ * when is_fwd is 0:
+ *  - maddr->dm_s is an lba, lba -> dm_s
+ */
+
+static int update_map_entry(megazone_t * megaz, z_mapped_t * mapped,
+			    z_map_addr_t * maddr, u64 to_addr, int is_fwd)
+{
+	int err = -ENOMEM;
+	if (mapped && mapped->mdata) {
+		u64 index = maddr->offentry;
+		u32 delta;
+		u32 value;
+
+		REF(mapped->refcount);
+
+		BUG_ON(index >= 1024);
+
+		mapped_lock(mapped);
+		delta = mapped->mdata[index];
+		mapped_unlock(mapped);
+
+		err = map_encode(megaz, to_addr, &value);
+		if (!err) {
+			mapped_lock(mapped);
+			/*
+			 * if the value is modified update the table and
+			 * place it on top of the active [inpool] list
+			 * this will keep the chunk of lookup table in
+			 * memory.
+			 */
+			if (mapped->mdata[index] != value) {
+				struct list_head *pool_pages =
+				    &(megaz->smtpool);
+				struct list_head *m_item = &(mapped->inpool);
+
+				mapped->mdata[index] = value;
+				mapped->age = jiffies_64;
+				set_bit(IS_DIRTY, &mapped->flags);
+				err = 1;
+
+				if (m_item->next) {
+					incore_hint(megaz, pool_pages, m_item);
+				}
+
+			} else {
+				Z_ERR(megaz->znd, "*ERR* mdata[index] (%x) == (%x)",
+					mapped->mdata[index], value );
+				dump_stack();
+			}
+			mapped_unlock(mapped);
+		} else {
+			Z_ERR(megaz->znd, "*ERR* Mapping: %" PRIx64 " to %" PRIx64,
+				to_addr, maddr->dm_s );
+
+		}
+
+		if (1 == err && is_fwd && (delta != MZTEV_UNUSED)) {
+			u64 old_phy = map_value(megaz, delta);
+			/* add to discard list of the controlling mzone
+			 * for the 'delta' physical block
+			 * unlikly, but they may be different megazones
+			 */
+			Z_DBG(megaz->znd, "%s: unused_phy: %" PRIu64
+			      " (new lba: %" PRIu64 ")",
+			      __func__, old_phy, to_addr);
+
+			BUG_ON(old_phy >= megaz->znd->nr_blocks);
+
+			err = unused_phy(megaz, old_phy, 0);
+			if (err) {
+				err = -ENOSPC;
+			}
+		}
+		
+		DEREF(mapped->refcount);
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ *  Copy a journal block into the sector map [SLT] for this megazone.
+ */
+static int move_to_map_tables(megazone_t * megaz, z_mapcache_t * jrnl)
+{
+	z_mapped_t *smtbl = NULL;
+	z_mapped_t *rmtbl = NULL;
+	z_map_addr_t *maddr;
+	z_map_addr_t *reverse;
+
+	u64 lut_s = ~0ul;
+	u64 lut_r = ~0ul;
+	int jentry;
+	int err = 0;
+	int is_fwd = 1;
+
+	/* the journal being move must remain stable so sorting
+	 * is disabled. If a sort is desired due to an unsorted
+	 * page the search devolves to a linear lookup.
+	 */
+	jrnl->no_sort_flag = 1;
+
+	maddr = ZDM_ALLOC(megaz->znd, sizeof(*maddr), KM_28);
+	reverse = ZDM_ALLOC(megaz->znd, sizeof(*reverse), KM_28);
+	if (!maddr || !reverse) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	for (jentry = jrnl->jcount; jentry > 0;) {
+		u64 dm_s = le64_to_lba48(jrnl->jdata[jentry].logical, NULL);
+		u64 lba = le64_to_lba48(jrnl->jdata[jentry].physical, NULL);
+
+		if (dm_s == Z_LOWER48 || lba == Z_LOWER48) {
+			jrnl->jcount = --jentry;
+			continue;
+		}
+
+		if (dm_s < 0x40000) {
+			Z_DBG(megaz->znd, "%s: -> entry %d -> dm_s:%" PRIx64
+				 " lba:%" PRIx64,
+				 __func__, jentry, dm_s, lba);
+		}
+
+		map_addr_calc(dm_s, maddr);
+		map_addr_calc(lba, reverse);
+
+		if (lut_s != maddr->lut_s) {
+			if (smtbl) {
+				DEREF(smtbl->refcount);
+			}
+			smtbl = sector_map_entry(megaz, maddr);
+			if (!smtbl) {
+				err = -ENOMEM;
+				goto out;
+			}
+			REF(smtbl->refcount);
+			lut_s = smtbl->lba;
+		}
+		if (lut_r != reverse->lut_r) {
+			if (rmtbl) {
+				DEREF(rmtbl->refcount);
+			}
+			rmtbl = reverse_map_entry(megaz, reverse);
+			if (!rmtbl) {
+				err = -ENOMEM;
+				goto out;
+			}
+			REF(rmtbl->refcount);
+			lut_r = rmtbl->lba;
+		}
+#if EXTRA_DEBUG
+		Z_DBG(megaz->znd, "%s: -> smtbl pg# %d [%" PRIu64 "] "
+			 "dm_s:%" PRIu64 " lba:%" PRIu64 " [lut_s:%" PRIu64 "] "
+			 "range [%" PRIu64 ", %" PRIu64 ")",
+			 __func__,
+			 smtbl->s_id,
+			 smtbl->lba, dm_s, lba, maddr->lut_s, low, high);
+#endif /* EXTRA_DEBUG */
+
+		is_fwd = 1;
+		err = update_map_entry(megaz, smtbl, maddr, lba, is_fwd);
+		if (err < 0) {
+			goto out;
+		}
+		is_fwd = 0;
+		err = update_map_entry(megaz, rmtbl, reverse, dm_s, is_fwd);
+		if (err == 1) {
+			err = 0;
+		}
+		if (err < 0) {
+			goto out;
+		}
+		jrnl->jdata[jentry].logical = ~0ul;
+		jrnl->jdata[jentry].physical = ~0ul;
+		jrnl->jcount = --jentry;
+	}
+out:
+	if (smtbl) {
+		DEREF(smtbl->refcount);
+	}
+	if (rmtbl) {
+		DEREF(rmtbl->refcount);
+	}
+	set_bit(DO_MEMPOOL, &megaz->flags);
+
+	if (maddr) {
+		ZDM_FREE(megaz->znd, maddr, sizeof(*maddr), KM_28);
+	}
+	if (reverse) {
+		ZDM_FREE(megaz->znd, reverse, sizeof(*reverse), KM_28);
+	}
+	jrnl->no_sort_flag = 0;
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * Add an unused block to the list of blocks to be
+ *  discarded during garbage collection.
+ */
+static int unused_phy(megazone_t * megaz, u64 lba, u64 orig_s)
+{
+	int err = 0;
+	z_mapped_t *mapped;
+	z_map_addr_t *reverse;
+	int z_off;
+
+	/* Any reason to *NOT* immediately stuff UNUSED into
+	   the reverse table entry? */
+
+	mutex_lock(&megaz->discard_lock);
+
+	reverse = ZDM_ALLOC(megaz->znd, sizeof(*reverse), KM_29);
+	if (!reverse) {
+		err =  -ENOMEM;
+		Z_ERR(megaz->znd, "unused_phy: Alloc failed");
+		goto out;
+	}
+
+	map_addr_calc(lba, reverse);
+	z_off = reverse->z_id % 1024;
+
+	mapped = reverse_map_entry(megaz, reverse);
+	if (!mapped) {
+		err = -EIO;
+		Z_ERR(megaz->znd, "unused_phy: Reverse Map Entry not found.");
+		goto out;
+	}
+
+	BUG_ON( !mapped->mdata );
+
+	REF(mapped->refcount);
+	mapped_lock(mapped);
+
+#if EXTRA_DEBUG
+	Z_DBG(megaz->znd, "%s: -> mapped %" PRIx64 "[%" PRIu64 "] lba:%" PRIx64
+		 " (S:%x)  [lut_r:%" PRIx64 "]",
+		 __func__, mapped->lba, reverse->offentry, lba,
+		 mapped->mdata[reverse->offentry], reverse->lut_r);
+#endif /* EXTRA_DEBUG */
+
+	/* if the value is modified update the table and
+	 * place it on top of the active [inpool] list */
+	if (mapped->mdata[reverse->offentry] != MZTEV_UNUSED) {
+		struct list_head *_pool = &(megaz->smtpool);
+		struct list_head *m_item = &(mapped->inpool);
+
+		if (orig_s) {
+			u32 enc = mapped->mdata[reverse->offentry];
+			u64 dm_s = map_value(megaz, enc);
+			int drop_discard = 0;
+
+			if ( _calc_zone(dm_s) < 4 ) {
+				drop_discard = 1;
+				Z_ERR(megaz->znd, "Discard invalid target %"
+				      PRIx64" - Is ZDM Meta %"PRIx64" vs %"
+				      PRIx64, lba, orig_s, dm_s );
+			}
+			if ( orig_s != dm_s ) {
+				drop_discard = 1;
+				Z_ERR(megaz->znd, "Discard %"PRIx64" mismatched "
+					"sources: %"PRIx64" vs %"PRIx64"",
+					lba, orig_s, dm_s );
+			}
+			if (drop_discard) {
+				goto out_unlock;
+			}
+		}
+
+		mapped->mdata[reverse->offentry] = MZTEV_UNUSED;
+		mapped->age = jiffies_64;
+		set_bit(IS_DIRTY, &mapped->flags);
+
+		if (m_item->next) {
+			incore_hint(megaz, _pool, m_item);
+		}
+		if (megaz->z_ptrs[z_off] && megaz->zfree_count[z_off] < Z_BLKSZ) {
+			megaz->zfree_count[z_off]++;
+		}
+
+		Z_DBG(megaz->znd, "Unused: updated %" PRIx64 " {zone:%" PRIu64
+			 " [-> MZ# %" PRIu64 " Z# %d]} " "free:%x wp:%x",
+			 lba, reverse->z_id, reverse->mz_id, z_off,
+			 megaz->zfree_count[z_off], megaz->z_ptrs[z_off]);
+	} else {
+		Z_DBG(megaz->znd, "lba: %" PRIx64 " alread reported as free?",
+		      lba);
+	}
+
+out_unlock:
+	mapped_unlock(mapped);
+	DEREF(mapped->refcount);
+
+out:
+	if (reverse) {
+		ZDM_FREE(megaz->znd, reverse, sizeof(*reverse), KM_29 );
+	}
+	mutex_unlock(&megaz->discard_lock);
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * Add an unused block to the list of blocks to be
+ *  discarded during garbage collection.
+ */
+static int unused_addr(megazone_t * megaz, u64 dm_s)
+{
+	int err = 0;
+	z_mapped_t *mapped;
+	z_map_addr_t *maddr;
+
+	/* Any reason to *NOT* immediately stuff UNUSED into
+	   the maddr table entry? */
+
+	maddr = ZDM_ALLOC(megaz->znd, sizeof(*maddr), KM_30);
+	if (!maddr) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	map_addr_calc(dm_s, maddr);
+	mapped = sector_map_entry(megaz, maddr);
+	if (!mapped) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	REF(mapped->refcount);
+	mapped_lock(mapped);
+
+	/* if the value is modified update the table and
+	 * place it on top of the active [inpool] list */
+	if (mapped->mdata[maddr->offentry] != MZTEV_UNUSED) {
+		struct list_head *_pool = &(megaz->smtpool);
+		struct list_head *m_item = &(mapped->inpool);
+
+		mapped->mdata[maddr->offentry] = MZTEV_UNUSED;
+		mapped->age = jiffies_64;
+		set_bit(IS_DIRTY, &mapped->flags);
+
+		if (m_item->next) {
+			incore_hint(megaz, _pool, m_item);
+		}
+	}
+	mapped_unlock(mapped);
+	DEREF(mapped->refcount);
+
+out:
+	if (maddr) {
+		ZDM_FREE(megaz->znd, maddr, sizeof(*maddr), KM_30);
+	}
+
+	return err;
+}
diff --git a/drivers/scsi/sd.c b/drivers/scsi/sd.c
index a20da8c..3bc5efe 100644
--- a/drivers/scsi/sd.c
+++ b/drivers/scsi/sd.c
@@ -7,20 +7,20 @@
  *              Subsequent revisions: Eric Youngdale
  *	Modification history:
  *       - Drew Eckhardt <drew@colorado.edu> original
- *       - Eric Youngdale <eric@andante.org> add scatter-gather, multiple 
+ *       - Eric Youngdale <eric@andante.org> add scatter-gather, multiple
  *         outstanding request, and other enhancements.
  *         Support loadable low-level scsi drivers.
- *       - Jirka Hanika <geo@ff.cuni.cz> support more scsi disks using 
+ *       - Jirka Hanika <geo@ff.cuni.cz> support more scsi disks using
  *         eight major numbers.
  *       - Richard Gooch <rgooch@atnf.csiro.au> support devfs.
- *	 - Torben Mathiasen <tmm@image.dk> Resource allocation fixes in 
+ *	 - Torben Mathiasen <tmm@image.dk> Resource allocation fixes in
  *	   sd_init and cleanups.
  *	 - Alex Davis <letmein@erols.com> Fix problem where partition info
- *	   not being read in sd_open. Fix problem where removable media 
+ *	   not being read in sd_open. Fix problem where removable media
  *	   could be ejected after sd_open.
  *	 - Douglas Gilbert <dgilbert@interlog.com> cleanup for lk 2.5.x
- *	 - Badari Pulavarty <pbadari@us.ibm.com>, Matthew Wilcox 
- *	   <willy@debian.org>, Kurt Garloff <garloff@suse.de>: 
+ *	 - Badari Pulavarty <pbadari@us.ibm.com>, Matthew Wilcox
+ *	   <willy@debian.org>, Kurt Garloff <garloff@suse.de>:
  *	   Support 32k/1M disks.
  *
  *	Logging policy (needs CONFIG_SCSI_LOGGING defined):
@@ -29,7 +29,7 @@
  *	 - entering sd_ioctl: SCSI_LOG_IOCTL level 1
  *	 - entering other commands: SCSI_LOG_HLQUEUE level 3
  *	Note: when the logging level is set by the user, it must be greater
- *	than the level indicated above to trigger output.	
+ *	than the level indicated above to trigger output.
  */
 
 #include <linux/module.h>
@@ -45,6 +45,7 @@
 #include <linux/init.h>
 #include <linux/blkdev.h>
 #include <linux/blkpg.h>
+#include <linux/blk-zoned-ctrl.h>
 #include <linux/delay.h>
 #include <linux/mutex.h>
 #include <linux/string_helpers.h>
@@ -537,16 +538,16 @@ static struct kobject *sd_default_probe(dev_t devt, int *partno, void *data)
 
 /*
  * Device no to disk mapping:
- * 
+ *
  *       major         disc2     disc  p1
  *   |............|.............|....|....| <- dev_t
  *    31        20 19          8 7  4 3  0
- * 
+ *
  * Inside a major, we have 16k disks, however mapped non-
  * contiguously. The first 16 disks are for major0, the next
- * ones with major1, ... Disk 256 is for major0 again, disk 272 
- * for major1, ... 
- * As we stay compatible with our numbering scheme, we can reuse 
+ * ones with major1, ... Disk 256 is for major0 again, disk 272
+ * for major1, ...
+ * As we stay compatible with our numbering scheme, we can reuse
  * the well-know SCSI majors 8, 65--71, 136--143.
  */
 static int sd_major(int major_idx)
@@ -920,7 +921,7 @@ static int sd_setup_read_write_cmnd(struct scsi_cmnd *SCpnt)
 
 	if (sdp->changed) {
 		/*
-		 * quietly refuse to do anything to a changed disc until 
+		 * quietly refuse to do anything to a changed disc until
 		 * the changed bit has been reset
 		 */
 		/* printk("SCSI disk has been changed or is not present. Prohibiting further I/O.\n"); */
@@ -1151,7 +1152,7 @@ static void sd_uninit_command(struct scsi_cmnd *SCpnt)
  *	@inode: only i_rdev member may be used
  *	@filp: only f_mode and f_flags may be used
  *
- *	Returns 0 if successful. Returns a negated errno value in case 
+ *	Returns 0 if successful. Returns a negated errno value in case
  *	of error.
  *
  *	Note: This can be called from a user context (e.g. fsck(1) )
@@ -1219,7 +1220,7 @@ static int sd_open(struct block_device *bdev, fmode_t mode)
 
 error_out:
 	scsi_disk_put(sdkp);
-	return retval;	
+	return retval;
 }
 
 /**
@@ -1228,7 +1229,7 @@ error_out:
  *	@inode: only i_rdev member may be used
  *	@filp: only f_mode and f_flags may be used
  *
- *	Returns 0. 
+ *	Returns 0.
  *
  *	Note: may block (uninterruptible) if error recovery is underway
  *	on this disk.
@@ -1266,7 +1267,7 @@ static int sd_getgeo(struct block_device *bdev, struct hd_geometry *geo)
         diskinfo[0] = 0x40;	/* 1 << 6 */
        	diskinfo[1] = 0x20;	/* 1 << 5 */
        	diskinfo[2] = sdkp->capacity >> 11;
-	
+
 	/* override with calculated, extended default, or driver values */
 	if (host->hostt->bios_param)
 		host->hostt->bios_param(sdp, bdev, sdkp->capacity, diskinfo);
@@ -1301,7 +1302,7 @@ static int sd_ioctl(struct block_device *bdev, fmode_t mode,
 	struct scsi_device *sdp = sdkp->device;
 	void __user *p = (void __user *)arg;
 	int error;
-    
+
 	SCSI_LOG_IOCTL(1, sd_printk(KERN_INFO, sdkp, "sd_ioctl: disk=%s, "
 				    "cmd=0x%x\n", disk->disk_name, cmd));
 
@@ -1330,6 +1331,24 @@ static int sd_ioctl(struct block_device *bdev, fmode_t mode,
 		case SCSI_IOCTL_GET_BUS_NUMBER:
 			error = scsi_ioctl(sdp, cmd, p);
 			break;
+		case SCSI_IOCTL_INQUIRY:
+			error = _inquiry_ioctl(disk, p);
+			break;
+		case SCSI_IOCTL_CLOSE_ZONE:
+			error = _zone_close_ioctl(disk, arg);
+			break;
+		case SCSI_IOCTL_FINISH_ZONE:
+			error = _zone_finish_ioctl(disk, arg);
+			break;
+		case SCSI_IOCTL_OPEN_ZONE:
+			error = _zone_open_ioctl(disk, arg);
+			break;
+		case SCSI_IOCTL_RESET_WP:
+			error = _reset_wp_ioctl(disk, arg);
+			break;
+		case SCSI_IOCTL_REPORT_ZONES:
+			error = _report_zones_ioctl(disk, p);
+			break;
 		default:
 			error = scsi_cmd_blk_ioctl(bdev, mode, cmd, p);
 			if (error != -ENOTTY)
@@ -1511,9 +1530,9 @@ static void sd_rescan(struct device *dev)
 
 
 #ifdef CONFIG_COMPAT
-/* 
- * This gets directly called from VFS. When the ioctl 
- * is not recognized we go back to the other translation paths. 
+/*
+ * This gets directly called from VFS. When the ioctl
+ * is not recognized we go back to the other translation paths.
  */
 static int sd_compat_ioctl(struct block_device *bdev, fmode_t mode,
 			   unsigned int cmd, unsigned long arg)
@@ -1525,12 +1544,12 @@ static int sd_compat_ioctl(struct block_device *bdev, fmode_t mode,
 			(mode & FMODE_NDELAY) != 0);
 	if (error)
 		return error;
-	       
-	/* 
+
+	/*
 	 * Let the static ioctl translation table take care of it.
 	 */
 	if (!sdev->host->hostt->compat_ioctl)
-		return -ENOIOCTLCMD; 
+		return -ENOIOCTLCMD;
 	return sdev->host->hostt->compat_ioctl(sdev, cmd, (void __user *)arg);
 }
 #endif
@@ -1777,7 +1796,7 @@ sd_spinup_disk(struct scsi_disk *sdkp)
 			if (the_result)
 				sense_valid = scsi_sense_valid(&sshdr);
 			retries++;
-		} while (retries < 3 && 
+		} while (retries < 3 &&
 			 (!scsi_status_is_good(the_result) ||
 			  ((driver_byte(the_result) & DRIVER_SENSE) &&
 			  sense_valid && sshdr.sense_key == UNIT_ATTENTION)));
@@ -1850,7 +1869,7 @@ sd_spinup_disk(struct scsi_disk *sdkp)
 			}
 			break;
 		}
-				
+
 	} while (spintime && time_before_eq(jiffies, spintime_expire));
 
 	if (spintime) {
@@ -2911,13 +2930,13 @@ static void sd_probe_async(void *data, async_cookie_t cookie)
  *	for each scsi device (not just disks) present.
  *	@dev: pointer to device object
  *
- *	Returns 0 if successful (or not interested in this scsi device 
+ *	Returns 0 if successful (or not interested in this scsi device
  *	(e.g. scanner)); 1 when there is an error.
  *
  *	Note: this function is invoked from the scsi mid-level.
- *	This function sets up the mapping between a given 
- *	<host,channel,id,lun> (found in sdp) and new device name 
- *	(e.g. /dev/sda). More precisely it is the block device major 
+ *	This function sets up the mapping between a given
+ *	<host,channel,id,lun> (found in sdp) and new device name
+ *	(e.g. /dev/sda). More precisely it is the block device major
  *	and minor number that is chosen here.
  *
  *	Assume sd_probe is not re-entrant (for time being)
@@ -3063,7 +3082,7 @@ static void scsi_disk_release(struct device *dev)
 {
 	struct scsi_disk *sdkp = to_scsi_disk(dev);
 	struct gendisk *disk = sdkp->disk;
-	
+
 	spin_lock(&sd_index_lock);
 	ida_remove(&sd_index_ida, sdkp->index);
 	spin_unlock(&sd_index_lock);
diff --git a/include/linux/ata.h b/include/linux/ata.h
old mode 100644
new mode 100755
index d2992bf..c4d3002
--- a/include/linux/ata.h
+++ b/include/linux/ata.h
@@ -305,6 +305,17 @@ enum {
 	/* marked obsolete in the ATA/ATAPI-7 spec */
 	ATA_CMD_RESTORE		= 0x10,
 
+	/* ZAC commands - need update when ZAC spec is available */
+	ATA_CMD_ZONE_MAN_OUT = 0x9F,
+
+	ATA_SUBCMD_CLOSE_ZONES = 0x01,
+	ATA_SUBCMD_FINISH_ZONES = 0x02,
+	ATA_SUBCMD_OPEN_ZONES = 0x03,
+	ATA_SUBCMD_RESET_WP = 0x04,
+
+	ATA_CMD_ZONE_MAN_IN = 0x4A,
+	ATA_SUBCMD_REP_ZONES = 0x00,
+
 	/* Subcmds for ATA_CMD_FPDMA_SEND */
 	ATA_SUBCMD_FPDMA_SEND_DSM            = 0x00,
 	ATA_SUBCMD_FPDMA_SEND_WR_LOG_DMA_EXT = 0x02,
@@ -899,6 +910,13 @@ static inline bool ata_drive_40wire_relaxed(const u16 *dev_id)
 	return true;
 }
 
+static inline bool ata_drive_zac_ha(const u16 *dev_id)
+{
+	if ((dev_id[69] & 0x0003) == 0x0001)
+		return true;
+	return false;
+}
+
 static inline int atapi_cdb_len(const u16 *dev_id)
 {
 	u16 tmp = dev_id[ATA_ID_CONFIG] & 0x3;
diff --git a/include/linux/blk-zoned-ctrl.h b/include/linux/blk-zoned-ctrl.h
new file mode 100644
index 0000000..9215db1
--- /dev/null
+++ b/include/linux/blk-zoned-ctrl.h
@@ -0,0 +1,222 @@
+/*
+ * Functions for zone based SMR devices.
+ *
+ * Copyright (C) 2015 Seagate Technology PLC
+ *
+ * Written by:
+ * Shaun Tancheff <shaun.tancheff@seagate.com>
+ *
+ * This file is licensed under  the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#ifndef BLK_ZONED_H
+#define BLK_ZONED_H
+
+enum zone_report_option {
+	ZOPT_NON_SEQ_AND_RESET   = 0x00,
+	ZOPT_ZC1_EMPTY,
+	ZOPT_ZC2_OPEN_IMPLICIT,
+	ZOPT_ZC3_OPEN_EXPLICIT,
+	ZOPT_ZC4_CLOSED,
+	ZOPT_ZC5_FULL,
+	ZOPT_ZC6_READ_ONLY,
+	ZOPT_ZC7_OFFLINE,
+	ZOPT_RESET               = 0x10,
+	ZOPT_NON_SEQ             = 0x11,
+	ZOPT_NON_WP_ZONES        = 0x3f,
+
+	ZOPT_USE_ATA_PASS        = 0x80,
+
+};
+
+/* Report, close, finish, open, reset wp: */
+enum zone_zm_action {
+	REPORT_ZONES_EXT   = 0x00,
+	CLOSE_ZONE_EXT,
+	FINISH_ZONE_EXT,
+	OPEN_ZONE_EXT,
+	RESET_WP_EXT,
+};
+
+struct bdev_zone_report_request_t {
+	__u64 zone_locator_lba;	  /* starting lba for first zone to be reported. */
+	__u32 return_page_count;  /* number of bytes allocated for result */
+	__u8  report_option;	  /* see: zone_report_option enum */
+};
+
+enum bdev_zone_type {
+	ZTYP_RESERVED            = 0,
+	ZTYP_CONVENTIONAL        = 1,
+	ZTYP_SEQ_WRITE_REQUIRED  = 2,
+	ZTYP_SEQ_WRITE_PREFERRED = 3,
+};
+
+enum bdev_zone_condition {
+	ZCOND_CONVENTIONAL       = 0, /* no write pointer */
+	ZCOND_ZC1_EMPTY          = 1,
+	ZCOND_ZC2_OPEN_IMPLICIT  = 2,
+	ZCOND_ZC3_OPEN_EXPLICIT  = 3,
+	ZCOND_ZC4_CLOSED         = 4,
+	/* 5 - 0xC - reserved */
+	ZCOND_ZC6_READ_ONLY      = 0xd,
+	ZCOND_ZC5_FULL           = 0xe,
+	ZCOND_ZC7_OFFLINE        = 0xf,
+};
+
+/* NOTE: all LBA's are u64 only use the lower 48 bits */
+
+struct bdev_zone_descriptor_entry_t {
+	u8  type;         /* see zone_type enum */
+	u8  flags;        /* 0:reset, 1:non-seq, 2-3: resv,
+                           * bits 4-7: see zone_condition enum */
+	u8  reserved1[6];
+	u64 length;       /* length of zone: in sectors */
+	u64 lba_start;    /* lba of zone start */
+	u64 lba_wptr;     /* lba of write pointer - ready to be written
+			   * next */
+        u8 reserved[32];
+} __packed;
+
+enum bdev_zone_same {
+	ZS_ALL_DIFFERENT        = 0,
+	ZS_ALL_SAME             = 1,
+	ZS_LAST_DIFFERS         = 2,
+	ZS_SAME_LEN_DIFF_TYPES  = 3,
+};
+
+struct bdev_zone_report_result_t {
+	u32 descriptor_count;   /* number of zone_descriptor entries that 
+				 * follow */
+	u8  same_field;         /* bits 0-3: enum zone_same (MASK: 0x0F) */
+	u8  reserved1[3];
+	u64 maximum_lba;        /* The MAXIMUM LBA field indicates the 
+				 * LBA of the last logical sector on the
+				 * device, including all logical sectors
+				 * in all zones. */
+	u8  reserved2[48];
+	struct bdev_zone_descriptor_entry_t descriptors[0];
+} __packed;
+
+struct bdev_zone_report_ioctl_t {
+	union {
+		struct bdev_zone_report_request_t in;
+		struct bdev_zone_report_result_t out;
+	} data;
+} __packed;
+
+
+/**
+ * According to the test result,
+ *  SEAGATE driver do not support this option,
+ *  need renew this definition in future
+ */
+typedef enum zc_report_options {
+	ZC_RO_RESET = 0x00,
+	ZC_RO_OFFLINE = 0x01,
+	ZC_RO_RDONLY = 0x02,
+	ZC_RO_FULL = 0x03,
+	ZC_RO_OP_NOT_READY = 0x4,
+	ZC_RO_ALL = 0xF,
+} zc_report_options_t;
+
+/**
+ * Flags to determine if the connected disk is ZONED:
+ *   - Host Aware of Host Managed (or not)
+ */
+typedef enum zc_type {
+	NOT_ZONED    = 0x00,
+	HOST_AWARE   = 0x01,
+	HOST_MANAGE  = 0x02,
+} zc_type_t;
+
+typedef enum zc_vendor_type {
+	ZONE_DEV_ATA_SEAGATE = 0x00,
+	ZONE_DEV_BLK         = 0x01,
+} zc_vendor_type_t;
+
+struct zoned_inquiry {
+	u8  evpd;
+	u8  pg_op;
+	u16 mx_resp_len;
+	u8  result[0];
+} __packed;
+typedef struct zoned_inquiry zoned_inquiry_t;
+
+/* ata passthrough variant */
+struct zoned_identity {
+	u8 type_id;
+	u8 reserved[3];
+} __packed;
+typedef struct zoned_identity zoned_identity_t;
+
+/*0xC6: Seagate SMR aware band configuration log*/
+typedef enum zc_zone_type {
+	ZC_TYPE_CONVENTIONAL  = 0x00,
+	ZC_TYPE_HOST_ASSISTED = 0x01,	/* sequential write preferred */
+	ZC_TYPE_HOST_MANAGED  = 0x02,	/* sequential write required  */
+} zc_zone_type_t;
+
+/*0xC6: Seagate SMR aware band configuration log*/
+typedef enum zc_zone_condition {
+	ZC_CON_RESET        = 0x00,
+	ZC_CON_OFFLINE      = 0x01,
+	ZC_CON_RDONLY       = 0x02,
+	ZC_CON_FULL         = 0x03,
+	ZC_CON_OP_NOT_READY = 0x4,
+} zc_zone_condition_t;
+
+/* zone descriptor format, device controller reports this infomation
+ * REPORT ZONES */
+typedef struct zc_zone {
+	zc_zone_type_t      zone_type;
+	zc_zone_condition_t zone_condition;
+	u64 zone_id;		/* Optional */
+	u64 zone_length;
+	u64 zone_start_lba;
+	u64 zone_write_pointer;
+} zc_zone_t;
+
+
+/* this is basically scsi_execute */
+int blk_cmd_execute(struct request_queue *queue,
+			   const unsigned char *cmd,
+			   int data_direction,
+			   void *buffer,
+			   unsigned bufflen,
+			   unsigned char *sense,
+			   int timeout,
+			   int retries,
+			   u64 flags,
+			   int *resid);
+int blk_cmd_with_sense(struct gendisk *disk,
+	u8 * cmd, int data_direction,
+	u8 * buf, u32 buf_len, u8 * sense_buffer);
+
+/* scsi zbc commands */
+int blk_zoned_report(struct gendisk *, u64, u8, u8 *, size_t);
+int blk_zoned_inquiry(struct gendisk *, u8, u8, u16, u8 *);
+int blk_zoned_close(struct gendisk *disk, u64 start_lba);
+int blk_zoned_finish(struct gendisk *disk, u64 start_lba);
+int blk_zoned_open(struct gendisk *disk, u64 start_lba);
+int blk_zoned_reset_wp(struct gendisk *disk, u64 start_lba);
+
+/* ata zac variants */
+int blk_zoned_report_ata(struct gendisk *disk, u64, u8, u8 *, size_t);
+int blk_zoned_identify_ata(struct gendisk *disk, zoned_identity_t *);
+int blk_zoned_close_ata(struct gendisk *disk, u64 start_lba);
+int blk_zoned_finish_ata(struct gendisk *disk, u64 start_lba);
+int blk_zoned_open_ata(struct gendisk *disk, u64 start_lba);
+int blk_zoned_reset_wp_ata(struct gendisk *disk, u64 start_lba);
+
+/* for testing from userspace via ioctl */
+int _inquiry_ioctl(struct gendisk *disk, void __user *parg);
+int _zone_close_ioctl(struct gendisk *disk, unsigned long arg);
+int _zone_finish_ioctl(struct gendisk *disk, unsigned long arg);
+int _zone_open_ioctl(struct gendisk *disk, unsigned long arg);
+int _reset_wp_ioctl(struct gendisk *disk, unsigned long arg);
+int _report_zones_ioctl(struct gendisk *disk, void __user *parg);
+
+
+#endif /* INT_BLK_ZONED_H */
diff --git a/include/scsi/scsi.h b/include/scsi/scsi.h
old mode 100644
new mode 100755
index e0a3398..31edc23
--- a/include/scsi/scsi.h
+++ b/include/scsi/scsi.h
@@ -201,6 +201,10 @@ static inline int scsi_is_wlun(u64 lun)
 #define DRIVER_HARD         0x07
 #define DRIVER_SENSE	    0x08
 
+/* Op code for ZBC */
+#define RESET_WP        0x94
+#define REPORT_ZONES    0x95
+
 /*
  * Internal return values.
  */
@@ -300,6 +304,14 @@ static inline int scsi_is_wlun(u64 lun)
 /* Used to obtain the PCI location of a device */
 #define SCSI_IOCTL_GET_PCI		0x5387
 
+/* Used for Zone based SMR devices */
+#define SCSI_IOCTL_INQUIRY		0x10000
+#define SCSI_IOCTL_CLOSE_ZONE		0x10001
+#define SCSI_IOCTL_FINISH_ZONE		0x10002
+#define SCSI_IOCTL_OPEN_ZONE		0x10003
+#define SCSI_IOCTL_RESET_WP		0x10004
+#define SCSI_IOCTL_REPORT_ZONES		0x10005
+
 /* Pull a u32 out of a SCSI message (using BE SCSI conventions) */
 static inline __u32 scsi_to_u32(__u8 *ptr)
 {
-- 
1.9.1

