From 47deaf76b6070bf3991e93ac253b10a2dc3e5d93 Mon Sep 17 00:00:00 2001
From: Shaun Tancheff <shaun@tancheff.com>
Date: Mon, 7 Mar 2016 12:38:44 -0600
Subject: [PATCH 16/29] Resolve SMRFFSEXT-231 and 214 ready for testing.

---
 drivers/md/dm-zoned.c |  38 +++-
 drivers/md/dm-zoned.h |   5 +-
 drivers/md/libzoned.c | 504 +++++++++++++++++++++++++++-----------------------
 3 files changed, 307 insertions(+), 240 deletions(-)

diff --git a/drivers/md/dm-zoned.c b/drivers/md/dm-zoned.c
index 3391b0b..76f5ce8 100644
--- a/drivers/md/dm-zoned.c
+++ b/drivers/md/dm-zoned.c
@@ -954,7 +954,7 @@ static int block_io(struct zoned *znd,
 
 /**
  * read_block() - Issue sync read maybe using using a worker thread.
- * @it: Device Mapper Target Instance
+ * @ti: Device Mapper Target Instance
  * @dtype: Type of memory in data
  * @data: Data for I/O
  * @lba: bLBA for I/O [4k resolution]
@@ -988,8 +988,8 @@ static int read_block(struct dm_target *ti, enum dm_io_mem_type dtype,
 }
 
 /**
- * write_block() - Issue sync write maybe using using a worker thread.
- * @it: Device Mapper Target Instance
+ * writef_block() - Issue sync write maybe using using a worker thread.
+ * @ti: Device Mapper Target Instance
  * @dtype: Type of memory in data
  * @data: Data for I/O
  * @lba: bLBA for I/O [4k resolution]
@@ -998,8 +998,8 @@ static int read_block(struct dm_target *ti, enum dm_io_mem_type dtype,
  *
  * Return 0 on success, otherwise error.
  */
-static int write_block(struct dm_target *ti, enum dm_io_mem_type dtype,
-		       void *data, u64 lba, unsigned int count, int queue)
+static int writef_block(struct dm_target *ti, int rw, enum dm_io_mem_type dtype,
+			void *data, u64 lba, unsigned int count, int queue)
 {
 	struct zoned *znd = ti->private;
 	sector_t block = lba << Z_SHFT4K;
@@ -1017,6 +1017,23 @@ static int write_block(struct dm_target *ti, enum dm_io_mem_type dtype,
 }
 
 /**
+ * write_block() - Issue sync write maybe using using a worker thread.
+ * @ti: Device Mapper Target Instance
+ * @dtype: Type of memory in data
+ * @data: Data for I/O
+ * @lba: bLBA for I/O [4k resolution]
+ * @count: Number of 4k blocks to read/write.
+ * @queue: if true then use worker thread for I/O and wait.
+ *
+ * Return 0 on success, otherwise error.
+ */
+static int write_block(struct dm_target *ti, enum dm_io_mem_type dtype,
+		       void *data, u64 lba, unsigned int count, int queue)
+{
+	return writef_block(ti, WRITE, dtype, data, lba, count, queue);
+}
+
+/**
  * struct zsplit_hook - Extra data attached to a hooked bio
  * @znd: ZDM Instance to update on BIO completion.
  * @endio: BIO's original bi_end_io handler
@@ -1777,6 +1794,11 @@ static int zoned_bio(struct zoned *znd, struct bio *bio)
 	}
 
 	/* check for SYNC flag */
+	if (bio->bi_rw & REQ_FLUSH) {
+		bio->bi_rw &= ~(REQ_FLUSH);
+		set_bit(DO_FLUSH, &znd->flags);
+		force_sync_now = 1;
+	}
 	if (bio->bi_rw & REQ_SYNC) {
 		set_bit(DO_SYNC, &znd->flags);
 		force_sync_now = 1;
@@ -1804,7 +1826,8 @@ static int zoned_bio(struct zoned *znd, struct bio *bio)
 		znd->age = jiffies;
 	}
 
-	if (test_bit(DO_SYNC, &znd->flags) ||
+	if (test_bit(DO_FLUSH, &znd->flags) ||
+	    test_bit(DO_SYNC, &znd->flags) ||
 	    test_bit(DO_JOURNAL_MOVE, &znd->flags) ||
 	    test_bit(DO_MEMPOOL, &znd->flags)) {
 		if (!test_bit(DO_METAWORK_QD, &znd->flags) &&
@@ -1878,6 +1901,9 @@ static void on_timeout_activity(struct zoned *znd, int delay)
 		if (count != 1 || --max_tries < 0)
 			break;
 
+		if (test_bit(ZF_FREEZE, &znd->flags))
+			return;
+
 	} while (is_expired_msecs(znd->age, DISCARD_IDLE_MSECS));
 
 	gc_queue_with_delay(znd, delay, NORMAL);
diff --git a/drivers/md/dm-zoned.h b/drivers/md/dm-zoned.h
index 07ae98a..4f0247b 100644
--- a/drivers/md/dm-zoned.h
+++ b/drivers/md/dm-zoned.h
@@ -181,6 +181,7 @@ enum znd_flags_enum {
 	DO_JOURNAL_MOVE,
 	DO_MEMPOOL,
 	DO_SYNC,
+	DO_FLUSH,
 	DO_JOURNAL_LOAD,
 	DO_GC_NO_PURGE,
 	DO_METAWORK_QD,
@@ -434,7 +435,9 @@ struct mz_superkey {
 	__le16 md_crc;
 	__le16 wp_crc[64];
 	__le16 zf_crc[64];
-	u8 reserved[1912];
+	__le16 discards;
+	__le16 maps;
+	u8 reserved[1908];
 	__le32 crc32;
 	__le64 generation;
 	__le64 magic;
diff --git a/drivers/md/libzoned.c b/drivers/md/libzoned.c
index 8c2bb19..66deba5 100644
--- a/drivers/md/libzoned.c
+++ b/drivers/md/libzoned.c
@@ -15,7 +15,7 @@
 #define BUILD_NO		104
 
 #define EXTRA_DEBUG		0
-#define ENABLE_PG_FREE_VIA_LAZY	0
+#define ENABLE_PG_FREE_VIA_LAZY	1
 
 #define MZ_MEMPOOL_SZ		512
 #define READ_AHEAD		16   /* number of LUT entries to read-ahead */
@@ -89,7 +89,8 @@ static struct io_4k_block *get_io_vcache(struct zoned *znd, gfp_t gfp);
 static int put_io_vcache(struct zoned *znd, struct io_4k_block *cache);
 static struct map_pg *get_map_entry(struct zoned *, u64 lba, gfp_t gfp);
 static void put_map_entry(struct map_pg *);
-static int cache_pg(struct zoned *znd, struct map_pg *pg, int, struct mpinfo *);
+static int cache_pg(struct zoned *znd, struct map_pg *pg, gfp_t gfp,
+		    struct mpinfo *mpi);
 static int move_to_map_tables(struct zoned *znd, struct map_cache *mcache);
 static void update_stale_ratio(struct zoned *znd, u32 zone);
 static int zoned_create_disk(struct dm_target *ti, struct zoned *znd);
@@ -101,7 +102,8 @@ static u64 z_lookup_table(struct zoned *znd, u64 addr, gfp_t gfp);
 static u64 current_mapping(struct zoned *znd, u64 addr, gfp_t gfp);
 static int z_mapped_add_one(struct zoned *znd, u64 dm_s, u64 lba, gfp_t gfp);
 static int z_mapped_discard(struct zoned *znd, u64 tlba, u64 count, gfp_t gfp);
-static int z_mapped_addmany(struct zoned *znd, u64 dm_s, u64 lba, u64, gfp_t gfp);
+static int z_mapped_addmany(struct zoned *znd, u64 dm_s, u64 lba, u64,
+			    gfp_t gfp);
 static int z_to_map_list(struct zoned *znd, u64 dm_s, u64 lba, gfp_t gfp);
 static int z_to_discard_list(struct zoned *znd, u64 dm_s, u64 blks, gfp_t gfp);
 static int discard_merge(struct zoned *znd, u64 tlba, u64 blks);
@@ -115,6 +117,8 @@ static int read_block(struct dm_target *, enum dm_io_mem_type,
 		      void *, u64, unsigned int, int);
 static int write_block(struct dm_target *, enum dm_io_mem_type,
 		       void *, u64, unsigned int, int);
+static int writef_block(struct dm_target *ti, int rw, enum dm_io_mem_type dtype,
+			void *data, u64 lba, unsigned int count, int queue);
 static int zoned_init_disk(struct dm_target *ti, struct zoned *znd,
 			   int create, int force);
 
@@ -714,36 +718,36 @@ void zdm_alloc_debug(struct zoned *znd, void *p, size_t sz, int id)
  * This (ugly) unified scheme helps to find leaks and monitor usage
  *   via ioctl tools.
  */
-static void *zdm_alloc(struct zoned *znd, size_t sz, int code, int gfp)
+static void *zdm_alloc(struct zoned *znd, size_t sz, int code, gfp_t gfp)
 {
 	void *pmem = NULL;
 	int id    = code & 0x00FFFF;
 	int flag  = code & 0xFF0000;
-	int gfp_flags;
+	gfp_t gfp_mask;
 
 #if USE_KTHREAD
-	gfp_flags = GFP_KERNEL;
+	gfp_mask = GFP_KERNEL;
 #else
-	gfp_flags = gfp ? GFP_ATOMIC : GFP_KERNEL;
+	gfp_mask = gfp ? GFP_ATOMIC : GFP_KERNEL;
 #endif
 
 	if (flag == GET_VM)
 		might_sleep();
 
-	if (gfp_flags != GFP_ATOMIC)
+	if (gfp_mask != GFP_ATOMIC)
 		might_sleep();
 
 	switch (flag) {
 	case GET_ZPG:
-		pmem = (void *)get_zeroed_page(gfp_flags);
-		if (!pmem && gfp_flags == GFP_ATOMIC) {
+		pmem = (void *)get_zeroed_page(gfp_mask);
+		if (!pmem && gfp_mask == GFP_ATOMIC) {
 			Z_ERR(znd, "No atomic for %d, try noio.", id);
 			pmem = (void *)get_zeroed_page(GFP_NOIO);
 		}
 		break;
 	case GET_KM:
-		pmem = kzalloc(sz, gfp_flags);
-		if (!pmem && gfp_flags == GFP_ATOMIC) {
+		pmem = kzalloc(sz, gfp_mask);
+		if (!pmem && gfp_mask == GFP_ATOMIC) {
 			Z_ERR(znd, "No atomic for %d, try noio.", id);
 			pmem = kzalloc(sz, GFP_NOIO);
 		}
@@ -773,15 +777,15 @@ static void *zdm_alloc(struct zoned *znd, size_t sz, int code, int gfp)
  * @znd: ZDM instance
  * @n: number of elements in array.
  * @sz: allocation size of each element.
- * @code: allocation strategy (VM, KM, PAGE, N-PAGES).
- * @gfp: kernel allocation flags.
+ * @c: allocation strategy (VM, KM, PAGE, N-PAGES).
+ * @q: kernel allocation flags.
  *
  * calloc is just an zeroed memory array alloc.
  * all zdm_alloc schemes are for zeroed memory so no extra memset needed.
  */
-static void *zdm_calloc(struct zoned *znd, size_t n, size_t sz, int code, int q)
+static void *zdm_calloc(struct zoned *znd, size_t n, size_t sz, int c, gfp_t q)
 {
-	return zdm_alloc(znd, sz * n, code, q);
+	return zdm_alloc(znd, sz * n, c, q);
 }
 
 /**
@@ -791,7 +795,7 @@ static void *zdm_calloc(struct zoned *znd, size_t n, size_t sz, int code, int q)
  *
  * Return: Pointer to pool memory or NULL.
  */
-static struct io_4k_block *get_io_vcache(struct zoned *znd, int gfp)
+static struct io_4k_block *get_io_vcache(struct zoned *znd, gfp_t gfp)
 {
 	struct io_4k_block *cache = NULL;
 	int avail;
@@ -815,7 +819,7 @@ static struct io_4k_block *get_io_vcache(struct zoned *znd, int gfp)
 /**
  * put_io_vcache() - Get a pre-allocated pool of memory for IO.
  * @znd: ZDM instance
- * @gfp: Allocation flags if no pre-allocated pool can be found.
+ * @cache: Allocated cache entry.
  *
  * Return: Pointer to pool memory or NULL.
  */
@@ -891,14 +895,13 @@ static int release_memcache(struct zoned *znd)
 		if (list_empty(head))
 			return 0;
 
+		SpinLock(&znd->mclck[no]);
 		list_for_each_entry_safe(mcache, _mc, head, mclist) {
-			/** move all the journal entries into the SLT */
-			SpinLock(&znd->mclck[no]);
 			list_del(&mcache->mclist);
 			ZDM_FREE(znd, mcache->jdata, Z_C4K, PG_08);
 			ZDM_FREE(znd, mcache, sizeof(*mcache), KM_07);
-			spin_unlock(&znd->mclck[no]);
 		}
+		spin_unlock(&znd->mclck[no]);
 
 	}
 	return 0;
@@ -970,7 +973,7 @@ static void mapped_free(struct zoned *znd, struct map_pg *mapped)
 static int flush_map(struct zoned *znd, struct map_pg **map, u32 count)
 {
 	const int use_wq = 1;
-	const int sync = 0;
+	const int sync = 1;
 	u32 ii;
 	int err = 0;
 
@@ -1005,15 +1008,22 @@ static int zoned_io_flush(struct zoned *znd)
 	set_bit(DO_JOURNAL_MOVE, &znd->flags);
 	set_bit(DO_MEMPOOL, &znd->flags);
 	set_bit(DO_SYNC, &znd->flags);
+	set_bit(DO_FLUSH, &znd->flags);
 	queue_work(znd->meta_wq, &znd->meta_work);
 	flush_workqueue(znd->meta_wq);
+	flush_workqueue(znd->bg_wq);
 
 	mod_delayed_work(znd->gc_wq, &znd->gc_work, 0);
 	flush_delayed_work(&znd->gc_work);
 	atomic_dec(&znd->gc_throttle);
 
-	INIT_LIST_HEAD(&znd->zltpool);
+	spin_lock(&znd->lzy_lck);
 	INIT_LIST_HEAD(&znd->lzy_pool);
+	spin_unlock(&znd->lzy_lck);
+
+	spin_lock(&znd->zlt_lck);
+	INIT_LIST_HEAD(&znd->zltpool);
+	spin_unlock(&znd->zlt_lck);
 
 	err = flush_map(znd, znd->fwd_tm, znd->map_count);
 	if (err)
@@ -1032,6 +1042,7 @@ static int zoned_io_flush(struct zoned *znd)
 		goto out;
 
 	set_bit(DO_SYNC, &znd->flags);
+	set_bit(DO_FLUSH, &znd->flags);
 	queue_work(znd->meta_wq, &znd->meta_work);
 	flush_workqueue(znd->meta_wq);
 
@@ -1114,6 +1125,7 @@ static void zoned_destroy(struct zoned *znd)
 
 	release_table_pages(znd);
 	release_memcache(znd);
+
 	if (znd->dev) {
 		dm_put_device(znd->ti, znd->dev);
 		znd->dev = NULL;
@@ -1991,7 +2003,7 @@ static void increment_used_blks(struct zoned *znd, u64 lba, u32 blks)
  *
  * Return: Disk LBA or 0 if not found.
  */
-static u64 _current_mapping(struct zoned *znd, int nodisc, u64 addr, int gfp)
+static u64 _current_mapping(struct zoned *znd, int nodisc, u64 addr, gfp_t gfp)
 {
 	u64 found = 0ul;
 
@@ -2025,7 +2037,7 @@ out:
  *
  * Return: Disk LBA or 0 if not found.
  */
-static u64 current_mapping(struct zoned *znd, u64 addr, int gfp)
+static u64 current_mapping(struct zoned *znd, u64 addr, gfp_t gfp)
 {
 	const int nodisc = 0;
 
@@ -2041,7 +2053,7 @@ static u64 current_mapping(struct zoned *znd, u64 addr, int gfp)
  *
  * Add a new extent entry.
  */
-static int z_mapped_add_one(struct zoned *znd, u64 dm_s, u64 lba, int gfp)
+static int z_mapped_add_one(struct zoned *znd, u64 dm_s, u64 lba, gfp_t gfp)
 {
 	int err = 0;
 
@@ -2094,7 +2106,7 @@ static int z_mapped_add_one(struct zoned *znd, u64 dm_s, u64 lba, int gfp)
  *
  * Add a new extent entry.
  */
-static int z_discard_small(struct zoned *znd, u64 tlba, u64 count, int gfp)
+static int z_discard_small(struct zoned *znd, u64 tlba, u64 count, gfp_t gfp)
 {
 	const int nodisc = 1;
 	int err = 0;
@@ -2122,7 +2134,7 @@ static int z_discard_small(struct zoned *znd, u64 tlba, u64 count, int gfp)
  *
  * Add a new extent entry.
  */
-static int z_discard_large(struct zoned *znd, u64 tlba, u64 blks, int gfp)
+static int z_discard_large(struct zoned *znd, u64 tlba, u64 blks, gfp_t gfp)
 {
 	int rcode = 0;
 
@@ -2147,14 +2159,14 @@ static int z_discard_large(struct zoned *znd, u64 tlba, u64 blks, int gfp)
  * Otherwise add a new extent entry.
  */
 static
-int mapped_discard(struct zoned *znd, u64 tlba, u64 blks, int merge, int gfp)
+int mapped_discard(struct zoned *znd, u64 tlba, u64 blks, int merge, gfp_t gfp)
 {
 	int err = 0;
 
 	if (merge && discard_merge(znd, tlba, blks))
 		goto done;
 
-	if (blks < DISCARD_MAX_INGRESS) {
+	if (blks < DISCARD_MAX_INGRESS || znd->dc_entries > 39) {
 		/* io_mutex is avoid adding map cache entries during SYNC */
 		MutexLock(&znd->mz_io_mutex);
 		err = z_discard_small(znd, tlba, blks, gfp);
@@ -2175,7 +2187,7 @@ done:
  * @blks: number of blocks being discard.
  * @gfp: Current memory allocation scheme.
  */
-static int z_mapped_discard(struct zoned *znd, u64 tlba, u64 blks, int gfp)
+static int z_mapped_discard(struct zoned *znd, u64 tlba, u64 blks, gfp_t gfp)
 {
 	const int merge = 1;
 
@@ -2187,7 +2199,7 @@ static int z_mapped_discard(struct zoned *znd, u64 tlba, u64 blks, int gfp)
  * @znd: ZDM Instance
  * @gfp: Current memory allocation scheme.
  */
-static struct map_cache *mcache_alloc(struct zoned *znd, int gfp)
+static struct map_cache *mcache_alloc(struct zoned *znd, gfp_t gfp)
 {
 	struct map_cache *mcache;
 
@@ -2482,7 +2494,7 @@ void mc_delete_entry(struct map_cache *mcache, int entry)
  * for writing. Ideally the punch only requires the extent to be shrunk.
  */
 static int _discard_split_entry(struct zoned *znd, struct map_cache *mcache,
-				int at, u64 addr, int gfp)
+				int at, u64 addr, gfp_t gfp)
 {
 	const int merge = 0;
 	int err = 0;
@@ -2591,7 +2603,7 @@ out:
  * The overall purpose of the discard pool is to reduce the amount of intake
  * on the memcache to avoid starving the I/O requests.
  */
-static u64 z_discard_range(struct zoned *znd, u64 addr, int gfp)
+static u64 z_discard_range(struct zoned *znd, u64 addr, gfp_t gfp)
 {
 	struct map_cache *mcache;
 	u64 found = 0ul;
@@ -2643,7 +2655,7 @@ out:
  * The overall purpose of the discard pool is to reduce the amount of intake
  * on the memcache to avoid starving the I/O requests.
  */
-static int z_discard_partial(struct zoned *znd, u32 minblks, int gfp)
+static int z_discard_partial(struct zoned *znd, u32 minblks, gfp_t gfp)
 {
 	struct map_cache *mcache;
 	int completions = 0;
@@ -2893,7 +2905,7 @@ static int _cached_to_tables(struct zoned *znd, u32 zone)
  *
  * Return: 0 on success or -errno value
  */
-static int z_flush_bdev(struct zoned *znd, int gfp)
+static int z_flush_bdev(struct zoned *znd, gfp_t gfp)
 {
 	int err;
 	sector_t bi_done;
@@ -2929,16 +2941,17 @@ static __always_inline void pg_delete(struct zoned *znd, struct map_pg *expg)
 		zlt = is_fwd ? znd->fwd_crc : znd->rev_crc;
 		lock = &znd->ct_lock;
 	}
-	spin_lock(lock);
+
+	spin_lock_nested(lock, SINGLE_DEPTH_NESTING);
 	if (index > -1 && index < count && expg == zlt[index] &&
 	    test_and_clear_bit(IS_DROPPED, &expg->flags)) {
-		zlt[index] = NULL;
+		int release = 0;
 
+		zlt[index] = NULL;
 		MutexLock(&expg->md_lock);
 		if (test_and_clear_bit(IS_LAZY, &expg->flags)) {
 			list_del(&expg->lazy);
 			clear_bit(DELAY_ADD, &expg->flags);
-
 			if (expg->data.addr) {
 				void *pg = expg->data.addr;
 
@@ -2946,11 +2959,13 @@ static __always_inline void pg_delete(struct zoned *znd, struct map_pg *expg)
 				ZDM_FREE(znd, pg, Z_C4K, PG_27);
 				znd->incore_count--;
 			}
-			ZDM_FREE(znd, expg, sizeof(*expg), KM_20);
+			release = 1;
 		} else {
 			Z_ERR(znd, "Detected double list del.");
 		}
 		mutex_unlock(&expg->md_lock);
+		if (release)
+			ZDM_FREE(znd, expg, sizeof(*expg), KM_20);
 	}
 	spin_unlock(lock);
 }
@@ -2995,15 +3010,16 @@ void manage_lazy_activity(struct zoned *znd)
 			spin_unlock(&znd->zlt_lck);
 #if ENABLE_PG_FREE_VIA_LAZY
 		} else {
-			u64 expired = expg->age + msecs_to_jiffies(5000);
-
 			/*
 			 * Delete page
 			 */
 			if (!test_bit(IN_ZLT, &expg->flags) &&
-			    test_bit(IS_DROPPED, &expg->flags) &&
-			    time_after64(jiffies_64, expired)) {
-				pg_delete(znd, expg);
+			    test_bit(IS_DROPPED, &expg->flags)) {
+				u32 msecs = MEM_PURGE_MSECS;
+
+				if (is_expired_msecs(expg->age, msecs)) {
+					pg_delete(znd, expg);
+				}
 			}
 #endif
 		}
@@ -3039,10 +3055,12 @@ static int do_sync_metadata(struct zoned *znd, int sync, int drop)
 		goto out;
 	}
 
-	err = z_flush_bdev(znd, GFP_KERNEL);
-	if (err) {
-		Z_ERR(znd, "Uh oh. flush_bdev failed. -> %d", err);
-		goto out;
+	if (test_and_clear_bit(DO_FLUSH, &znd->flags)) {
+		err = z_flush_bdev(znd, GFP_KERNEL);
+		if (err) {
+			Z_ERR(znd, "Uh oh. flush_bdev failed. -> %d", err);
+			goto out;
+		}
 	}
 out:
 	return err;
@@ -3090,8 +3108,12 @@ static int do_journal_to_table(struct zoned *znd)
 static int do_sync_to_disk(struct zoned *znd)
 {
 	int err = 0;
-	int sync = test_and_clear_bit(DO_SYNC, &znd->flags);
 	int drop = 0;
+	int sync = 0;
+
+	if (test_and_clear_bit(DO_SYNC, &znd->flags) ||
+	    test_bit(DO_FLUSH, &znd->flags))
+		sync = 1;
 
 	if (test_and_clear_bit(DO_MEMPOOL, &znd->flags)) {
 		int pool_size = MZ_MEMPOOL_SZ * 4;
@@ -3129,7 +3151,8 @@ static void meta_work_task(struct work_struct *work)
 
 	err = do_init_from_journal(znd);
 
-	if (test_bit(DO_JOURNAL_MOVE, &znd->flags) &&
+	if (test_bit(DO_JOURNAL_MOVE, &znd->flags) ||
+	    test_bit(DO_FLUSH, &znd->flags) ||
 	    test_bit(DO_SYNC, &znd->flags)) {
 		MutexLock(&znd->mz_io_mutex);
 		locked = 1;
@@ -3169,7 +3192,7 @@ static void meta_work_task(struct work_struct *work)
  * the old block being stale.
  */
 static int _update_entry(struct zoned *znd, struct map_cache *mcache, int at,
-			 u64 dm_s, u64 lba, int gfp)
+			 u64 dm_s, u64 lba, gfp_t gfp)
 {
 	struct map_sect_to_lba *data;
 	u64 lba_was;
@@ -3225,7 +3248,7 @@ static int _update_disc(struct zoned *znd, struct map_cache *mcache, int at,
  * @gfp: Allocation (kmalloc) flags
  */
 static int mc_update(struct zoned *znd, struct map_cache *mcache, int at,
-		     u64 dm_s, u64 lba, int type, int gfp)
+		     u64 dm_s, u64 lba, int type, gfp_t gfp)
 {
 	int rcode;
 
@@ -3245,7 +3268,7 @@ static int mc_update(struct zoned *znd, struct map_cache *mcache, int at,
  * @type: List (type) to be adding to (MAP or DISCARD)
  * @gfp: Allocation (kmalloc) flags
  */
-static int _mapped_list(struct zoned *znd, u64 dm_s, u64 lba, int type, int gfp)
+static int _mapped_list(struct zoned *znd, u64 dm_s, u64 lba, int type, gfp_t gfp)
 {
 	struct map_cache *mcache = NULL;
 	struct map_cache *mc_add = NULL;
@@ -3343,7 +3366,7 @@ out:
  * @blks: Number of blocks in the extent.
  * @gfp: Allocation (kmalloc) flags
  */
-static int z_to_discard_list(struct zoned *znd, u64 dm_s, u64 blks, int gfp)
+static int z_to_discard_list(struct zoned *znd, u64 dm_s, u64 blks, gfp_t gfp)
 {
 	return _mapped_list(znd, dm_s, blks, IS_DISCARD, gfp);
 }
@@ -3355,7 +3378,7 @@ static int z_to_discard_list(struct zoned *znd, u64 dm_s, u64 blks, int gfp)
  * @lba: bLBA mapping to
  * @gfp: Allocation (kmalloc) flags
  */
-static int z_to_map_list(struct zoned *znd, u64 dm_s, u64 lba, int gfp)
+static int z_to_map_list(struct zoned *znd, u64 dm_s, u64 lba, gfp_t gfp)
 {
 	return _mapped_list(znd, dm_s, lba, IS_MAP, gfp);
 }
@@ -3492,6 +3515,8 @@ static int z_mapped_sync(struct zoned *znd)
 	int nblks = 1;
 	int use_wq = 0;
 	int rc = 1;
+	int discards = 0;
+	int maps = 0;
 	int jwrote = 0;
 	int cached = 0;
 	int idx = 0;
@@ -3511,36 +3536,34 @@ static int z_mapped_sync(struct zoned *znd)
 		goto out;
 	}
 
-
-/* TIME TO DO SOME WORK!! */
-
-/* dirty wp blocks (zone pointers) [allow for 64*4 blocks] */
-/* md_crcs-> 0x2148/9 */
-
-	lba = 0x2048;
+	lba = 0x2048ul;
 	for (idx = 0; idx < znd->gz_count; idx++) {
 		struct meta_pg *wpg = &znd->wp[idx];
 
 		if (test_bit(IS_DIRTY, &wpg->flags)) {
-			znd->bmkeys->wp_crc[idx] = crc_md_le16(wpg->wp_alloc,
-				Z_CRC_4K);
-			znd->bmkeys->zf_crc[idx] = crc_md_le16(wpg->zf_est,
-				Z_CRC_4K);
-			if (test_bit(IS_DIRTY, &wpg->flags)) {
-				memcpy(io_vcache[0].data, wpg->wp_alloc, Z_C4K);
-				memcpy(io_vcache[1].data, wpg->zf_est, Z_C4K);
-
-				rc = write_block(ti, DM_IO_VMA, io_vcache, lba,
-					cached, use_wq);
-				if (rc)
-					goto out;
-			}
+			cached = 0;
+			memcpy(io_vcache[cached].data, wpg->wp_alloc, Z_C4K);
+			znd->bmkeys->wp_crc[idx] =
+				crc_md_le16(io_vcache[cached].data, Z_CRC_4K);
+			cached++;
+			memcpy(io_vcache[cached].data, wpg->zf_est, Z_C4K);
+			znd->bmkeys->zf_crc[idx] =
+				crc_md_le16(io_vcache[cached].data, Z_CRC_4K);
+			cached++;
+			rc = write_block(ti, DM_IO_VMA, io_vcache, lba,
+				cached, use_wq);
+			if (rc)
+				goto out;
 			clear_bit(IS_DIRTY, &wpg->flags);
+
+                        Z_DBG(znd, "%d# -- WP: %04x | ZF: %04x",
+			      idx, znd->bmkeys->wp_crc[idx],
+				   znd->bmkeys->zf_crc[idx]);
 		}
 		lba += 2;
 	}
 
-	lba += (generation % modulo) * incr;
+	lba = (generation % modulo) * incr;
 	if (lba == 0)
 		lba++;
 
@@ -3550,8 +3573,6 @@ static int z_mapped_sync(struct zoned *znd)
 
 	idx = 0;
 	for (no = 0; no < MAP_COUNT; no++) {
-		int incr_w = 0;
-
 		mcache = mcache_first_get(znd, no);
 		while (mcache) {
 			u64 phy = le64_to_lba48(mcache->jdata[0].bval, NULL);
@@ -3566,7 +3587,11 @@ static int z_mapped_sync(struct zoned *znd)
 				memcpy(io_vcache[cached].data,
 					mcache->jdata, Z_C4K);
 				cached++;
-				incr_w++;
+
+				if (no == IS_DISCARD)
+					discards++;
+				else
+					maps++;
 			}
 
 			if (cached == IO_VCACHE_PAGES) {
@@ -3586,28 +3611,36 @@ static int z_mapped_sync(struct zoned *znd)
 			}
 			mcache = mcache_put_get_next(znd, mcache, no);
 		}
-		if (incr_w > 40)
-			Z_ERR(znd, "**WARNING** large map cache %d [%d]",
-				incr_w, no);
 	}
 	jwrote += cached;
+	if (discards > 40)
+		Z_ERR(znd, "**WARNING** large discard cache %d", discards);
+	if (maps > 40)
+		Z_ERR(znd, "**WARNING** large map cache %d", maps);
 
 	znd->bmkeys->md_crc = crc_md_le16(znd->md_crcs, 2 * Z_CRC_4K);
 	znd->bmkeys->n_crcs = cpu_to_le16(jwrote);
+	znd->bmkeys->discards = cpu_to_le16(discards);
+	znd->bmkeys->maps = cpu_to_le16(maps);
 	znd->bmkeys->crc32 = 0;
 	znd->bmkeys->crc32 = cpu_to_le32(crc32c(~0u, znd->bmkeys, Z_CRC_4K));
 	if (cached < (IO_VCACHE_PAGES - 3)) {
 		memcpy(io_vcache[cached].data, znd->bmkeys, Z_C4K);
 		cached++;
 		memcpy(io_vcache[cached].data, znd->md_crcs, Z_C4K * 2);
-		cached++;
+		cached += 2;
 		need_sync_io = 0;
 	}
 
 	do {
 		if (cached > 0) {
-			rc = write_block(ti, DM_IO_VMA, io_vcache, lba, cached,
-					 use_wq);
+			int rw = WRITE;
+
+			if (!need_sync_io &&
+			    test_and_clear_bit(DO_FLUSH, &znd->flags))
+				rw |= REQ_FLUSH;
+			rc = writef_block(ti, rw, DM_IO_VMA, io_vcache, lba,
+					  cached, use_wq);
 			if (rc) {
 				Z_ERR(znd, "%s: mcache-> %" PRIu64
 				      " [%d blks] %p -> %d",
@@ -3621,6 +3654,8 @@ static int z_mapped_sync(struct zoned *znd)
 		if (need_sync_io) {
 			memcpy(io_vcache[cached].data, znd->bmkeys, Z_C4K);
 			cached++;
+			memcpy(io_vcache[cached].data, znd->md_crcs, Z_C4K * 2);
+			cached += 2;
 			need_sync_io = 0;
 		}
 	} while (cached > 0);
@@ -3668,7 +3703,7 @@ void zoned_personality(struct zoned *znd, struct zdm_superblock *sblock)
 }
 
 /**
- * find_superblock() - Find superblock following lba
+ * find_superblock_at() - Find superblock following lba
  * @znd: ZDM instance
  * @lba: Lba to start scanning for superblock.
  * @use_wq: If a work queue is needed to scanning.
@@ -3736,13 +3771,18 @@ out:
 static int find_superblock(struct zoned *znd, int use_wq, int do_init)
 {
 	int found = 0;
-	u64 lba;
+	int iter = 0;
+	u64 lba = LBA_SB_START;
+	u64 last = MAX_CACHE_INCR * CACHE_COPIES;
 
-	for (lba = LBA_SB_START; lba < 0x800; lba += MAX_CACHE_INCR) {
+	do {
 		found = find_superblock_at(znd, lba, use_wq, do_init);
 		if (found)
 			break;
-	}
+		iter++;
+		lba = MAX_CACHE_INCR * iter;
+	} while (lba < last);
+
 	return found;
 }
 
@@ -3784,11 +3824,6 @@ static u64 mcache_find_gen(struct zoned *znd, u64 lba, int use_wq, u64 *sb_lba)
 				*sb_lba = lba;
 			goto out;
 		}
-		if (data[0] == 0 && data[1] == 0) {
-			/* No SB here... */
-			Z_DBG(znd, "FGen: Invalid block %" PRIx64 "?", lba);
-			goto out;
-		}
 		lba++;
 		count++;
 		if (count > MAX_CACHE_SYNC) {
@@ -3815,8 +3850,8 @@ static inline int cmp_gen(u64 left, u64 right)
 	if (left != right) {
 		u64 delta = (left > right) ? left - right : right - left;
 
-		result = -1;
-		if (delta > 1) {
+		result = (left > right) ? -1 : 1;
+		if (delta > 0xFFFFFFFF) {
 			if (left == BAD_ADDR)
 				result = 1;
 		} else {
@@ -3854,7 +3889,7 @@ u64 mcache_greatest_gen(struct zoned *znd, int use_wq, u64 *sb, u64 *at_lba)
 		gen_no[idx] = mcache_find_gen(znd, lba, use_wq, pAt);
 		if (gen_no[idx])
 			pick = idx;
-		lba += incr;
+		lba = idx * incr;
 	}
 
 	for (idx = 0; idx < locations; idx++) {
@@ -3899,6 +3934,55 @@ static u64 count_stale_blocks(struct zoned *znd, u32 gzno, struct meta_pg *wpg)
 	return stale;
 }
 
+static int do_load_cache(struct zoned *znd, int type, u64 lba, int idx, int wq)
+{
+	u16 count;
+	__le16 crc;
+	int rc = 0;
+	int blks = 1;
+	struct map_cache *mcache = mcache_alloc(znd, NORMAL);
+
+	if (!mcache) {
+		rc = -ENOMEM;
+		goto out;
+	}
+
+	rc = read_block(znd->ti, DM_IO_KMEM, mcache->jdata, lba, blks, wq);
+	if (rc) {
+		Z_ERR(znd, "%s: mcache-> %" PRIu64
+			   " [%d blks] %p -> %d",
+		      __func__, lba, blks, mcache->jdata, rc);
+
+		ZDM_FREE(znd, mcache->jdata, Z_C4K, PG_08);
+		ZDM_FREE(znd, mcache, sizeof(*mcache), KM_07);
+		goto out;
+	}
+	crc = crc_md_le16(mcache->jdata, Z_CRC_4K);
+	if (crc != znd->bmkeys->crcs[idx]) {
+		rc = -EIO;
+		Z_ERR(znd, "%s: bad crc %" PRIu64,  __func__, lba);
+		goto out;
+	}
+	(void)le64_to_lba48(mcache->jdata[0].bval, &count);
+	mcache->jcount = count;
+	mclist_add(znd, mcache, type);
+
+out:
+	return rc;
+}
+
+static int do_load_map_cache(struct zoned *znd, u64 lba, int idx, int wq)
+{
+	return do_load_cache(znd, IS_MAP, lba, idx, wq);
+}
+
+static int do_load_discard_cache(struct zoned *znd, u64 lba, int idx, int wq)
+{
+	return do_load_cache(znd, IS_DISCARD, lba, idx, wq);
+}
+
+
+
 /**
  * z_mapped_init() - Re-Load an existing ZDM instance from the block device.
  * @znd: ZDM instance
@@ -3911,10 +3995,9 @@ static int z_mapped_init(struct zoned *znd)
 	int nblks = 1;
 	int wq = 0;
 	int rc = 1;
-	int done = 0;
-	int jfound = 0;
 	int idx = 0;
-	struct list_head hjload;
+	int jcount = 0;
+	u64 sblba = 0;
 	u64 lba = 0;
 	u64 generation;
 	__le32 crc_chk;
@@ -3929,25 +4012,7 @@ static int z_mapped_init(struct zoned *znd)
 		goto out;
 	}
 
-	/*
-	 * Read write printers
-	 */
-	lba = 0x2048;
-	for (idx = 0; idx < znd->gz_count; idx++) {
-		struct meta_pg *wpg = &znd->wp[idx];
-
-		rc = read_block(ti, DM_IO_KMEM, wpg->wp_alloc, lba, 1, wq);
-		if (rc)
-			goto out;
-		rc = read_block(ti, DM_IO_KMEM, wpg->zf_est,   lba + 1, 1, wq);
-		if (rc)
-			goto out;
-		lba += 2;
-	}
-
-	INIT_LIST_HEAD(&hjload);
-
-	generation = mcache_greatest_gen(znd, wq, NULL, &lba);
+	generation = mcache_greatest_gen(znd, wq, &sblba, &lba);
 	if (generation == 0) {
 		rc = -ENODATA;
 		goto out;
@@ -3956,47 +4021,32 @@ static int z_mapped_init(struct zoned *znd)
 	if (lba == 0)
 		lba++;
 
-	do {
-		struct map_cache *mcache = mcache_alloc(znd, NORMAL);
+	/* read superblock */
+	rc = read_block(ti, DM_IO_VMA, io_vcache, sblba, nblks, wq);
+	if (rc)
+		goto out;
 
-		if (!mcache) {
-			rc = -ENOMEM;
-			goto out;
-		}
+	memcpy(znd->bmkeys, io_vcache, sizeof(*znd->bmkeys));
 
-		rc = read_block(ti, DM_IO_KMEM,
-				mcache->jdata, lba, nblks, wq);
-		if (rc) {
-			Z_ERR(znd, "%s: mcache-> %" PRIu64
-				   " [%d blks] %p -> %d",
-			      __func__, lba, nblks, mcache->jdata, rc);
+	/* read in map cache */
+	for (idx = 0; idx < le16_to_cpu(znd->bmkeys->maps); idx++) {
+		rc = do_load_map_cache(znd, lba++, jcount++, wq);
+		if (rc)
 			goto out;
-		}
-		lba++;
-
-		if (is_key_page(mcache->jdata)) {
-			memcpy(znd->bmkeys, mcache->jdata, Z_C4K);
-			mcache->jcount = 0;
-			done = 1;
-			ZDM_FREE(znd, mcache->jdata, Z_C4K, PG_08);
-			ZDM_FREE(znd, mcache, sizeof(*mcache), KM_07);
-			mcache = NULL;
-		} else {
-			u16 jcount;
-
-			(void)le64_to_lba48(mcache->jdata[0].bval, &jcount);
-			mcache->jcount = jcount;
-
-			list_add(&(mcache->mclist), &hjload);
-			jfound++;
-		}
+	}
 
-		if (jfound > MAX_CACHE_SYNC) {
-			rc = -EIO;
+	/* read in discard cache */
+	for (idx = 0; idx < le16_to_cpu(znd->bmkeys->discards); idx++) {
+		rc = do_load_discard_cache(znd, lba++, jcount++, wq);
+		if (rc)
 			goto out;
-		}
-	} while (!done);
+	}
+
+	/* skip re-read of superblock */
+	if (lba == sblba)
+		lba++;
 
+        /* read in CRC pgs */
 	rc = read_block(ti, DM_IO_VMA, znd->md_crcs, lba, 2, wq);
 	if (rc)
 		goto out;
@@ -4011,66 +4061,61 @@ static int z_mapped_init(struct zoned *znd)
 		      le32_to_cpu(crc_chk),
 		      le32_to_cpu(znd->bmkeys->crc32));
 		rc = -EIO;
+		goto out;
 	}
 
-	if (jfound != le16_to_cpu(znd->bmkeys->n_crcs)) {
+	if (jcount != le16_to_cpu(znd->bmkeys->n_crcs)) {
 		Z_ERR(znd, " ... mcache entries: found = %u, expected = %u",
-		      jfound, le16_to_cpu(znd->bmkeys->n_crcs));
+		      jcount, le16_to_cpu(znd->bmkeys->n_crcs));
 		rc = -EIO;
+		goto out;
 	}
-	if ((crc_chk == znd->bmkeys->crc32) && !list_empty(&hjload)) {
-		struct map_cache *mcache;
-		struct map_cache *jsafe;
-
-/*
- *  FIXME: mcache is MAP_COUNT lists stashed at sync.
- *	 we need to retrieve both lists.
- */
-
-		idx = 0;
-		list_for_each_entry_safe(mcache, jsafe, &hjload, mclist) {
-			__le16 crc = crc_md_le16(mcache->jdata, Z_CRC_4K);
-
-			Z_DBG(znd, "mcache CRC: %u: %04x [vs %04x] (c:%d)",
-			      idx, le16_to_cpu(crc),
-			      le16_to_cpu(znd->bmkeys->crcs[idx]),
-			      mcache->jcount);
 
-			if (crc == znd->bmkeys->crcs[idx]) {
-/* WRONG ... -------> */
-				int no = le64_to_lba48(mcache->jdata[1].tlba,
-							NULL) % MAP_COUNT;
-
-				mclist_add(znd, mcache, no);
-			} else {
-				Z_ERR(znd, ".. %04x [vs %04x] (c:%d)",
-				      le16_to_cpu(crc),
-				      le16_to_cpu(znd->bmkeys->crcs[idx]),
-				      mcache->jcount);
-				rc = -EIO;
-			}
-			idx++;
-		}
-	}
 	crc_chk = crc_md_le16(znd->md_crcs, Z_CRC_4K * 2);
 	if (crc_chk != znd->bmkeys->md_crc) {
 		Z_ERR(znd, "CRC of CRC PGs: Ex %04x vs %04x  <- calculated",
 		      le16_to_cpu(znd->bmkeys->md_crc),
 		      le16_to_cpu(crc_chk));
+		rc = -EIO;
+		goto out;
 	}
 
+
+	/*
+	 * Read write printers
+	 */
+	lba = 0x2048ul;
 	znd->discard_count = 0;
 	for (idx = 0; idx < znd->gz_count; idx++) {
 		struct meta_pg *wpg = &znd->wp[idx];
-		__le16 crc_wp = crc_md_le16(wpg->wp_alloc, Z_CRC_4K);
-		__le16 crc_zf = crc_md_le16(wpg->zf_est, Z_CRC_4K);
+                __le16 crc_wp;
+		__le16 crc_zf;
+
+		rc = read_block(ti, DM_IO_KMEM, wpg->wp_alloc, lba, 1, wq);
+		if (rc)
+			goto out;
+		crc_wp = crc_md_le16(wpg->wp_alloc, Z_CRC_4K);
+		if (znd->bmkeys->wp_crc[idx] != crc_wp)
+			Z_ERR(znd, "WP @ %d does not match written.", idx);
+
+		rc = read_block(ti, DM_IO_KMEM, wpg->zf_est, lba + 1, 1, wq);
+		if (rc)
+			goto out;
+		crc_zf = crc_md_le16(wpg->zf_est, Z_CRC_4K);
+		if (znd->bmkeys->zf_crc[idx] != crc_zf)
+			Z_ERR(znd, "ZF @ %d does not match written.", idx);
+
+		Z_DBG(znd, "%d# -- WP: %04x [%04x] | ZF: %04x [%04x]",
+		      idx, znd->bmkeys->wp_crc[idx], crc_wp,
+			   znd->bmkeys->zf_crc[idx], crc_zf);
+
 
 		if (znd->bmkeys->wp_crc[idx] == crc_wp &&
-		    znd->bmkeys->zf_crc[idx] == crc_zf) {
+		    znd->bmkeys->zf_crc[idx] == crc_zf)
 			znd->discard_count += count_stale_blocks(znd, idx, wpg);
-		}
-	}
 
+		lba += 2;
+	}
 	znd->z_gc_resv   = le32_to_cpu(znd->bmkeys->gc_resv);
 	znd->z_meta_resv = le32_to_cpu(znd->bmkeys->meta_resv);
 
@@ -4088,13 +4133,14 @@ out:
  * @c: Number of (contiguous) map entries to add.
  * @fp: Allocation flags (for _ALLOC)
  */
-static int z_mapped_addmany(struct zoned *znd, u64 dm_s, u64 lba, u64 c, int fp)
+static int z_mapped_addmany(struct zoned *znd, u64 dm_s, u64 lba,
+			    u64 count, gfp_t gfp)
 {
 	int rc = 0;
 	sector_t blk;
 
-	for (blk = 0; blk < c; blk++) {
-		rc = z_mapped_add_one(znd, dm_s + blk, lba + blk, fp);
+	for (blk = 0; blk < count; blk++) {
+		rc = z_mapped_add_one(znd, dm_s + blk, lba + blk, gfp);
 		if (rc)
 			goto out;
 	}
@@ -4113,7 +4159,7 @@ out:
  * @gfp: Allocation flags (for _ALLOC)
  */
 static struct map_pg *alloc_pg(struct zoned *znd, int entry, u64 lba,
-			       struct mpinfo *mpi, int ahead, int gfp)
+			       struct mpinfo *mpi, int ahead, gfp_t gfp)
 {
 	spinlock_t *lock = &znd->ct_lock;
 	struct map_pg *found = ZDM_ALLOC(znd, sizeof(*found), KM_20, gfp);
@@ -4176,12 +4222,9 @@ static __always_inline int _maybe_undrop(struct zoned *znd, struct map_pg *pg)
 		pg->hotness += MEM_HOT_BOOST_INC;
 		pg->age = jiffies_64 + msecs_to_jiffies(pg->hotness);
 
-		if (getref_pg(pg) != 1)
+		if (getref_pg(pg) != 0)
 			Z_ERR(znd, "Undelete with elevated ref: %u",
 			      getref_pg(pg));
-		if (pg->data.addr)
-			Z_ERR(znd, "Undelete with hot page?");
-
 		undrop = 1;
 	} else {
 		ref_pg(pg);
@@ -4200,7 +4243,7 @@ static __always_inline int _maybe_undrop(struct zoned *znd, struct map_pg *pg)
  * itself on disk). The recursion is never deep but it can
  * be avoided or mitigated by keep such 'key' pages in cache.
  */
-static int _load_backing_pages(struct zoned *znd, u64 lba, int gfp)
+static int _load_backing_pages(struct zoned *znd, u64 lba, gfp_t gfp)
 {
 	const int raflg = 1;
 	int rc = 0;
@@ -4255,7 +4298,7 @@ out:
  * into cache. Rather than defer it to cache_pg() it's brought into the
  * cache here.
  */
-static int _load_crc_page(struct zoned *znd, struct mpinfo *mpi, int gfp)
+static int _load_crc_page(struct zoned *znd, struct mpinfo *mpi, gfp_t gfp)
 {
 	const int raflg = 0;
 	int rc = 0;
@@ -4291,7 +4334,6 @@ static int _load_crc_page(struct zoned *znd, struct mpinfo *mpi, int gfp)
 	return rc;
 }
 
-
 /**
  * put_map_entry() - Decrement refcount of mapped page.
  * @pg: mapped page
@@ -4312,7 +4354,7 @@ static inline void put_map_entry(struct map_pg *pg)
  *
  * Page will be loaded from disk it if is not already in core memory.
  */
-static struct map_pg *get_map_entry(struct zoned *znd, u64 lba, int gfp)
+static struct map_pg *get_map_entry(struct zoned *znd, u64 lba, gfp_t gfp)
 {
 	struct mpinfo mpi;
 	struct map_pg *ahead[READ_AHEAD];
@@ -5136,7 +5178,7 @@ static void update_stale_ratio(struct zoned *znd, u32 zone)
  * @delay: Delay queue metric
  * @gfp: Allocation scheme.
  */
-static int z_zone_compact_queue(struct zoned *znd, u32 z_gc, int delay, int gfp)
+static int z_zone_compact_queue(struct zoned *znd, u32 z_gc, int delay, gfp_t gfp)
 {
 	unsigned long flags;
 	int do_queue = 0;
@@ -5195,7 +5237,7 @@ static u32 zone_zfest(struct zoned *znd, u32 z_id)
  * @gfp: Default memory allocation scheme.
  *
  */
-static int gc_compact_check(struct zoned *znd, int bin, int delay, int gfp)
+static int gc_compact_check(struct zoned *znd, int bin, int delay, gfp_t gfp)
 {
 	unsigned long flags;
 	int queued = 0;
@@ -5512,7 +5554,7 @@ static inline int is_reserved(struct zoned *znd, const u32 z_pref)
  * @delay: Delay metric
  * @gfp: Allocation flags to use.
  */
-static int gc_can_cherrypick(struct zoned *znd, u32 bin, int delay, int gfp)
+static int gc_can_cherrypick(struct zoned *znd, u32 bin, int delay, gfp_t gfp)
 {
 	u32 z_id = bin * znd->stale.binsz;
 	u32 s_end = z_id + znd->stale.binsz;
@@ -5547,7 +5589,7 @@ static int gc_can_cherrypick(struct zoned *znd, u32 bin, int delay, int gfp)
  * @delay: Delay metric
  * @gfp: Allocation flags to use.
  */
-static int gc_queue_with_delay(struct zoned *znd, int delay, int gfp)
+static int gc_queue_with_delay(struct zoned *znd, int delay, gfp_t gfp)
 {
 	int gc_idle = 0;
 	unsigned long flags;
@@ -5596,7 +5638,7 @@ static int gc_queue_with_delay(struct zoned *znd, int delay, int gfp)
  * @znd: ZDM Instance
  * @gfp: Allocation flags to use.
  */
-static int gc_immediate(struct zoned *znd, int wait, int gfp)
+static int gc_immediate(struct zoned *znd, int wait, gfp_t gfp)
 {
 	const int delay = 0;
 	int can_retry = 0;
@@ -5793,7 +5835,7 @@ static u64 z_acquire(struct zoned *znd, u32 flags, u32 nblks, u32 *nfound)
 	u32 stream_id = 0;
 	u32 z_find;
 	const int wait = 1;
-	int gfp = flags & Z_AQ_NORMAL ? CRIT : NORMAL;
+	gfp_t gfp = (flags & Z_AQ_NORMAL) ? GFP_ATOMIC : GFP_KERNEL;
 
 	zone_filled_cleanup(znd);
 
@@ -6256,7 +6298,6 @@ static int _sync_dirty(struct zoned *znd, int bit_type, int sync, int drop)
 {
 	int err = 0;
 	int entries = 0;
-	int do_bflush = 0;
 	struct map_pg *expg = NULL;
 	struct map_pg *_tpg;
 	struct map_pg **wset = NULL;
@@ -6297,30 +6338,30 @@ static int _sync_dirty(struct zoned *znd, int bit_type, int sync, int drop)
 			if (index > -1 && index < count &&
 			    getref_pg(expg) == 1 && expg == zlt[index]) {
 				zlt[expg->index] = NULL;
-
-				MutexLock(&expg->md_lock);
 				list_del(&expg->zltlst);
 				clear_bit(IN_ZLT, &expg->flags);
 				drop--;
-				if (expg->data.addr) {
-					void *pg = expg->data.addr;
-
-					expg->data.addr = NULL;
-					ZDM_FREE(znd, pg, Z_C4K, PG_27);
-					znd->incore_count--;
-				}
 				expg->age = jiffies_64;
-				mutex_unlock(&expg->md_lock);
-				do_bflush = 1;
+					  + msecs_to_jiffies(expg->hotness);
 
 #if ENABLE_PG_FREE_VIA_LAZY
 				lazy_pool_add(znd, expg, IS_DROPPED);
 				deref_pg(expg);
-
+				zlt[expg->index] = expg; /* so undrop works */
 				if (getref_pg(expg) > 0)
 					Z_ERR(znd, "Moving elv ref: %u",
 					      getref_pg(expg));
 #else
+				MutexLock(&expg->md_lock);
+				if (expg->data.addr) {
+					void *pg = expg->data.addr;
+
+					expg->data.addr = NULL;
+					ZDM_FREE(znd, pg, Z_C4K, PG_27);
+					znd->incore_count--;
+				}
+				mutex_unlock(&expg->md_lock);
+
 				/* Delete has ref count elevated. It's Okay */
 				ZDM_FREE(znd, expg, sizeof(*expg), KM_20);
 #endif
@@ -6351,9 +6392,6 @@ out:
 	if (wset)
 		ZDM_FREE(znd, wset, sizeof(*wset) * MAX_WSET, KM_19);
 
-	if (do_bflush && err >= 0)
-		err = z_flush_bdev(znd, GFP_KERNEL);
-
 	return err;
 }
 
@@ -6454,7 +6492,7 @@ static int map_addr_calc(struct zoned *znd, u64 origin, struct map_addr *out)
  *
  * Return: 1 if page exists, 0 if unmodified, else -errno on error.
  */
-static int read_pg(struct zoned *znd, struct map_pg *pg, u64 lba48, int gfp,
+static int read_pg(struct zoned *znd, struct map_pg *pg, u64 lba48, gfp_t gfp,
 		   struct mpinfo *mpi)
 {
 	int rcode = 0;
@@ -6540,7 +6578,7 @@ out:
  *
  * Return: 1 if page loaded from disk, 0 if empty, else -errno on error.
  */
-static int cache_pg(struct zoned *znd, struct map_pg *pg, int gfp,
+static int cache_pg(struct zoned *znd, struct map_pg *pg, gfp_t gfp,
 		    struct mpinfo *mpi)
 {
 	int rc = 0;
@@ -6586,7 +6624,7 @@ static int cache_pg(struct zoned *znd, struct map_pg *pg, int gfp,
  * @addr: Address to resolve (via FWD map).
  * @gfp: Current allocation flags.
  */
-static u64 z_lookup_table(struct zoned *znd, u64 addr, int gfp)
+static u64 z_lookup_table(struct zoned *znd, u64 addr, gfp_t gfp)
 {
 	struct map_addr maddr;
 	struct map_pg *pg;
@@ -6806,7 +6844,7 @@ out:
  * Add an unused block to the list of blocks to be discarded during
  * garbage collection.
  */
-static int unused_phy(struct zoned *znd, u64 lba, u64 orig_s, int gfp)
+static int unused_phy(struct zoned *znd, u64 lba, u64 orig_s, gfp_t gfp)
 {
 	int err = 0;
 	struct map_pg *pg;
-- 
2.7.0

