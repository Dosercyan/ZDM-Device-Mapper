From efb13703bb5b0ef3132339df54f83339c083b4a9 Mon Sep 17 00:00:00 2001
From: Shaun Tancheff <shaun@tancheff.com>
Date: Mon, 14 Mar 2016 19:06:28 -0500
Subject: [PATCH 27/29] Bug fixes for ZDM v105.

Handle low bit set/clear in block ioctl to signal ATA 16
workaround for older SAS HBA's.

Add counters for ZLT and LZY lists to debug ZTL memory leaks.

Fix a memory leak on LZY list undropped blocks no being added
back to the ZTL list.

Fix GC Queue returning error and not boolean.
Also added a warning when nothing was queued (and delay is 0).

Reworked gc_immediate() again. Wait/complete seems messier
than mutex locking ...

Restored whitespace changes in:
   drivers/scsi/sd.c
   fs/read_write.c

Signed-off-by: Shaun Tancheff <shaun.tancheff@seagate.com>
---
 block/ioctl.c         | 127 ++++++++++++++++------------
 drivers/md/dm-zoned.c |  43 +++++-----
 drivers/md/dm-zoned.h |   4 +-
 drivers/md/libzoned.c | 225 +++++++++++++++++++++++++++++++++-----------------
 drivers/md/raid5.c    |   2 -
 drivers/scsi/sd.c     |  67 ++++++++-------
 fs/read_write.c       |   4 +-
 7 files changed, 284 insertions(+), 188 deletions(-)

diff --git a/block/ioctl.c b/block/ioctl.c
index 436f165..7ac1d3a 100644
--- a/block/ioctl.c
+++ b/block/ioctl.c
@@ -195,61 +195,6 @@ int blkdev_reread_part(struct block_device *bdev)
 }
 EXPORT_SYMBOL(blkdev_reread_part);
 
-static int blk_ioctl_discard(struct block_device *bdev, fmode_t mode,
-		unsigned long arg, unsigned long flags)
-{
-	uint64_t range[2];
-	uint64_t start, len;
-
-	if (!(mode & FMODE_WRITE))
-		return -EBADF;
-
-	if (copy_from_user(range, (void __user *)arg, sizeof(range)))
-		return -EFAULT;
-
-	start = range[0];
-	len = range[1];
-
-	if (start & 511)
-		return -EINVAL;
-	if (len & 511)
-		return -EINVAL;
-	start >>= 9;
-	len >>= 9;
-
-	if (start + len > (i_size_read(bdev->bd_inode) >> 9))
-		return -EINVAL;
-	return blkdev_issue_discard(bdev, start, len, GFP_KERNEL, flags);
-}
-
-static int blk_ioctl_zeroout(struct block_device *bdev, fmode_t mode,
-		unsigned long arg)
-{
-	uint64_t range[2];
-	uint64_t start, len;
-
-	if (!(mode & FMODE_WRITE))
-		return -EBADF;
-
-	if (copy_from_user(range, (void __user *)arg, sizeof(range)))
-		return -EFAULT;
-
-	start = range[0];
-	len = range[1];
-
-	if (start & 511)
-		return -EINVAL;
-	if (len & 511)
-		return -EINVAL;
-	start >>= 9;
-	len >>= 9;
-
-	if (start + len > (i_size_read(bdev->bd_inode) >> 9))
-		return -EINVAL;
-
-	return blkdev_issue_zeroout(bdev, start, len, GFP_KERNEL, false);
-}
-
 static int blk_zoned_report_ioctl(struct block_device *bdev, fmode_t mode,
 		void __user *parg)
 {
@@ -317,6 +262,17 @@ static int blk_zoned_action_ioctl(struct block_device *bdev, fmode_t mode,
 	if (!(mode & FMODE_WRITE))
 		return -EBADF;
 
+	/*
+	 * When the low bit is set force ATA passthrough try to work around
+	 * older SAS HBA controllers that don't support ZBC to ZAC translation.
+	 *
+	 * When the low bit is clear follow the normal path but also correct
+	 * for ~0ul LBA means 'for all lbas'.
+	 *
+	 * NB: We should do extra checking here to see if the user specified
+	 *     the entire block device as opposed to a partition of the
+	 *     device....
+	 */
 	if (arg & 1) {
 		bi_rw = REQ_PRIO;
 		if (arg != ~0ul)
@@ -333,6 +289,9 @@ static int blk_zoned_action_ioctl(struct block_device *bdev, fmode_t mode,
 	case BLKCLOSEZONE:
 		bi_rw = REQ_CLOSE_ZONE;
 		break;
+	case BLKDISCARD:
+		bi_rw = REQ_DISCARD;
+		break;
 	default:
 		pr_err("%s: Unknown action: %u", __func__, cmd);
 		WARN_ON(1);
@@ -340,6 +299,64 @@ static int blk_zoned_action_ioctl(struct block_device *bdev, fmode_t mode,
 	return blkdev_issue_zone_action(bdev, bi_rw, arg, GFP_KERNEL);
 }
 
+static int blk_ioctl_discard(struct block_device *bdev, fmode_t mode,
+		unsigned long arg, unsigned long flags)
+{
+	uint64_t range[2];
+	uint64_t start, len;
+
+	if (!(mode & FMODE_WRITE))
+		return -EBADF;
+
+	if (copy_from_user(range, (void __user *)arg, sizeof(range)))
+		return -EFAULT;
+
+	start = range[0];
+	len = range[1];
+
+	if (start & 1 && len == (1 << 29))
+		return blk_zoned_action_ioctl(bdev, mode, BLKDISCARD, start);
+
+	if (start & 511)
+		return -EINVAL;
+	if (len & 511)
+		return -EINVAL;
+	start >>= 9;
+	len >>= 9;
+
+	if (start + len > (i_size_read(bdev->bd_inode) >> 9))
+		return -EINVAL;
+	return blkdev_issue_discard(bdev, start, len, GFP_KERNEL, flags);
+}
+
+static int blk_ioctl_zeroout(struct block_device *bdev, fmode_t mode,
+		unsigned long arg)
+{
+	uint64_t range[2];
+	uint64_t start, len;
+
+	if (!(mode & FMODE_WRITE))
+		return -EBADF;
+
+	if (copy_from_user(range, (void __user *)arg, sizeof(range)))
+		return -EFAULT;
+
+	start = range[0];
+	len = range[1];
+
+	if (start & 511)
+		return -EINVAL;
+	if (len & 511)
+		return -EINVAL;
+	start >>= 9;
+	len >>= 9;
+
+	if (start + len > (i_size_read(bdev->bd_inode) >> 9))
+		return -EINVAL;
+
+	return blkdev_issue_zeroout(bdev, start, len, GFP_KERNEL, false);
+}
+
 static int put_ushort(unsigned long arg, unsigned short val)
 {
 	return put_user(val, (unsigned short __user *)arg);
diff --git a/drivers/md/dm-zoned.c b/drivers/md/dm-zoned.c
index 2a13e0b..8e8cf7a 100644
--- a/drivers/md/dm-zoned.c
+++ b/drivers/md/dm-zoned.c
@@ -42,6 +42,8 @@
 #define PRIx32 "x"
 #define PRIu32 "u"
 
+#define BIOSET_RESV 4
+
 /**
  * _zdisk() - Return a pretty ZDM name.
  * @znd: ZDM Instance
@@ -789,24 +791,25 @@ static int zoned_ctr(struct dm_target *ti, unsigned argc, char **argv)
 		return -EINVAL;
 	}
 
-#if USE_KTHREAD
-
-	znd->bio_set = bioset_create(4, 0);
+	znd->bio_set = bioset_create(BIOSET_RESV, 0);
 	if (!znd->bio_set)
 		return -ENOMEM;
 
+#if USE_KTHREAD
 	r = kfifo_alloc(&znd->bio_fifo, KFIFO_SIZE, GFP_KERNEL);
 	if (r)
 		return r;
 
 	znd->bio_kthread = kthread_run(znd_bio_kthread, znd, "zdm-io-%s",
 		znd->bdev_name);
-	if (!znd->bio_kthread) {
+	if (IS_ERR(znd->bio_kthread)) {
+		r = PTR_ERR(znd->bio_kthread);
 		ti->error = "Couldn't alloc kthread";
 		zoned_destroy(znd);
-		return -EINVAL;
+		return r;
 	}
 #endif
+
 	r = zdm_create_proc_entries(znd);
 	if (r) {
 		ti->error = "Failed to create /proc entries";
@@ -843,11 +846,9 @@ static void zoned_dtr(struct dm_target *ti)
 	kthread_stop(znd->bio_kthread);
 	kfifo_free(&znd->bio_fifo);
 #endif
-
 	if (znd->bio_set)
 		bioset_free(znd->bio_set);
 	zdm_remove_proc_entries(znd);
-
 	zoned_destroy(znd);
 }
 
@@ -1361,7 +1362,7 @@ static int zm_write_bios(struct zoned *znd, struct bio *bio, u64 s_zdm)
 			lba = z_acquire(znd, acqflgs, mapped, &mapped);
 
 		if (!lba) {
-			if (znd->gc_throttle.counter == 0) {
+			if (atomic_read(&znd->gc_throttle) == 0) {
 				err = -ENOSPC;
 				goto out;
 			}
@@ -1451,7 +1452,7 @@ reacquire:
 			lba = z_acquire(znd, acqflgs, mapped, &mapped);
 
 		if (!lba) {
-			if (znd->gc_throttle.counter == 0) {
+			if (atomic_read(&znd->gc_throttle) == 0) {
 				err = -ENOSPC;
 				goto out;
 			}
@@ -1884,7 +1885,7 @@ static inline int _do_mem_purge(struct zoned *znd)
 {
 	int do_work = 0;
 
-	if (znd->incore_count > 3) {
+	if (atomic_read(&znd->incore) > 3) {
 		set_bit(DO_MEMPOOL, &znd->flags);
 		if (!work_pending(&znd->meta_work))
 			do_work = 1;
@@ -2313,7 +2314,7 @@ static int zdm_status_show(struct seq_file *seqf, void *unused)
 
 	status.memstat = znd->memstat;
 	memcpy(status.bins, znd->bins, sizeof(status.bins));
-	status.mlut_blocks = znd->incore_count;
+	status.mlut_blocks = atomic_read(&znd->incore);
 
 	return seq_write(seqf, &status, sizeof(status));
 }
@@ -2347,17 +2348,19 @@ static int zdm_info_show(struct seq_file *seqf, void *unused)
 {
 	struct zoned *znd = seqf->private;
 
-	seq_printf(seqf, "On device:    %s\n", _zdisk(znd));
-	seq_printf(seqf, "Data Zones:   %u\n", znd->data_zones);
-	seq_printf(seqf, "Empty Zones:  %u\n", znd->z_gc_free);
-	seq_printf(seqf, "Cached Pages: %u\n", znd->mc_entries);
+	seq_printf(seqf, "On device:     %s\n", _zdisk(znd));
+	seq_printf(seqf, "Data Zones:    %u\n", znd->data_zones);
+	seq_printf(seqf, "Empty Zones:   %u\n", znd->z_gc_free);
+	seq_printf(seqf, "Cached Pages:  %u\n", znd->mc_entries);
 	seq_printf(seqf, "Discard Pages: %u\n", znd->dc_entries);
-
-	seq_printf(seqf, "ZTL Pages:    %u\n", znd->incore_count);
-	seq_printf(seqf, "RAM in Use:   %lu\n", znd->memstat);
-	seq_printf(seqf, "Zones GC'd:   %u\n", znd->gc_events);
+	seq_printf(seqf, "ZTL Pages:     %d\n", atomic_read(&znd->incore));
+	seq_printf(seqf, "   in ZTL:     %d\n", znd->in_zlt);
+	seq_printf(seqf, "   in LZY:     %d\n", znd->in_lzy);
+	seq_printf(seqf, "RAM in Use:    %lu\n", znd->memstat);
+	seq_printf(seqf, "Zones GC'd:    %u\n", znd->gc_events);
+	seq_printf(seqf, "GC Throttle:   %d\n", atomic_read(&znd->gc_throttle));
 #if ALLOC_DEBUG
-	seq_printf(seqf, "Max Allocs:   %u\n", znd->hw_allocs);
+	seq_printf(seqf, "Max Allocs:    %u\n", znd->hw_allocs);
 #endif
 
 	return 0;
diff --git a/drivers/md/dm-zoned.h b/drivers/md/dm-zoned.h
index a4b19cb..75eb82e 100644
--- a/drivers/md/dm-zoned.h
+++ b/drivers/md/dm-zoned.h
@@ -613,7 +613,7 @@ struct zoned {
 	u32 gz_count;
 	u32 zdstart;
 	u32 z_gc_free;
-	s32 incore_count;
+	atomic_t incore;
 	u32 discard_count;
 	u32 z_current;
 	u32 z_meta_resv;
@@ -621,6 +621,8 @@ struct zoned {
 	u32 gc_events;
 	int mc_entries;
 	int dc_entries;
+	int in_zlt;
+	int in_lzy;
 	int meta_result;
 	struct stale_tracking stale;
 
diff --git a/drivers/md/libzoned.c b/drivers/md/libzoned.c
index 18621f7..4dd1c30 100644
--- a/drivers/md/libzoned.c
+++ b/drivers/md/libzoned.c
@@ -12,7 +12,7 @@
  * warranty of any kind, whether express or implied.
  */
 
-#define BUILD_NO		104
+#define BUILD_NO		105
 
 #define EXTRA_DEBUG		0
 #define ENABLE_PG_FREE_VIA_LAZY	1
@@ -402,6 +402,7 @@ void lazy_pool_add(struct zoned *znd, struct map_pg *expg, int bit)
 	if (!test_bit(IS_LAZY, &expg->flags)) {
 		set_bit(IS_LAZY, &expg->flags);
 		list_add(&expg->lazy, &znd->lzy_pool);
+		znd->in_lzy++;
 	}
 	set_bit(bit, &expg->flags);
 	spin_unlock_irqrestore(&znd->lzy_lck, flags);
@@ -420,11 +421,10 @@ void lazy_pool_splice(struct zoned *znd, struct list_head *list)
 	unsigned long flags;
 
 	spin_lock_irqsave(&znd->lzy_lck, flags);
-	list_splice(list, &znd->lzy_pool);
+	list_splice_tail(list, &znd->lzy_pool);
 	spin_unlock_irqrestore(&znd->lzy_lck, flags);
 }
 
-
 /**
  * pool_add() - Add metadata block to zltlst
  * @znd: ZDM instance
@@ -444,6 +444,7 @@ static inline int pool_add(struct zoned *znd, struct map_pg *expg)
 			} else {
 				set_bit(IN_ZLT, &expg->flags);
 				list_add(&expg->zltlst, &znd->zltpool);
+				znd->in_zlt++;
 			}
 			spin_unlock_irqrestore(&znd->zlt_lck, flags);
 			rcode = 0;
@@ -974,7 +975,7 @@ static void mapped_free(struct zoned *znd, struct map_pg *mapped)
 		WARN_ON(test_bit(IS_DIRTY, &mapped->flags));
 		if (mapped->data.addr) {
 			ZDM_FREE(znd, mapped->data.addr, Z_C4K, PG_27);
-			znd->incore_count--;
+			atomic_dec(&znd->incore);
 		}
 		mutex_unlock(&mapped->md_lock);
 		ZDM_FREE(znd, mapped, sizeof(*mapped), KM_20);
@@ -1360,15 +1361,13 @@ static int do_init_zoned(struct dm_target *ti, struct zoned *znd)
 	}
 	spin_lock_init(&znd->zlt_lck);
 	spin_lock_init(&znd->lzy_lck);
-
-	mutex_init(&znd->pool_mtx);
-	mutex_init(&znd->gc_wait);
-
 	spin_lock_init(&znd->gc_lock);
 	spin_lock_init(&znd->stats_lock);
 	spin_lock_init(&znd->mapkey_lock);
 	spin_lock_init(&znd->ct_lock);
 
+	mutex_init(&znd->pool_mtx);
+	mutex_init(&znd->gc_wait);
 	mutex_init(&znd->gc_postmap.cached_lock);
 	mutex_init(&znd->gc_vcio_lock);
 	mutex_init(&znd->vcio_lock);
@@ -1526,10 +1525,7 @@ static int do_init_zoned(struct dm_target *ti, struct zoned *znd)
 	INIT_WORK(&znd->bg_work, bg_work_task);
 	INIT_DELAYED_WORK(&znd->gc_work, gc_work_task);
 	setup_timer(&znd->timer, activity_timeout, (unsigned long)znd);
-
-	znd->incore_count = 0;
 	znd->last_w = BAD_ADDR;
-
 	set_bit(DO_SYNC, &znd->flags);
 
 out:
@@ -2959,7 +2955,8 @@ static __always_inline int pg_delete(struct zoned *znd, struct map_pg *expg)
 {
 	struct map_pg **zlt;
 	spinlock_t *lock;
-	int release = 0;
+	int req_flush = 0;
+	int dropped = 0;
 	int index = expg->index;
 	int is_lut = test_bit(IS_LUT, &expg->flags);
 	int is_fwd = test_bit(IS_FWD, &expg->flags);
@@ -2974,32 +2971,37 @@ static __always_inline int pg_delete(struct zoned *znd, struct map_pg *expg)
 	}
 
 	if (!spin_trylock(lock))
-		return release;
+		return req_flush;
 
 	if (index > -1 && index < count && expg == zlt[index] &&
-	    test_and_clear_bit(IS_DROPPED, &expg->flags)) {
+	    test_bit(IS_DROPPED, &expg->flags)) {
 		zlt[index] = NULL;
 		MutexLock(&expg->md_lock);
 		if (test_and_clear_bit(IS_LAZY, &expg->flags)) {
-			list_del(&expg->lazy);
+			clear_bit(IS_DROPPED, &expg->flags);
 			clear_bit(DELAY_ADD, &expg->flags);
 			if (expg->data.addr) {
 				void *pg = expg->data.addr;
 
 				expg->data.addr = NULL;
 				ZDM_FREE(znd, pg, Z_C4K, PG_27);
-				znd->incore_count--;
+				atomic_dec(&znd->incore);
+			} else {
+				Z_ERR(znd, "** No data pg? %llx", expg->lba);
 			}
-			release = !test_bit(IS_FLUSH, &expg->flags);
+			list_del(&expg->lazy);
+			znd->in_lzy--;
+			req_flush = !test_bit(IS_FLUSH, &expg->flags);
+			dropped = 1;
 		} else {
 			Z_ERR(znd, "Detected double list del.");
 		}
 		mutex_unlock(&expg->md_lock);
-		if (release)
+		if (dropped)
 			ZDM_FREE(znd, expg, sizeof(*expg), KM_20);
 	}
 	spin_unlock(lock);
-	return release;
+	return req_flush;
 }
 
 /**
@@ -3018,9 +3020,9 @@ static int manage_lazy_activity(struct zoned *znd)
 	struct map_pg *expg;
 	struct map_pg *_tpg;
 	int want_flush = 0;
+	const u32 msecs = MEM_HOT_BOOST_INC;
 
 	spin_lock(&znd->lzy_lck);
-
 	expg = list_first_entry_or_null(&znd->lzy_pool, typeof(*expg), lazy);
 	if (!expg || (&expg->lazy == &znd->lzy_pool))
 		goto out;
@@ -3037,15 +3039,19 @@ static int manage_lazy_activity(struct zoned *znd)
 		 */
 		if (test_bit(DELAY_ADD, &expg->flags)) {
 			if (spin_trylock(&znd->zlt_lck)) {
-				list_del(&expg->lazy);
-				clear_bit(IS_LAZY, &expg->flags);
-				clear_bit(IS_DROPPED, &expg->flags);
-
 				if (!test_bit(IN_ZLT, &expg->flags)) {
+					list_del(&expg->lazy);
+					znd->in_lzy--;
+					clear_bit(IS_LAZY, &expg->flags);
+					clear_bit(IS_DROPPED, &expg->flags);
+					clear_bit(DELAY_ADD, &expg->flags);
 					set_bit(IN_ZLT, &expg->flags);
 					list_add(&expg->zltlst, &znd->zltpool);
+					znd->in_zlt++;
+				} else {
+					Z_ERR(znd, "** ZLT double add? %llx",
+						expg->lba);
 				}
-				clear_bit(DELAY_ADD, &expg->flags);
 				spin_unlock(&znd->zlt_lck);
 			}
 #if ENABLE_PG_FREE_VIA_LAZY
@@ -3054,9 +3060,9 @@ static int manage_lazy_activity(struct zoned *znd)
 			 * Delete page
 			 */
 			if (!test_bit(IN_ZLT, &expg->flags) &&
-			     test_bit(IS_DROPPED, &expg->flags) &&
-			     is_expired_msecs(expg->age, MEM_PURGE_MSECS)) {
-				want_flush |= pg_delete(znd, expg);
+			     test_bit(IS_DROPPED, &expg->flags)) {
+				if (is_expired_msecs(expg->age, msecs))
+					want_flush |= pg_delete(znd, expg);
 			}
 #endif
 		}
@@ -3069,14 +3075,16 @@ out:
 	return want_flush;
 }
 
+
+
 /**
- * mark_clean_flush() - Mark all non-dirty ZLT blocks as 'FLUSH'
+ * mark_clean_flush_zlt() - Mark all non-dirty ZLT blocks as 'FLUSH'
  * @znd: ZDM instance
  *
  * After a FLUSH/FUA these blocks are on disk and redundant FLUSH
  * can be skipped if the block is later ejected.
  */
-static void mark_clean_flush(struct zoned *znd)
+static void mark_clean_flush_zlt(struct zoned *znd)
 {
 	struct map_pg *expg = NULL;
 	struct map_pg *_tpg;
@@ -3104,6 +3112,50 @@ out:
 }
 
 /**
+ * mark_clean_flush_lzy() - Mark all non-dirty ZLT blocks as 'FLUSH'
+ * @znd: ZDM instance
+ *
+ * After a FLUSH/FUA these blocks are on disk and redundant FLUSH
+ * can be skipped if the block is later ejected.
+ */
+static void mark_clean_flush_lzy(struct zoned *znd)
+{
+	struct map_pg *expg = NULL;
+	struct map_pg *_tpg;
+
+	spin_lock(&znd->lzy_lck);
+	expg = list_first_entry_or_null(&znd->lzy_pool, typeof(*expg), lazy);
+	if (!expg || (&expg->lazy == &znd->lzy_pool))
+		goto out;
+
+	_tpg = list_next_entry(expg, lazy);
+	while (&expg->lazy != &znd->lzy_pool) {
+		ref_pg(expg);
+		if (!test_bit(IS_DIRTY, &expg->flags))
+			set_bit(IS_FLUSH, &expg->flags);
+		deref_pg(expg);
+		expg = _tpg;
+		_tpg = list_next_entry(expg, lazy);
+	}
+
+out:
+	spin_unlock(&znd->lzy_lck);
+}
+
+/**
+ * mark_clean_flush() - Mark all non-dirty ZLT/LZY blocks as 'FLUSH'
+ * @znd: ZDM instance
+ *
+ * After a FLUSH/FUA these blocks are on disk and redundant FLUSH
+ * can be skipped if the block is later ejected.
+ */
+static void mark_clean_flush(struct zoned *znd)
+{
+	mark_clean_flush_zlt(znd);
+	mark_clean_flush_lzy(znd);
+}
+
+/**
  * do_sync_metadata() - Write ZDM state to disk.
  * @znd: ZDM instance
  *
@@ -3224,8 +3276,8 @@ static int do_sync_to_disk(struct zoned *znd)
 		else if (is_expired_msecs(znd->age, MEM_PURGE_MSECS))
 			pool_size = MZ_MEMPOOL_SZ;
 
-		if (znd->incore_count > pool_size)
-			drop = znd->incore_count - pool_size;
+		if (atomic_read(&znd->incore) > pool_size)
+			drop = atomic_read(&znd->incore) - pool_size;
 	}
 	if (sync || drop)
 		err = do_sync_metadata(znd, sync, drop);
@@ -4182,9 +4234,8 @@ static int z_mapped_init(struct zoned *znd)
 		goto out;
 	}
 
-
 	/*
-	 * Read write printers
+	 * Read write pointers / free counters.
 	 */
 	lba = 0x2048ul;
 	znd->discard_count = 0;
@@ -4316,21 +4367,18 @@ static __always_inline int _maybe_undrop(struct zoned *znd, struct map_pg *pg)
 
 	if (test_bit(IS_DROPPED, &pg->flags)) {
 		spin_lock(&znd->lzy_lck);
-		if (test_and_clear_bit(IS_DROPPED, &pg->flags) &&
-		    test_and_clear_bit(IS_LAZY, &pg->flags))
-			list_del(&pg->lazy);
+		if (test_bit(IS_DROPPED, &pg->flags) &&
+		    test_bit(IS_LAZY, &pg->flags)) {
+			clear_bit(IS_DROPPED, &pg->flags);
+			set_bit(DELAY_ADD, &pg->flags);
+		}
 		spin_unlock(&znd->lzy_lck);
-
-		pg->hotness += MEM_HOT_BOOST_INC;
+		if (pg->hotness < MEM_PURGE_MSECS)
+			pg->hotness += MEM_HOT_BOOST_INC;
 		pg->age = jiffies_64 + msecs_to_jiffies(pg->hotness);
-
-		if (getref_pg(pg) != 0)
-			Z_ERR(znd, "Undelete with elevated ref: %u",
-			      getref_pg(pg));
 		undrop = 1;
-	} else {
-		ref_pg(pg);
 	}
+	ref_pg(pg);
 	return undrop;
 }
 
@@ -5282,6 +5330,8 @@ static void update_stale_ratio(struct zoned *znd, u32 zone)
  * @z_gc: Zone to queue.
  * @delay: Delay queue metric
  * @gfp: Allocation scheme.
+ *
+ * Return: 1 on success, 0 if not queued/busy, negative on error.
  */
 static
 int z_zone_compact_queue(struct zoned *znd, u32 z_gc, int delay, gfp_t gfp)
@@ -5305,16 +5355,20 @@ int z_zone_compact_queue(struct zoned *znd, u32 z_gc, int delay, gfp_t gfp)
 	if (!znd->gc_active) {
 		znd->gc_active = gc_entry;
 		do_queue = 1;
+	} else {
+		Z_ERR(znd, "GC: Tried to queue but already active");
 	}
 	spin_unlock_irqrestore(&znd->gc_lock, flags);
 
 	if (do_queue) {
 		unsigned long tval = msecs_to_jiffies(delay);
 
-		queue_delayed_work(znd->gc_wq, &znd->gc_work, tval);
+		if (queue_delayed_work(znd->gc_wq, &znd->gc_work, tval))
+			err = 1;
 	} else {
 		ZDM_FREE(znd, gc_entry, sizeof(*gc_entry), KM_16);
 		znd->gc_backlog--;
+		Z_ERR(znd, "GC: FAILED to queue work");
 	}
 
 	return err;
@@ -5335,7 +5389,7 @@ static u32 zone_zfest(struct zoned *znd, u32 z_id)
 }
 
 /**
- * gc_compact_check() - Called periodically to initiate GC
+ * gc_request_queued() - Called periodically to initiate GC
  *
  * @znd: ZDM instance
  * @bin: Bin with stale zones to scan for GC
@@ -5343,7 +5397,7 @@ static u32 zone_zfest(struct zoned *znd, u32 z_id)
  * @gfp: Default memory allocation scheme.
  *
  */
-static int gc_compact_check(struct zoned *znd, int bin, int delay, gfp_t gfp)
+static int gc_request_queued(struct zoned *znd, int bin, int delay, gfp_t gfp)
 {
 	unsigned long flags;
 	int queued = 0;
@@ -5396,11 +5450,13 @@ static int gc_compact_check(struct zoned *znd, int bin, int delay, gfp_t gfp)
 		}
 	}
 
+	if (!delay && top_roi == NOZONE)
+		Z_ERR(znd, "No GC candidate in bin: %u -> %u", z_gc, s_end);
+
 	/* determine the cut-off for GC based on MZ overall staleness */
 	if (top_roi != NOZONE) {
 		u32 state_metric = GC_PRIO_DEFAULT;
-		u32 n_filled = znd->data_zones - znd->z_gc_free;
-		u32 n_empty = znd->data_zones - n_filled;
+		u32 n_empty = znd->z_gc_free;
 		int pctfree = n_empty * 100 / znd->data_zones;
 
 		/*
@@ -5416,9 +5472,20 @@ static int gc_compact_check(struct zoned *znd, int bin, int delay, gfp_t gfp)
 		else if (pctfree < 25)
 			state_metric = GC_PRIO_LOW;
 
-		if (zone_zfest(znd, top_roi) > state_metric)
-			if (z_zone_compact_queue(znd, top_roi, delay * 5, gfp))
+		if (zone_zfest(znd, top_roi) > state_metric) {
+			int rc;
+
+			rc = z_zone_compact_queue(znd, top_roi, delay * 5, gfp);
+			if (rc == 1)
 				queued = 1;
+			else if (rc < 0)
+				Z_ERR(znd, "GC: Z#%u !Q: ERR: %d", top_roi, rc);
+		}
+
+		if (!delay && !queued)
+			Z_ERR(znd, "GC: Z#%u !Q .. M: %u E: %u PCT: %d ZF: %u",
+				   top_roi, state_metric, n_empty, pctfree,
+				   zone_zfest(znd, top_roi));
 	}
 out:
 	return queued;
@@ -5696,6 +5763,8 @@ static int gc_can_cherrypick(struct zoned *znd, u32 bin, int delay, gfp_t gfp)
  * @znd: ZDM Instance
  * @delay: Delay metric
  * @gfp: Allocation flags to use.
+ *
+ * Return 1 if gc in progress or queued. 0 otherwise.
  */
 static int gc_queue_with_delay(struct zoned *znd, int delay, gfp_t gfp)
 {
@@ -5727,12 +5796,12 @@ static int gc_queue_with_delay(struct zoned *znd, int delay, gfp_t gfp)
 				gc_idle = 0;
 
 		/* Otherwise compact a zone in the stream */
-		if (gc_idle && gc_compact_check(znd, bin, delay, gfp))
+		if (gc_idle && gc_request_queued(znd, bin, delay, gfp))
 			gc_idle = 0;
 
 		/* Otherwise compact *something* */
 		for (iter = 0; gc_idle && (iter < znd->stale.count); iter++)
-			if (gc_idle && gc_compact_check(znd, iter, delay, gfp))
+			if (gc_idle && gc_request_queued(znd, iter, delay, gfp))
 				gc_idle = 0;
 	}
 	return !gc_idle;
@@ -5747,6 +5816,7 @@ static int gc_immediate(struct zoned *znd, int wait, gfp_t gfp)
 {
 	const int delay = 0;
 	int can_retry = 0;
+	int lock_owner = 0;
 	int queued;
 
 	/*
@@ -5757,12 +5827,15 @@ static int gc_immediate(struct zoned *znd, int wait, gfp_t gfp)
 	 *       mutex hack should do as a temporary workaround.
 	 */
 	atomic_inc(&znd->gc_throttle);
-
 	if (atomic_read(&znd->gc_throttle) == 1) {
 		mutex_lock(&znd->gc_wait);
+		lock_owner = 1;
 	} else if (atomic_read(&znd->gc_throttle) > 1) {
-		if (wait)
+		if (wait) {
 			mutex_lock(&znd->gc_wait);
+			can_retry = 1;
+			mutex_unlock(&znd->gc_wait);
+		}
 		goto out;
 	}
 	flush_delayed_work(&znd->gc_work);
@@ -5776,13 +5849,8 @@ static int gc_immediate(struct zoned *znd, int wait, gfp_t gfp)
 	can_retry = flush_delayed_work(&znd->gc_work);
 
 out:
-	if (atomic_read(&znd->gc_throttle) == 1) {
-		mutex_unlock(&znd->gc_wait);
-	} else if (atomic_read(&znd->gc_throttle) > 1 && wait) {
+	if (lock_owner)
 		mutex_unlock(&znd->gc_wait);
-		can_retry = 1;
-	}
-
 	atomic_dec(&znd->gc_throttle);
 
 	return can_retry;
@@ -5809,7 +5877,10 @@ static inline void set_current(struct zoned *znd, u32 flags, u32 zone)
 		znd->bmkeys->stream[stream_id] = cpu_to_le32(zone);
 	}
 	znd->z_current = zone;
-	znd->z_gc_free--;
+	if (znd->z_gc_free > 0)
+		znd->z_gc_free--;
+	else
+		Z_ERR(znd, "Dec z_gc_free below 0?");
 }
 
 /**
@@ -6415,9 +6486,8 @@ static int _sync_dirty(struct zoned *znd, int bit_type, int sync, int *_drop)
 	struct map_pg *expg = NULL;
 	struct map_pg *_tpg;
 	struct map_pg **wset = NULL;
-	struct list_head droplist;
-
-	INIT_LIST_HEAD(&droplist);
+	int dlstsz = 0;
+	LIST_HEAD(droplist);
 
 	wset = ZDM_CALLOC(znd, sizeof(*wset), MAX_WSET, KM_19, NORMAL);
 
@@ -6455,20 +6525,26 @@ static int _sync_dirty(struct zoned *znd, int bit_type, int sync, int *_drop)
 			    getref_pg(expg) == 1 && expg == zlt[index]) {
 				zlt[expg->index] = NULL;
 				list_del(&expg->zltlst);
+				znd->in_zlt--;
 				clear_bit(IN_ZLT, &expg->flags);
 				drop--;
 				expg->age = jiffies_64;
 					  + msecs_to_jiffies(expg->hotness);
 
 #if ENABLE_PG_FREE_VIA_LAZY
+				if (test_bit(IS_LAZY, &expg->flags))
+					Z_ERR(znd, "** Pg is lazy && zlt %llx",
+						expg->lba);
 
 				if (!test_bit(IS_LAZY, &expg->flags)) {
-					set_bit(IS_LAZY, &expg->flags);
 					list_add(&expg->lazy, &droplist);
+					znd->in_lzy++;
+					set_bit(IS_LAZY, &expg->flags);
+					set_bit(IS_DROPPED, &expg->flags);
+					dlstsz++;
 				}
-				set_bit(IS_DROPPED, &expg->flags);
-				deref_pg(expg);
 				zlt[expg->index] = expg; /* so undrop works */
+				deref_pg(expg);
 				if (getref_pg(expg) > 0)
 					Z_ERR(znd, "Moving elv ref: %u",
 					      getref_pg(expg));
@@ -6479,14 +6555,15 @@ static int _sync_dirty(struct zoned *znd, int bit_type, int sync, int *_drop)
 
 					expg->data.addr = NULL;
 					ZDM_FREE(znd, pg, Z_C4K, PG_27);
-					znd->incore_count--;
-
+					atomic_dec(&znd->incore);
 					if (!test_bit(IS_FLUSH, &expg->flags))
 						want_flush++;
 				}
 				mutex_unlock(&expg->md_lock);
-
-				/* Delete has ref count elevated. It's Okay */
+				deref_pg(expg);
+				if (getref_pg(expg) > 0)
+					Z_ERR(znd, "Dropped elv ref: %u",
+					      getref_pg(expg));
 				ZDM_FREE(znd, expg, sizeof(*expg), KM_20);
 #endif
 			}
@@ -6738,7 +6815,7 @@ static int cache_pg(struct zoned *znd, struct map_pg *pg, gfp_t gfp,
 		}
 		if (pg->data.addr) {
 			set_bit(IS_FLUSH, &pg->flags);
-			znd->incore_count++;
+			atomic_inc(&znd->incore);
 			pg->age = jiffies_64 + msecs_to_jiffies(pg->hotness);
 			rc = pool_add(znd, pg);
 			Z_DBG(znd, "Page loaded: lba: %" PRIx64, pg->lba);
diff --git a/drivers/md/raid5.c b/drivers/md/raid5.c
index 30f3261..1ab8aa9 100644
--- a/drivers/md/raid5.c
+++ b/drivers/md/raid5.c
@@ -7003,7 +7003,6 @@ static int run(struct mddev *mddev)
 						limits.discard_zeroes_data)
 				discard_supported = false;
 
-
 			if (!bdev_discard_raid_safe(rdev->bdev)) {
 				if (discard_supported) {
 					pr_info("md/raid456: No discard: %s.\n",
@@ -7013,7 +7012,6 @@ static int run(struct mddev *mddev)
 				discard_supported = false;
 			}
 
-
 			/* Unfortunately, discard_zeroes_data is not currently
 			 * a guarantee - just a hint.  So we only allow DISCARD
 			 * if the sysadmin has confirmed that only safe devices
diff --git a/drivers/scsi/sd.c b/drivers/scsi/sd.c
index 2c9b4d3..28654c9 100644
--- a/drivers/scsi/sd.c
+++ b/drivers/scsi/sd.c
@@ -7,20 +7,20 @@
  *              Subsequent revisions: Eric Youngdale
  *	Modification history:
  *       - Drew Eckhardt <drew@colorado.edu> original
- *       - Eric Youngdale <eric@andante.org> add scatter-gather, multiple
+ *       - Eric Youngdale <eric@andante.org> add scatter-gather, multiple 
  *         outstanding request, and other enhancements.
  *         Support loadable low-level scsi drivers.
- *       - Jirka Hanika <geo@ff.cuni.cz> support more scsi disks using
+ *       - Jirka Hanika <geo@ff.cuni.cz> support more scsi disks using 
  *         eight major numbers.
  *       - Richard Gooch <rgooch@atnf.csiro.au> support devfs.
- *	 - Torben Mathiasen <tmm@image.dk> Resource allocation fixes in
+ *	 - Torben Mathiasen <tmm@image.dk> Resource allocation fixes in 
  *	   sd_init and cleanups.
  *	 - Alex Davis <letmein@erols.com> Fix problem where partition info
- *	   not being read in sd_open. Fix problem where removable media
+ *	   not being read in sd_open. Fix problem where removable media 
  *	   could be ejected after sd_open.
  *	 - Douglas Gilbert <dgilbert@interlog.com> cleanup for lk 2.5.x
- *	 - Badari Pulavarty <pbadari@us.ibm.com>, Matthew Wilcox
- *	   <willy@debian.org>, Kurt Garloff <garloff@suse.de>:
+ *	 - Badari Pulavarty <pbadari@us.ibm.com>, Matthew Wilcox 
+ *	   <willy@debian.org>, Kurt Garloff <garloff@suse.de>: 
  *	   Support 32k/1M disks.
  *
  *	Logging policy (needs CONFIG_SCSI_LOGGING defined):
@@ -29,7 +29,7 @@
  *	 - entering sd_ioctl: SCSI_LOG_IOCTL level 1
  *	 - entering other commands: SCSI_LOG_HLQUEUE level 3
  *	Note: when the logging level is set by the user, it must be greater
- *	than the level indicated above to trigger output.
+ *	than the level indicated above to trigger output.	
  */
 
 #include <linux/module.h>
@@ -552,16 +552,16 @@ static struct kobject *sd_default_probe(dev_t devt, int *partno, void *data)
 
 /*
  * Device no to disk mapping:
- *
+ * 
  *       major         disc2     disc  p1
  *   |............|.............|....|....| <- dev_t
  *    31        20 19          8 7  4 3  0
- *
+ * 
  * Inside a major, we have 16k disks, however mapped non-
  * contiguously. The first 16 disks are for major0, the next
- * ones with major1, ... Disk 256 is for major0 again, disk 272
- * for major1, ...
- * As we stay compatible with our numbering scheme, we can reuse
+ * ones with major1, ... Disk 256 is for major0 again, disk 272 
+ * for major1, ... 
+ * As we stay compatible with our numbering scheme, we can reuse 
  * the well-know SCSI majors 8, 65--71, 136--143.
  */
 static int sd_major(int major_idx)
@@ -968,7 +968,7 @@ static int sd_setup_read_write_cmnd(struct scsi_cmnd *SCpnt)
 
 	if (sdp->changed) {
 		/*
-		 * quietly refuse to do anything to a changed disc until
+		 * quietly refuse to do anything to a changed disc until 
 		 * the changed bit has been reset
 		 */
 		/* printk("SCSI disk has been changed or is not present. Prohibiting further I/O.\n"); */
@@ -1218,7 +1218,6 @@ static int sd_setup_zoned_cmnd(struct scsi_cmnd *cmd)
 			cmd->cmnd[2] = 0x0e;
 			cmd->cmnd[3] = bio_get_streamid(bio);
 			cmd->cmnd[4] = ATA_SUBCMD_REP_ZONES;
-			cmd->cmnd[4] = bio_get_streamid(bio);
 			cmd->cmnd[5] = ((nr_bytes / 512) >> 8) & 0xff;
 			cmd->cmnd[6] = (nr_bytes / 512) & 0xff;
 
@@ -1228,10 +1227,10 @@ static int sd_setup_zoned_cmnd(struct scsi_cmnd *cmd)
 			cmd->cmnd[14] = ATA_CMD_ZONE_MAN_IN;
 		} else {
 			cmd->cmnd[0] = ZBC_REPORT_ZONES;
-			cmd->cmnd[1] = bio_get_streamid(bio);
+			cmd->cmnd[1] = ZBC_REPORT_OPT;
 			put_unaligned_be64(sector, &cmd->cmnd[2]);
 			put_unaligned_be32(nr_bytes, &cmd->cmnd[10]);
-			cmd->cmnd[14] = 0;
+			cmd->cmnd[14] = bio_get_streamid(bio);
 		}
 		cmd->sc_data_direction = DMA_FROM_DEVICE;
 		cmd->sdb.length = nr_bytes;
@@ -1320,7 +1319,7 @@ static void sd_uninit_command(struct scsi_cmnd *SCpnt)
  *	@inode: only i_rdev member may be used
  *	@filp: only f_mode and f_flags may be used
  *
- *	Returns 0 if successful. Returns a negated errno value in case
+ *	Returns 0 if successful. Returns a negated errno value in case 
  *	of error.
  *
  *	Note: This can be called from a user context (e.g. fsck(1) )
@@ -1388,7 +1387,7 @@ static int sd_open(struct block_device *bdev, fmode_t mode)
 
 error_out:
 	scsi_disk_put(sdkp);
-	return retval;
+	return retval;	
 }
 
 /**
@@ -1397,7 +1396,7 @@ error_out:
  *	@inode: only i_rdev member may be used
  *	@filp: only f_mode and f_flags may be used
  *
- *	Returns 0.
+ *	Returns 0. 
  *
  *	Note: may block (uninterruptible) if error recovery is underway
  *	on this disk.
@@ -1435,7 +1434,7 @@ static int sd_getgeo(struct block_device *bdev, struct hd_geometry *geo)
         diskinfo[0] = 0x40;	/* 1 << 6 */
        	diskinfo[1] = 0x20;	/* 1 << 5 */
        	diskinfo[2] = sdkp->capacity >> 11;
-
+	
 	/* override with calculated, extended default, or driver values */
 	if (host->hostt->bios_param)
 		host->hostt->bios_param(sdp, bdev, sdkp->capacity, diskinfo);
@@ -1470,7 +1469,7 @@ static int sd_ioctl(struct block_device *bdev, fmode_t mode,
 	struct scsi_device *sdp = sdkp->device;
 	void __user *p = (void __user *)arg;
 	int error;
-
+    
 	SCSI_LOG_IOCTL(1, sd_printk(KERN_INFO, sdkp, "sd_ioctl: disk=%s, "
 				    "cmd=0x%x\n", disk->disk_name, cmd));
 
@@ -1698,9 +1697,9 @@ static void sd_rescan(struct device *dev)
 
 
 #ifdef CONFIG_COMPAT
-/*
- * This gets directly called from VFS. When the ioctl
- * is not recognized we go back to the other translation paths.
+/* 
+ * This gets directly called from VFS. When the ioctl 
+ * is not recognized we go back to the other translation paths. 
  */
 static int sd_compat_ioctl(struct block_device *bdev, fmode_t mode,
 			   unsigned int cmd, unsigned long arg)
@@ -1712,12 +1711,12 @@ static int sd_compat_ioctl(struct block_device *bdev, fmode_t mode,
 			(mode & FMODE_NDELAY) != 0);
 	if (error)
 		return error;
-
-	/*
+	       
+	/* 
 	 * Let the static ioctl translation table take care of it.
 	 */
 	if (!sdev->host->hostt->compat_ioctl)
-		return -ENOIOCTLCMD;
+		return -ENOIOCTLCMD; 
 	return sdev->host->hostt->compat_ioctl(sdev, cmd, (void __user *)arg);
 }
 #endif
@@ -2059,7 +2058,7 @@ sd_spinup_disk(struct scsi_disk *sdkp)
 			if (the_result)
 				sense_valid = scsi_sense_valid(&sshdr);
 			retries++;
-		} while (retries < 3 &&
+		} while (retries < 3 && 
 			 (!scsi_status_is_good(the_result) ||
 			  ((driver_byte(the_result) & DRIVER_SENSE) &&
 			  sense_valid && sshdr.sense_key == UNIT_ATTENTION)));
@@ -2132,7 +2131,7 @@ sd_spinup_disk(struct scsi_disk *sdkp)
 			}
 			break;
 		}
-
+				
 	} while (spintime && time_before_eq(jiffies, spintime_expire));
 
 	if (spintime) {
@@ -3248,13 +3247,13 @@ static void sd_probe_async(void *data, async_cookie_t cookie)
  *	for each scsi device (not just disks) present.
  *	@dev: pointer to device object
  *
- *	Returns 0 if successful (or not interested in this scsi device
+ *	Returns 0 if successful (or not interested in this scsi device 
  *	(e.g. scanner)); 1 when there is an error.
  *
  *	Note: this function is invoked from the scsi mid-level.
- *	This function sets up the mapping between a given
- *	<host,channel,id,lun> (found in sdp) and new device name
- *	(e.g. /dev/sda). More precisely it is the block device major
+ *	This function sets up the mapping between a given 
+ *	<host,channel,id,lun> (found in sdp) and new device name 
+ *	(e.g. /dev/sda). More precisely it is the block device major 
  *	and minor number that is chosen here.
  *
  *	Assume sd_probe is not re-entrant (for time being)
@@ -3400,7 +3399,7 @@ static void scsi_disk_release(struct device *dev)
 {
 	struct scsi_disk *sdkp = to_scsi_disk(dev);
 	struct gendisk *disk = sdkp->disk;
-
+	
 	spin_lock(&sd_index_lock);
 	ida_remove(&sd_index_ida, sdkp->index);
 	spin_unlock(&sd_index_lock);
diff --git a/fs/read_write.c b/fs/read_write.c
index 02482bd..7f139c5 100644
--- a/fs/read_write.c
+++ b/fs/read_write.c
@@ -4,7 +4,7 @@
  *  Copyright (C) 1991, 1992  Linus Torvalds
  */
 
-#include <linux/slab.h>
+#include <linux/slab.h> 
 #include <linux/stat.h>
 #include <linux/fcntl.h>
 #include <linux/file.h>
@@ -624,7 +624,7 @@ SYSCALL_DEFINE4(pwrite64, unsigned int, fd, const char __user *, buf,
 	f = fdget(fd);
 	if (f.file) {
 		ret = -ESPIPE;
-		if (f.file->f_mode & FMODE_PWRITE)
+		if (f.file->f_mode & FMODE_PWRITE)  
 			ret = vfs_write(f.file, buf, count, &pos);
 		fdput(f);
 	}
-- 
2.7.0

