From a25d3d08919d397d02cee5121979a4735a35dc2b Mon Sep 17 00:00:00 2001
From: Shaun Tancheff <shaun@tancheff.com>
Date: Mon, 7 Mar 2016 17:30:15 -0600
Subject: [PATCH 18/29] Disable RAID5 trim by default ... it's too slow. Add
 more do-not-flush-blocks-that-have-been-flushed optimizations.

---
 drivers/md/dm-zoned.c |  6 +++-
 drivers/md/dm-zoned.h |  2 ++
 drivers/md/libzoned.c | 82 +++++++++++++++++++++++++++++++++++++++++++++++----
 3 files changed, 83 insertions(+), 7 deletions(-)

diff --git a/drivers/md/dm-zoned.c b/drivers/md/dm-zoned.c
index 76f5ce8..0a70688 100644
--- a/drivers/md/dm-zoned.c
+++ b/drivers/md/dm-zoned.c
@@ -649,6 +649,7 @@ static int zoned_ctr(struct dm_target *ti, unsigned argc, char **argv)
 	}
 
 	znd->trim = 1;
+	znd->raid5_trim = 0;
 
 	if (argc < 1) {
 		ti->error = "Invalid argument count";
@@ -678,6 +679,8 @@ static int zoned_ctr(struct dm_target *ti, unsigned argc, char **argv)
 			znd->trim = 1;
 		if (!strcasecmp("nodiscard", argv[r]))
 			znd->trim = 0;
+		if (!strcasecmp("raid5_trim", argv[r]))
+			znd->raid5_trim = 0;
 
 		if (!strncasecmp("reserve=", argv[r], 8)) {
 			long long mz_resv;
@@ -2008,7 +2011,8 @@ static void zoned_io_hints(struct dm_target *ti, struct queue_limits *limits)
 		limits->max_discard_sectors = 1 << 30;
 		limits->max_hw_discard_sectors = 1 << 30;
 		limits->discard_zeroes_data = 1;
-		limits->raid_discard_safe = 1;
+		if (znd->raid5_trim)
+			limits->raid_discard_safe = 1;
 	}
 }
 
diff --git a/drivers/md/dm-zoned.h b/drivers/md/dm-zoned.h
index 4f0247b..febb677 100644
--- a/drivers/md/dm-zoned.h
+++ b/drivers/md/dm-zoned.h
@@ -137,6 +137,7 @@ struct z_io_req_t {
 enum pg_flag_enum {
 	IS_DIRTY,
 	IS_STALE,
+	IS_FLUSH,
 
 	IS_FWD,
 	IS_REV,
@@ -658,6 +659,7 @@ struct zoned {
 	unsigned issue_close_zone:1;
 	unsigned is_empty:1;
 	unsigned trim:1;
+	unsigned raid5_trim:1;
 
 };
 
diff --git a/drivers/md/libzoned.c b/drivers/md/libzoned.c
index 5589142..ae9ea35 100644
--- a/drivers/md/libzoned.c
+++ b/drivers/md/libzoned.c
@@ -1282,6 +1282,7 @@ struct meta_pg *_alloc_wp(struct zoned *znd)
 		}
 		_init_wp(znd, gzno, wpg);
 		set_bit(IS_DIRTY, &wpg->flags);
+		clear_bit(IS_FLUSH, &wpg->flags);
 	}
 
 out:
@@ -2978,7 +2979,7 @@ static __always_inline int pg_delete(struct zoned *znd, struct map_pg *expg)
 				ZDM_FREE(znd, pg, Z_C4K, PG_27);
 				znd->incore_count--;
 			}
-			release = 1;
+			release = !test_bit(IS_FLUSH, &expg->flags);
 		} else {
 			Z_ERR(znd, "Detected double list del.");
 		}
@@ -3016,6 +3017,11 @@ static int manage_lazy_activity(struct zoned *znd)
 	_tpg = list_next_entry(expg, lazy);
 	while (&expg->lazy != &znd->lzy_pool) {
 		/*
+		 * this should never happen:
+		 */
+		if (test_bit(IS_DIRTY, &expg->flags))
+			set_bit(DELAY_ADD, &expg->flags);
+		/*
 		 * Migrage pg to zltlst list
 		 */
 		if (test_bit(DELAY_ADD, &expg->flags)) {
@@ -3056,6 +3062,39 @@ out:
 }
 
 /**
+ * mark_clean_flush() - Mark all non-dirty ZLT blocks as 'FLUSH'
+ * @znd: ZDM instance
+ *
+ * After a FLUSH/FUA these blocks are on disk and redundant FLUSH
+ * can be skipped if the block is later ejected.
+ */
+static void mark_clean_flush(struct zoned *znd)
+{
+	struct map_pg *expg = NULL;
+	struct map_pg *_tpg;
+
+	if (list_empty(&znd->zltpool))
+		goto out;
+
+	expg = list_last_entry(&znd->zltpool, typeof(*expg), zltlst);
+	if (!expg || &expg->zltlst == (&znd->zltpool))
+		goto out;
+
+	_tpg = list_prev_entry(expg, zltlst);
+	while (&expg->zltlst != &znd->zltpool) {
+		ref_pg(expg);
+		if (!test_bit(IS_DIRTY, &expg->flags))
+			set_bit(IS_FLUSH, &expg->flags);
+		deref_pg(expg);
+		expg = _tpg;
+		_tpg = list_prev_entry(expg, zltlst);
+	}
+
+out:
+	spin_unlock(&znd->zlt_lck);
+}
+
+/**
  * do_sync_metadata() - Write ZDM state to disk.
  * @znd: ZDM instance
  *
@@ -3079,6 +3118,18 @@ static int do_sync_metadata(struct zoned *znd, int sync, int drop)
 	if (drop)
 		set_bit(DO_FLUSH, &znd->flags);
 
+	/*
+	 * If we are lucky then this sync will get us to a 'clean'
+	 * state and the follow on bdev flush is redunant and skipped
+	 *
+	 * If not we will suffer a performance stall because we were
+	 * ejected blocks.
+	 *
+	 * TODO: On Sync/Flush/FUA we can mark all of our clean ZLT
+	 *       as flushed and we can bypass elevating the drop count
+	 *       to trigger a flush for such already flushed blocks.
+	 */
+	want_flush = test_bit(DO_FLUSH, &znd->flags);
 	err = z_mapped_sync(znd);
 	if (err) {
 		Z_ERR(znd, "Uh oh. z_mapped_sync -> %d", err);
@@ -3091,7 +3142,11 @@ static int do_sync_metadata(struct zoned *znd, int sync, int drop)
 			Z_ERR(znd, "Uh oh. flush_bdev failed. -> %d", err);
 			goto out;
 		}
+		want_flush = 1;
 	}
+	if (want_flush)
+		mark_clean_flush(znd);
+
 out:
 	return err;
 }
@@ -3675,7 +3730,7 @@ static int z_mapped_sync(struct zoned *znd)
 
 			if (!need_sync_io &&
 			    test_and_clear_bit(DO_FLUSH, &znd->flags))
-				rw |= REQ_FLUSH;
+				rw |= (REQ_FLUSH | REQ_FUA);
 			rc = writef_block(ti, rw, DM_IO_VMA, io_vcache, lba,
 					  cached, use_wq);
 			if (rc) {
@@ -4537,6 +4592,7 @@ static int metadata_dirty_fling(struct zoned *znd, u64 dm_s)
 		pg->age = jiffies_64 + msecs_to_jiffies(pg->hotness);
 		clear_bit(IS_READA, &pg->flags);
 		set_bit(IS_DIRTY, &pg->flags);
+		clear_bit(IS_FLUSH, &pg->flags);
 		is_flung = 1;
 		mutex_unlock(&pg->md_lock);
 
@@ -4983,6 +5039,7 @@ static void clear_gc_target_flag(struct zoned *znd)
 			wpg->wp_alloc[gzoff] = cpu_to_le32(wp);
 		}
 		set_bit(IS_DIRTY, &wpg->flags);
+		clear_bit(IS_FLUSH, &wpg->flags);
 		spin_unlock(&wpg->wplck);
 	}
 }
@@ -5157,6 +5214,7 @@ static sector_t _blkalloc(struct zoned *znd, u32 z_at, u32 flags,
 		wpg->wp_alloc[gzoff] = cpu_to_le32(wptr);
 		wpg->zf_est[gzoff] = cpu_to_le32(zf_est);
 		set_bit(IS_DIRTY, &wpg->flags);
+		clear_bit(IS_FLUSH, &wpg->flags);
 	}
 	spin_unlock(&wpg->wplck);
 
@@ -5392,6 +5450,7 @@ static int z_zone_gc_compact(struct gc_state *gc_entry)
 		wp |= Z_WP_GC_FULL;
 		wpg->wp_alloc[gzoff] = cpu_to_le32(wp);
 		set_bit(IS_DIRTY, &wpg->flags);
+		clear_bit(IS_FLUSH, &wpg->flags);
 		spin_unlock(&wpg->wplck);
 
 		err = _cached_to_tables(znd, gc_entry->z_gc);
@@ -5512,6 +5571,7 @@ next_in_queue:
 		else if (znd->z_meta_resv & Z_WP_GC_ACTIVE)
 			znd->z_meta_resv = gc_entry->z_gc;
 		set_bit(IS_DIRTY, &wpg->flags);
+		clear_bit(IS_FLUSH, &wpg->flags);
 		spin_unlock(&wpg->wplck);
 
 		znd->gc_events++;
@@ -5845,6 +5905,7 @@ static void zone_filled_cleanup(struct zoned *znd)
 				wpg->wp_alloc[gzoff] = cpu_to_le32(wp
 						     | Z_WP_GC_READY);
 				set_bit(IS_DIRTY, &wpg->flags);
+				clear_bit(IS_FLUSH, &wpg->flags);
 			} else {
 				Z_ERR(znd, "Zone %u seems bogus.", zone);
 			}
@@ -6093,6 +6154,7 @@ static void pg_crc(struct zoned *znd, struct map_pg *pg, __le16 md_crc)
 			crc_pg->data.crc[entry] = md_crc;
 			clear_bit(IS_READA, &crc_pg->flags);
 			set_bit(IS_DIRTY, &crc_pg->flags);
+			clear_bit(IS_FLUSH, &crc_pg->flags);
 			crc_pg->age = jiffies_64
 				    + msecs_to_jiffies(crc_pg->hotness);
 			deref_pg(crc_pg);
@@ -6327,7 +6389,9 @@ out_queued:
  * _sync_dirty() - Write all *dirty* ZLT blocks to disk (journal->SYNC->home)
  * @znd: ZDM instance
  * @bit_type: MAP blocks then CRC blocks.
- * @count: (MAX) Number of entries to write-back per call.
+ * @sync: If true write dirty blocks to disk
+ * @_drop: IN: Number of ZLT blocks to free.
+ *        OUT: Number of (clean) blocks removed tha are not FLUSH flagged.
  *
  * Return: 0 on success or -errno value
  */
@@ -6335,7 +6399,7 @@ static int _sync_dirty(struct zoned *znd, int bit_type, int sync, int *_drop)
 {
 	int err = 0;
 	int entries = 0;
-	int removed = 0;
+	int want_flush = 0;
 	int drop = *_drop;
 	struct map_pg *expg = NULL;
 	struct map_pg *_tpg;
@@ -6405,7 +6469,9 @@ static int _sync_dirty(struct zoned *znd, int bit_type, int sync, int *_drop)
 					expg->data.addr = NULL;
 					ZDM_FREE(znd, pg, Z_C4K, PG_27);
 					znd->incore_count--;
-					removed++;
+
+                                        if (!test_bit(IS_FLUSH, &expg->flags))
+						want_flush++;
 				}
 				mutex_unlock(&expg->md_lock);
 
@@ -6436,7 +6502,7 @@ writeback:
 	}
 
 out:
-	*_drop = removed;
+	*_drop = want_flush;
 	if (!list_empty(&droplist))
 		lazy_pool_splice(znd, &droplist);
 
@@ -6660,6 +6726,7 @@ static int cache_pg(struct zoned *znd, struct map_pg *pg, gfp_t gfp,
 			rc = -ENOMEM;
 		}
 		if (pg->data.addr) {
+			set_bit(IS_FLUSH, &pg->flags);
 			znd->incore_count++;
 			pg->age = jiffies_64 + msecs_to_jiffies(pg->hotness);
 			rc = pool_add(znd, pg);
@@ -6749,6 +6816,7 @@ static int update_map_entry(struct zoned *znd, struct map_pg *pg,
 				pg->age = jiffies_64
 					    + msecs_to_jiffies(pg->hotness);
 				set_bit(IS_DIRTY, &pg->flags);
+				clear_bit(IS_FLUSH, &pg->flags);
 				clear_bit(IS_READA, &pg->flags);
 				was_updated = 1;
 			} else if (value != MZTEV_UNUSED) {
@@ -6964,6 +7032,7 @@ static int unused_phy(struct zoned *znd, u64 lba, u64 orig_s, gfp_t gfp)
 		pg->age = jiffies_64 + msecs_to_jiffies(pg->hotness);
 		set_bit(IS_DIRTY, &pg->flags);
 		clear_bit(IS_READA, &pg->flags);
+		clear_bit(IS_FLUSH, &pg->flags);
 
 		SpinLock(&wpg->wplck);
 		wp = le32_to_cpu(wpg->wp_alloc[gzoff]);
@@ -6974,6 +7043,7 @@ static int unused_phy(struct zoned *znd, u64 lba, u64 orig_s, gfp_t gfp)
 			wpg->zf_est[gzoff] = cpu_to_le32(zf | stream_id);
 			wpg->wp_alloc[gzoff] = cpu_to_le32(wp | Z_WP_RRECALC);
 			set_bit(IS_DIRTY, &wpg->flags);
+			clear_bit(IS_FLUSH, &wpg->flags);
 		}
 		spin_unlock(&wpg->wplck);
 		if ((wp & Z_WP_VALUE_MASK) == Z_BLKSZ)
-- 
2.7.0

